
@article{bednarikEMIPEyeMovements2020,
  title = {{{EMIP}}: {{The}} Eye Movements in Programming Dataset},
  shorttitle = {{{EMIP}}},
  author = {Bednarik, Roman and Busjahn, Teresa and Gibaldi, Agostino and Ahadi, Alireza and Bielikova, Maria and Crosby, Martha and Essig, Kai and Fagerholm, Fabian and Jbara, Ahmad and Lister, Raymond and Orlov, Pavel and Paterson, James and Sharif, Bonita and Sirki{\"a}, Teemu and Stelovsky, Jan and Tvarozek, Jozef and Vrzakova, Hana and {van der Linde}, Ian},
  year = {2020},
  month = oct,
  volume = {198},
  pages = {102520},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2020.102520},
  abstract = {A large dataset that contains the eye movements of N=216 programmers of different experience levels captured during two code comprehension tasks is presented. Data are grouped in terms of programming expertise (from none to high) and other demographic descriptors. Data were collected through an international collaborative effort that involved eleven research teams across eight countries on four continents. The same eye tracking apparatus and software was used for the data collection. The Eye Movements in Programming (EMIP) dataset is freely available for download. The varied metadata in the EMIP dataset provides fertile ground for the analysis of gaze behavior and may be used to make novel insights about code comprehension.},
  file = {/Users/aslak/Zotero/storage/ECVX59TH/Bednarik et al. - 2020 - EMIP The eye movements in programming dataset.pdf;/Users/aslak/Zotero/storage/W2YVK52Q/S0167642320301283.html},
  journal = {Science of Computer Programming},
  keywords = {Dataset,Eye-tracking,Program comprehension},
  language = {en}
}

@inproceedings{duchowskiLowHighIndex2020,
  title = {The {{Low}}/{{High Index}} of {{Pupillary Activity}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Duchowski, Andrew T. and Krejtz, Krzysztof and Gehrer, Nina A. and Bafna, Tanya and B{\ae}kgaard, Per},
  year = {2020},
  month = apr,
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376394},
  abstract = {A novel eye-tracked measure of pupil diameter oscillation is derived as an indicator of cognitive load. The new metric, termed the Low/High Index of Pupillary Activity (LHIPA), is able to discriminate cognitive load (vis-a-vis task difficulty) in several experiments where the Index of Pupillary Activity fails to do so. Rationale for the LHIPA is tied to the functioning of the human autonomic nervous system yielding a hybrid measure based on the ratio of Low/High frequencies of pupil oscillation. The paper's contribution is twofold. First, full documentation is provided for the calculation of the LHIPA. As with the IPA, it is possible for researchers to apply this metric to their own experiments where a measure of cognitive load is of interest. Second, robustness of the LHIPA is shown in analysis of three experiments, a restrictive fixed-gaze number counting task, a less restrictive fixed-gaze n-back task, and an applied eye-typing task.},
  file = {/Users/aslak/Zotero/storage/9LNGISLY/Duchowski et al. - 2020 - The LowHigh Index of Pupillary Activity.pdf},
  isbn = {978-1-4503-6708-0},
  keywords = {eye tracking,pupillometry,task difficulty},
  series = {{{CHI}} '20}
}

@inproceedings{fritzUsingPsychophysiologicalMeasures2014,
  title = {Using Psycho-Physiological Measures to Assess Task Difficulty in Software Development},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Software Engineering}}},
  author = {Fritz, Thomas and Begel, Andrew and M{\"u}ller, Sebastian C. and {Yigit-Elliott}, Serap and Z{\"u}ger, Manuela},
  year = {2014},
  month = may,
  pages = {402--413},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2568225.2568266},
  abstract = {Software developers make programming mistakes that cause serious bugs for their customers. Existing work to detect problematic software focuses mainly on post hoc identification of correlations between bug fixes and code. We propose a new approach to address this problem --- detect when software developers are experiencing difficulty while they work on their programming tasks, and stop them before they can introduce bugs into the code. In this paper, we investigate a novel approach to classify the difficulty of code comprehension tasks using data from psycho-physiological sensors. We present the results of a study we conducted with 15 professional programmers to see how well an eye-tracker, an electrodermal activity sensor, and an electroencephalography sensor could be used to predict whether developers would find a task to be difficult. We can predict nominal task difficulty (easy/difficult) for a new developer with 64.99\% precision and 64.58\% recall, and for a new task with 84.38\% precision and 69.79\% recall. We can improve the Naive Bayes classifier's performance if we trained it on just the eye-tracking data over the entire dataset, or by using a sliding window data collection schema with a 55 second time window. Our work brings the community closer to a viable and reliable measure of task difficulty that could power the next generation of programming support tools.},
  file = {/Users/aslak/Zotero/storage/4C5J3B8V/Fritz et al. - 2014 - Using psycho-physiological measures to assess task.pdf},
  isbn = {978-1-4503-2756-5},
  keywords = {psycho-physiological,study,task difficulty},
  series = {{{ICSE}} 2014}
}

@misc{LowHighIndex,
  title = {The {{Low}}/{{High Index}} of {{Pupillary Activity}} | {{Proceedings}} of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  file = {/Users/aslak/Zotero/storage/KVIR3VAS/3313831.html},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/3313831.3376394}
}

@article{saariGeneralizabilitySimplicityCriteria2011,
  title = {Generalizability and {{Simplicity}} as {{Criteria}} in {{Feature Selection}}: {{Application}} to {{Mood Classification}} in {{Music}}},
  shorttitle = {Generalizability and {{Simplicity}} as {{Criteria}} in {{Feature Selection}}},
  author = {Saari, P. and Eerola, T. and Lartillot, O.},
  year = {2011},
  month = aug,
  volume = {19},
  pages = {1802--1812},
  issn = {1558-7924},
  doi = {10.1109/TASL.2010.2101596},
  abstract = {Classification of musical audio signals according to expressed mood or emotion has evident applications to content-based music retrieval in large databases. Wrapper selection is a dimension reduction method that has been proposed for improving classification performance. However, the technique is prone to lead to overfitting of the training data, which decreases the generalizability of the obtained results. We claim that previous attempts to apply wrapper selection in the field of music information retrieval (MIR) have led to disputable conclusions about the used methods due to inadequate analysis frameworks, indicative of overfitting, and biased results. This paper presents a framework based on cross-indexing for obtaining realistic performance estimate of wrapper selection by taking into account the simplicity and generalizability of the classification models. The framework is applied on sets of film soundtrack excerpts that are consensually associated with particular basic emotions, comparing Naive Bayes, k-NN, and SVM classifiers using both forward selection (FS) and backward elimination (BE). K-NN with BE yields the most promising results - 56.5\% accuracy with only four features. The most useful feature subset for k-NN contains mode majorness and key clarity, combined with dynamical, rhythmical, and structural features.},
  file = {/Users/aslak/Zotero/storage/B6SMWPYH/5676183.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {Accuracy,audio signal processing,backward elimination,Bayes methods,content-based music retrieval,cross-indexing,Cross-indexing,dimension reduction method,Emotion recognition,Feature extraction,feature selection,film soundtrack,forward selection,k-NN,Materials,MIR,Mood,mood classification,music,Music,music and emotion,music information retrieval,musical audio signal classification,musical features,naive Bayes,overfitting,Prediction algorithms,signal classification,support vector machines,SVM classifier,wrapper selection},
  number = {6}
}

@article{sharmaAssessingCognitivePerformance2020,
  title = {Assessing {{Cognitive Performance Using Physiological}} and {{Facial Features}}: {{Generalizing}} across {{Contexts}}},
  shorttitle = {Assessing {{Cognitive Performance Using Physiological}} and {{Facial Features}}},
  author = {Sharma, Kshitij and Niforatos, Evangelos and Giannakos, Michail and Kostakos, Vassilis},
  year = {2020},
  month = sep,
  volume = {4},
  pages = {95:1--95:41},
  doi = {10.1145/3411811},
  abstract = {Sensing and machine learning advances have enabled the unobtrusive measurement of physiological responses and facial expressions so as to estimate one's cognitive performance. This often boils down to mapping the states of the cognitive processes underpinning human cognition: physiological responses (e.g., heart rate) and facial expressions (e.g., frowning) often reflect the states of our cognitive processes. However, it remains unclear whether physiological responses and facial expressions used in one particular task (e.g., gaming) can reliably assess cognitive performance in another task (e.g., coding), because complex and diverse tasks often require varying levels and combinations of cognitive processes. In this paper, we measure the cross-task reliability of physiological and facial responses. Specifically, we assess cognitive performance based on physiological responses and facial expressions for 123 participants in 4 independent studies (3 studies for out-of-sampling training and testing, and 1 study for evaluation only): (1) a Pac-Man game, (2) an adaptive-assessment task, (3) a code-debugging task, and (4) a gaze-based game. We follow an ensemble learning approach after cross-training and cross-testing with all possible combinations of the 3 first datasets. We save the 4th dataset only for testing purposes, and we showcase how to engineer generalizable features that predict cognitive performance. Our results show that the extracted features do generalize, and can reliably predict cognitive performance across a diverse set of cognitive tasks that require different combinations of problem-solving, decision-making, and learning processes for their completion.},
  file = {/Users/aslak/Zotero/storage/PFRPWWCK/Sharma et al. - 2020 - Assessing Cognitive Performance Using Physiologica.pdf},
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  keywords = {Adaptive assessment,Cognitive performance,Debugging,Educational technology,Ensemble learning,Feature Generalizability,GARCH,Gaze-contingency,Learning Analytics,MMLA,Multimodal Learning Analytics,Skill-acquisition},
  number = {3}
}

@article{shojaeizadehDetectingTaskDemand2019,
  title = {Detecting Task Demand via an Eye Tracking Machine Learning System},
  author = {Shojaeizadeh, Mina and Djamasbi, Soussan and Paffenroth, Randy C. and Trapp, Andrew C.},
  year = {2019},
  month = jan,
  volume = {116},
  pages = {91--101},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2018.10.012},
  abstract = {Computerized systems play a significant role in today's fast-paced digital economy. Because task demand is a major factor that influences how computerized systems are used to make decisions, identifying task demand automatically provides an opportunity for designing advanced decision support systems that can respond to user needs at a personalized level. A first step for designing such advanced decision tools is to investigate possibilities for developing automatic task load detectors. Grounded in decision making, eye tracking, and machine learning literature, we argue that task demand can be detected automatically, reliably, and unobtrusively using eye movements only. To investigate this possibility, we developed an eye tracking task load detection system and tested its effectiveness. Our results revealed that our task load detection system reliably predicted increased task demand from users' eye movement data. These results and their implications for research and practice are discussed.},
  file = {/Users/aslak/Zotero/storage/XSSUTKWT/Shojaeizadeh et al. - 2019 - Detecting task demand via an eye tracking machine .pdf;/Users/aslak/Zotero/storage/CV5AWDFS/S0167923618301696.html},
  journal = {Decision Support Systems},
  keywords = {Adaptive decision making,Cognitive effort,Eye tracking,Human computer interaction,Machine learning,Task demand},
  language = {en}
}


