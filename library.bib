
@article{bednarikEMIPEyeMovements2020,
  title = {{{EMIP}}: {{The}} Eye Movements in Programming Dataset},
  shorttitle = {{{EMIP}}},
  author = {Bednarik, Roman and Busjahn, Teresa and Gibaldi, Agostino and Ahadi, Alireza and Bielikova, Maria and Crosby, Martha and Essig, Kai and Fagerholm, Fabian and Jbara, Ahmad and Lister, Raymond and Orlov, Pavel and Paterson, James and Sharif, Bonita and Sirki{\"a}, Teemu and Stelovsky, Jan and Tvarozek, Jozef and Vrzakova, Hana and {van der Linde}, Ian},
  year = {2020},
  month = oct,
  volume = {198},
  pages = {102520},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2020.102520},
  abstract = {A large dataset that contains the eye movements of N=216 programmers of different experience levels captured during two code comprehension tasks is presented. Data are grouped in terms of programming expertise (from none to high) and other demographic descriptors. Data were collected through an international collaborative effort that involved eleven research teams across eight countries on four continents. The same eye tracking apparatus and software was used for the data collection. The Eye Movements in Programming (EMIP) dataset is freely available for download. The varied metadata in the EMIP dataset provides fertile ground for the analysis of gaze behavior and may be used to make novel insights about code comprehension.},
  file = {/Users/aslak/Zotero/storage/ECVX59TH/Bednarik et al. - 2020 - EMIP The eye movements in programming dataset.pdf;/Users/aslak/Zotero/storage/W2YVK52Q/S0167642320301283.html},
  journal = {Science of Computer Programming},
  keywords = {Dataset,Eye-tracking,Program comprehension},
  language = {en}
}

@article{bojkoEyeTrackingUser2005,
  title = {Eye {{Tracking}} in {{User Experience Testing}}: {{How}} to {{Make}} the {{Most}} of {{It}}},
  shorttitle = {Eye {{Tracking}} in {{User Experience Testing}}},
  author = {Bojko, Agnieszka},
  year = {2005},
  month = jan,
  abstract = {As eye tracking technology becomes more precise, affordable, and unobtrusive, its popularity continues to increase among usability practitioners. This paper introduces eye tracking as a user experience testing tool. It focuses on how to design and conduct studies involving eye tracking, so that eye movement data can effectively supplement data obtained through more conventional methods. Using examples from actual studies, we share lessons learned and provide advice on how to avoid common mistakes.},
  file = {/Users/aslak/Zotero/storage/WU82XG4M/Bojko - 2005 - Eye Tracking in User Experience Testing How to Ma.pdf},
  journal = {Proceedings of the 14th Annual Conference of the Usability Professionals' Association (UPA). Montr\'eal, Canada}
}

@article{bollerslevGeneralizedAutoregressiveConditional1986,
  title = {Generalized Autoregressive Conditional Heteroskedasticity},
  author = {Bollerslev, Tim},
  year = {1986},
  month = apr,
  volume = {31},
  pages = {307--327},
  issn = {0304-4076},
  doi = {10.1016/0304-4076(86)90063-1},
  abstract = {A natural generalization of the ARCH (Autoregressive Conditional Heteroskedastic) process introduced in Engle (1982) to allow for past conditional variances in the current conditional variance equation is proposed. Stationarity conditions and autocorrelation structure for this new class of parametric models are derived. Maximum likelihood estimation and testing are also considered. Finally an empirical example relating to the uncertainty of the inflation rate is presented.},
  file = {/Users/aslak/Zotero/storage/ADKZ8E6C/Bollerslev - 1986 - Generalized autoregressive conditional heteroskeda.pdf;/Users/aslak/Zotero/storage/2A3SUBLM/0304407686900631.html},
  journal = {Journal of Econometrics},
  language = {en},
  number = {3}
}

@inproceedings{cantoniEyeTrackingComputer2014,
  title = {Eye Tracking as a Computer Input and Interaction Method},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Computer Systems}} and {{Technologies}}},
  author = {Cantoni, Virginio and Porta, Marco},
  year = {2014},
  month = jun,
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2659532.2659592},
  abstract = {Eye tracking applications can be considered under two points of view: in the former the eye tracker is a passive sensor that monitors the eyes to determine what the user is watching. In the latter the eye tracker has an active role that allows the user to control a computer. As a computer input device, an eye tracker typically substitutes the mouse point-select operation with a look-select process to: press buttons, select icons, follow links, etc. While look-select operations are naturally suited to eye input, controlling an interface element is not, because the eyes move covertly by saccades -- quick movements of the point of gaze from one location to another. Since the main task of the eyes is simply to see, if they are also used for interacting with the computer it may be difficult to decide, for example, whether a button is watched to understand its function or to trigger the associated action. In general, eye tracking systems present significant challenges when used for computer input and much research has been carried out in this field.},
  file = {/Users/aslak/Zotero/storage/FA3TY38J/Cantoni and Porta - 2014 - Eye tracking as a computer input and interaction m.pdf},
  isbn = {978-1-4503-2753-4},
  keywords = {eye tracking,human computer interaction,pointing devices,visual attention},
  series = {{{CompSysTech}} '14}
}

@article{chekroudCrosstrialPredictionTreatment2016,
  title = {Cross-Trial Prediction of Treatment Outcome in Depression: A Machine Learning Approach},
  shorttitle = {Cross-Trial Prediction of Treatment Outcome in Depression},
  author = {Chekroud, Adam Mourad and Zotti, Ryan Joseph and Shehzad, Zarrar and Gueorguieva, Ralitza and Johnson, Marcia K and Trivedi, Madhukar H and Cannon, Tyrone D and Krystal, John Harrison and Corlett, Philip Robert},
  year = {2016},
  month = mar,
  volume = {3},
  pages = {243--250},
  issn = {2215-0366},
  doi = {10.1016/S2215-0366(15)00471-X},
  abstract = {Background Antidepressant treatment efficacy is low, but might be improved by matching patients to interventions. At present, clinicians have no empirically validated mechanisms to assess whether a patient with depression will respond to a specific antidepressant. We aimed to develop an algorithm to assess whether patients will achieve symptomatic remission from a 12-week course of citalopram. Methods We used patient-reported data from patients with depression (n=4041, with 1949 completers) from level 1 of the Sequenced Treatment Alternatives to Relieve Depression (STAR*D; ClinicalTrials.gov, number NCT00021528) to identify variables that were most predictive of treatment outcome, and used these variables to train a machine-learning model to predict clinical remission. We externally validated the model in the escitalopram treatment group (n=151) of an independent clinical trial (Combining Medications to Enhance Depression Outcomes [COMED]; ClinicalTrials.gov, number NCT00590863). Findings We identified 25 variables that were most predictive of treatment outcome from 164 patient-reportable variables, and used these to train the model. The model was internally cross-validated, and predicted outcomes in the STAR*D cohort with accuracy significantly above chance (64{$\cdot$}6\% [SD 3{$\cdot$}2]; p{$<$}0{$\cdot$}0001). The model was externally validated in the escitalopram treatment group (N=151) of COMED (accuracy 59{$\cdot$}6\%, p=0.043). The model also performed significantly above chance in a combined escitalopram-buproprion treatment group in COMED (n=134; accuracy 59{$\cdot$}7\%, p=0{$\cdot$}023), but not in a combined venlafaxine-mirtazapine group (n=140; accuracy 51{$\cdot$}4\%, p=0{$\cdot$}53), suggesting specificity of the model to underlying mechanisms. Interpretation Building statistical models by mining existing clinical trial data can enable prospective identification of patients who are likely to respond to a specific antidepressant. Funding Yale University.},
  file = {/Users/aslak/Zotero/storage/QS8ZLE8Z/S221503661500471X.html},
  journal = {The Lancet Psychiatry},
  language = {en},
  number = {3}
}

@article{chenAutomaticClassificationEye2013,
  title = {Automatic Classification of Eye Activity for Cognitive Load Measurement with Emotion Interference},
  author = {Chen, Siyuan and Epps, Julien},
  year = {2013},
  month = may,
  volume = {110},
  pages = {111--124},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2012.10.021},
  abstract = {Measuring cognitive load changes can contribute to better treatment of patients, can help design effective strategies to reduce medical errors among clinicians and can facilitate user evaluation of health care information systems. This paper proposes an eye-based automatic cognitive load measurement (CLM) system toward realizing these prospects. Three types of eye activity are investigated: pupillary response, blink and eye movement (fixation and saccade). Eye activity features are investigated in the presence of emotion interference, which is a source of undesirable variability, to determine the susceptibility of CLM systems to other factors. Results from an experiment combining arithmetic-based tasks and affective image stimuli demonstrate that arousal effects are dominated by cognitive load during task execution. To minimize the arousal effect on CLM, the choice of segments for eye-based features is examined. We then propose a feature set and classify three levels of cognitive load. The performance of cognitive load level prediction was found to be close to that of a reaction time measure, showing the feasibility of eye activity features for near-real time CLM.},
  file = {/Users/aslak/Zotero/storage/MTWIHBJ8/Chen and Epps - 2013 - Automatic classification of eye activity for cogni.pdf;/Users/aslak/Zotero/storage/TFEH2M59/S0169260712002830.html},
  journal = {Computer Methods and Programs in Biomedicine},
  keywords = {Blink,Cognitive load,Eye activity,Eye movement,Fixation,Physiological measures,Pupil,Saccade},
  language = {en},
  number = {2}
}

@article{chengContextMayBe2016,
  title = {Context May Be {{King}}, but Generalizability Is the {{Emperor}}!},
  author = {Cheng, Aaron and Dimoka, Angelika and Pavlou, Paul},
  year = {2016},
  month = sep,
  volume = {31},
  doi = {10.1057/s41265-016-0005-7},
  abstract = {The relative importance of context and generalizability (or particularism and universalism) has long been debated in scientific research. Recently, Davison and Martinsons raised valid concerns about the possibility of false universalism in IS research, discussed its negative consequences, and made a call for explicitly including particularism in research design and reporting. In this commentary, we generally agree with the notion that context should matter more in IS research; yet, the importance of generalizability in research should not be downplayed. Specifically, we posit that generalizability should be given higher position in the scientific process and be the ultimate goal for researchers. Still, researchers need to fully understand the research context, which, in combination and replication, can help to cautiously make generalizable knowledge claims. Therefore, we characterize the relationship between context and generalizability as that of a ?King? (as an analogy of the local role of context) versus the ?Emperor? (as an analogy of the global role of generalizability).},
  file = {/Users/aslak/Zotero/storage/69DPBLX2/Cheng et al. - 2016 - Context may be King, but generalizability is the E.pdf},
  journal = {Journal of Information Technology}
}

@article{chenUsingTaskInducedPupil2014,
  title = {Using {{Task}}-{{Induced Pupil Diameter}} and {{Blink Rate}} to {{Infer Cognitive Load}}},
  author = {Chen, Siyuan and Epps, Julien},
  year = {2014},
  month = apr,
  volume = {29},
  doi = {10.1080/07370024.2014.892428},
  abstract = {Minimizing user cognitive load is suggested as an integral part of human-centered design, where a more intuitive, easy to learn, and adaptive interface is desired. In this context, it is difficult to develop optimal strategies to improve the design without first knowing how user cognitive load fluctuates during interaction. In this study, we investigate how cognitive load measurement is affected by different task types from the perspective of the load theory of attention, using pupil diameter and blink measures. We induced five levels of cognitive load during low and high perceptual load tasks and found that although pupil diameter showed significant effects on cognitive load when the perceptual load was low, neither blink rate nor pupil diameter showed significant effects on cognitive load when the perceptual load was high. The results indicate that pupil diameter can index cognitive load only in the situation of low perceptual load and are the first to provide empirical support for the cognitive control aspect of the load theory of attention, in the context of cognitive load measurement. Meanwhile, blink is a better indicator of perceptual load than cognitive load. This study also implies that perceptual load should be considered in cognitive load measurement using pupil diameter and blink measures. Automatic detection of the type and level of load in this manner helps pave the way for better reasoning about user internal processes for human-centered interface design.},
  file = {/Users/aslak/Zotero/storage/QUQHRMFD/Chen and Epps - 2014 - Using Task-Induced Pupil Diameter and Blink Rate t.pdf},
  journal = {Human-Computer Interaction}
}

@article{davisonContextKingConsidering2016,
  title = {Context Is King! {{Considering}} Particularism in Research Design and Reporting},
  author = {Davison, Robert M and Martinsons, Maris G},
  year = {2016},
  month = sep,
  volume = {31},
  pages = {241--249},
  publisher = {{SAGE Publications Ltd}},
  issn = {0268-3962},
  doi = {10.1057/jit.2015.19},
  abstract = {We aim to raise awareness of context by examining its role in empirical research. We apply the dichotomy of universalism and particularism, and discuss the interaction of theory and culture in order to consider the scope of validity for research findings and conclusions. We illustrate our arguments by referencing three cases, each of which has contextual inadequacies. We aim to discourage the conduct of research, and acceptance of papers, that falsely implies universalism, relies on convenient samples or ignores indigenous constructs. We offer specific prescriptions for authors, editors and reviewers to help ensure that both the research context and scope of validity are adequately communicated and understood.},
  file = {/Users/aslak/Zotero/storage/F3PNFZIG/Davison and Martinsons - 2016 - Context is king! Considering particularism in rese.pdf},
  journal = {Journal of Information Technology},
  keywords = {context,culture,particularism,research,theory,universalism},
  language = {en},
  number = {3}
}

@article{dexterGeneralizationMachineLearning2020,
  title = {Generalization of {{Machine Learning Approaches}} to {{Identify Notifiable Conditions}} from a {{Statewide Health Information Exchange}}},
  author = {Dexter, Gregory P. and Grannis, Shaun J. and Dixon, Brian E. and Kasthurirathne, Suranga N.},
  year = {2020},
  month = may,
  volume = {2020},
  pages = {152--161},
  issn = {2153-4063},
  abstract = {Healthcare analytics is impeded by a lack of machine learning (ML) model generalizability, the ability of a model to predict accurately on varied data sources not included in the model's training dataset. We leveraged free-text laboratory data from a Health Information Exchange network to evaluate ML generalization using Notifiable Condition Detection (NCD) for public health surveillance as a use case. We 1) built ML models for detecting syphilis, salmonella, and histoplasmosis; 2) evaluated generalizability of these models across data from holdout lab systems, and; 3) explored factors that influence weak model generalizability. Models for predicting each disease reported considerable accuracy. However, they demonstrated poor generalizability across data from holdout lab systems being tested. Our evaluation determined that weak generalization was influenced by variant syntactic nature of free-text datasets across each lab system. Results highlight the need for actionable methodology to generalize ML solutions for healthcare analytics.},
  file = {/Users/aslak/Zotero/storage/X35CTV6M/Dexter et al. - 2020 - Generalization of Machine Learning Approaches to I.pdf},
  journal = {AMIA Summits on Translational Science Proceedings},
  pmcid = {PMC7233074},
  pmid = {32477634}
}

@inproceedings{duchowskiIndexPupillaryActivity2018,
  title = {The {{Index}} of {{Pupillary Activity}}: {{Measuring Cognitive Load}} {\emph{Vis-\&\#xe0;-Vis}} {{Task Difficulty}} with {{Pupil Oscillation}}},
  shorttitle = {The {{Index}} of {{Pupillary Activity}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Duchowski, Andrew T. and Krejtz, Krzysztof and Krejtz, Izabela and Biele, Cezary and Niedzielska, Anna and Kiefer, Peter and Raubal, Martin and Giannopoulos, Ioannis},
  year = {2018},
  month = apr,
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3173574.3173856},
  abstract = {A novel eye-tracked measure of the frequency of pupil diameter oscillation is proposed for capturing what is thought to be an indicator of cognitive load. The proposed metric, termed the Index of Pupillary Activity, is shown to discriminate task difficulty vis-a-vis cognitive load (if the implied causality can be assumed) in an experiment where participants performed easy and difficult mental arithmetic tasks while fixating a central target (a requirement for replication of prior work). The paper's contribution is twofold: full documentation is provided for the calculation of the proposed measurement which can be considered as an alternative to the existing proprietary Index of Cognitive Activity (ICA). Thus, it is possible for researchers to replicate the experiment and build their own software which implements this measurement. Second, several aspects of the ICA are approached in a more data-sensitive way with the goal of improving the measurement's performance.},
  file = {/Users/aslak/Zotero/storage/ZIUSUW3N/Duchowski et al. - 2018 - The Index of Pupillary Activity Measuring Cogniti.pdf},
  isbn = {978-1-4503-5620-6},
  keywords = {eye tracking,pupillometry,task difficulty},
  series = {{{CHI}} '18}
}

@inproceedings{duchowskiLowHighIndex2020,
  title = {The {{Low}}/{{High Index}} of {{Pupillary Activity}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Duchowski, Andrew T. and Krejtz, Krzysztof and Gehrer, Nina A. and Bafna, Tanya and B{\ae}kgaard, Per},
  year = {2020},
  month = apr,
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376394},
  abstract = {A novel eye-tracked measure of pupil diameter oscillation is derived as an indicator of cognitive load. The new metric, termed the Low/High Index of Pupillary Activity (LHIPA), is able to discriminate cognitive load (vis-a-vis task difficulty) in several experiments where the Index of Pupillary Activity fails to do so. Rationale for the LHIPA is tied to the functioning of the human autonomic nervous system yielding a hybrid measure based on the ratio of Low/High frequencies of pupil oscillation. The paper's contribution is twofold. First, full documentation is provided for the calculation of the LHIPA. As with the IPA, it is possible for researchers to apply this metric to their own experiments where a measure of cognitive load is of interest. Second, robustness of the LHIPA is shown in analysis of three experiments, a restrictive fixed-gaze number counting task, a less restrictive fixed-gaze n-back task, and an applied eye-typing task.},
  file = {/Users/aslak/Zotero/storage/9LNGISLY/Duchowski et al. - 2020 - The LowHigh Index of Pupillary Activity.pdf},
  isbn = {978-1-4503-6708-0},
  keywords = {eye tracking,pupillometry,task difficulty},
  series = {{{CHI}} '20}
}

@inproceedings{fritzUsingPsychophysiologicalMeasures2014,
  title = {Using Psycho-Physiological Measures to Assess Task Difficulty in Software Development},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Software Engineering}}},
  author = {Fritz, Thomas and Begel, Andrew and M{\"u}ller, Sebastian C. and {Yigit-Elliott}, Serap and Z{\"u}ger, Manuela},
  year = {2014},
  month = may,
  pages = {402--413},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2568225.2568266},
  abstract = {Software developers make programming mistakes that cause serious bugs for their customers. Existing work to detect problematic software focuses mainly on post hoc identification of correlations between bug fixes and code. We propose a new approach to address this problem --- detect when software developers are experiencing difficulty while they work on their programming tasks, and stop them before they can introduce bugs into the code. In this paper, we investigate a novel approach to classify the difficulty of code comprehension tasks using data from psycho-physiological sensors. We present the results of a study we conducted with 15 professional programmers to see how well an eye-tracker, an electrodermal activity sensor, and an electroencephalography sensor could be used to predict whether developers would find a task to be difficult. We can predict nominal task difficulty (easy/difficult) for a new developer with 64.99\% precision and 64.58\% recall, and for a new task with 84.38\% precision and 69.79\% recall. We can improve the Naive Bayes classifier's performance if we trained it on just the eye-tracking data over the entire dataset, or by using a sliding window data collection schema with a 55 second time window. Our work brings the community closer to a viable and reliable measure of task difficulty that could power the next generation of programming support tools.},
  file = {/Users/aslak/Zotero/storage/4C5J3B8V/Fritz et al. - 2014 - Using psycho-physiological measures to assess task.pdf},
  isbn = {978-1-4503-2756-5},
  keywords = {psycho-physiological,study,task difficulty},
  series = {{{ICSE}} 2014}
}

@article{grankaEyeTrackingAnalysisUser2004,
  title = {Eye-{{Tracking Analysis}} of {{User Behavior}} in {{WWW}}-{{Search}}},
  author = {Granka, Laura and Joachims, Thorsten and Gay, Geri},
  year = {2004},
  month = apr,
  doi = {10.1145/1008992.1009079},
  abstract = {We investigate how users interact with the results page of a WWW search engine using eye-tracking. The goal is to gain insight into how users browse the presented abstracts and how they select links for further exploration. Such understanding is valuable for improved interface design, as well as for more accurate interpretations of implicit feedback (e.g. clickthrough) for machine learning. The following presents initial results, focusing on the amount of time spent viewing the presented abstracts, the total number of abstract viewed, as well as data like query word frequency [6]. Howev tracking, these measurements can at best give in diameter, as a lar measures of how thoroughly searchers evaluate their results set.},
  file = {/Users/aslak/Zotero/storage/Y5T4CUFN/Granka et al. - 2004 - Eye-Tracking Analysis of User Behavior in WWW-Sear.pdf},
  journal = {Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval}
}

@inproceedings{haapalainenPsychophysiologicalMeasuresAssessing2010,
  title = {Psycho-Physiological Measures for Assessing Cognitive Load},
  booktitle = {Proceedings of the 12th {{ACM}} International Conference on {{Ubiquitous}} Computing},
  author = {Haapalainen, Eija and Kim, SeungJun and Forlizzi, Jodi F. and Dey, Anind K.},
  year = {2010},
  month = sep,
  pages = {301--310},
  publisher = {{ACM}},
  address = {{Copenhagen Denmark}},
  doi = {10.1145/1864349.1864395},
  abstract = {With a focus on presenting information at the right time, the ubicomp community can benefit greatly from learning the most salient human measures of cognitive load. Cognitive load can be used as a metric to determine when or whether to interrupt a user. In this paper, we collected data from multiple sensors and compared their ability to assess cognitive load. Our focus is on visual perception and cognitive speed-focused tasks that leverage cognitive abilities common in ubicomp applications. We found that across all participants, the electrocardiogram median absolute deviation and median heat flux measurements were the most accurate at distinguishing between low and high levels of cognitive load, providing a classification accuracy of over 80\% when used together. Our contribution is a real-time, objective, and generalizable method for assessing cognitive load in cognitive tasks commonly found in ubicomp systems and situations of divided attention.},
  file = {/Users/aslak/Zotero/storage/UBNRN5X6/Haapalainen et al. - 2010 - Psycho-physiological measures for assessing cognit.pdf},
  isbn = {978-1-60558-843-8},
  language = {en}
}

@inproceedings{huttTimeScaleGeneralizable2019,
  title = {Time to {{Scale}}: {{Generalizable Affect Detection}} for {{Tens}} of {{Thousands}} of {{Students}} across {{An Entire School Year}}},
  shorttitle = {Time to {{Scale}}},
  booktitle = {{{CHI}} '19: {{Proceedings}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Hutt, Stephen and Grafsgaard, Joseph and D'Mello, Sidney},
  year = {2019},
  month = apr,
  pages = {1--14},
  doi = {10.1145/3290605.3300726},
  abstract = {We developed generalizable affect detectors using 133,966 instances of 18 affective states collected from 69,174 students who interacted with an online math learning platform called Algebra Nation over the entire school year. To enable scalability and generalizability, we used generic interaction features (e.g., viewing a video, taking a quiz), which do not require specialized sensors and are domain- and (to a certain extent) system-independent. We experimented with standard classifiers, recurrent neural networks, and genetically evolved neural networks for affect modeling. Prediction accuracies, quantified with Spearman's rho, were modest and ranged from .08 (for surprise) to .34 (for happiness) with a mean of .25. Our model trained on Algebra students generalized to a different set of Geometry students (n = 28,458) on the same platform. We discuss implications for scaling up affect detection for affect-sensitive online learning environments which aim to improve engagement and learning by detecting and responding to student affect.},
  isbn = {978-1-4503-5970-2}
}

@incollection{johnIrrelevantFeaturesSubset1994,
  title = {Irrelevant {{Features}} and the {{Subset Selection Problem}}},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  author = {John, George H. and Kohavi, Ron and Pfleger, Karl},
  year = {1994},
  pages = {121--129},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-335-6.50023-4},
  abstract = {We address the problem of nding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the de nitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present de nitions for irrelevance and for two degrees of relevance. These de nitions improve our understanding of the behavior of previous subset selection algorithms, and help de ne the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on arti cial and real datasets.},
  file = {/Users/aslak/Zotero/storage/KWMZQY9N/John et al. - 1994 - Irrelevant Features and the Subset Selection Probl.pdf},
  isbn = {978-1-55860-335-6},
  language = {en}
}

@misc{LowHighIndex,
  title = {The {{Low}}/{{High Index}} of {{Pupillary Activity}} | {{Proceedings}} of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  file = {/Users/aslak/Zotero/storage/KVIR3VAS/3313831.html},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/3313831.3376394}
}

@misc{MachineLearningArtificial,
  title = {Machine Learning and Artificial Intelligence Research for Patient Benefit: 20 Critical Questions on Transparency, Replicability, Ethics, and Effectiveness | {{The BMJ}}},
  file = {/Users/aslak/Zotero/storage/JSBTZ8TP/bmj.l6927.html},
  howpublished = {https://www.bmj.com/content/368/bmj.l6927.abstract}
}

@inproceedings{mcnallyPredictingPriceBitcoin2018,
  title = {Predicting the {{Price}} of {{Bitcoin Using Machine Learning}}},
  booktitle = {2018 26th {{Euromicro International Conference}} on {{Parallel}}, {{Distributed}} and {{Network}}-Based {{Processing}} ({{PDP}})},
  author = {McNally, Sean and Roche, Jason and Caton, Simon},
  year = {2018},
  month = mar,
  pages = {339--343},
  issn = {2377-5750},
  doi = {10.1109/PDP2018.2018.00060},
  abstract = {The goal of this paper is to ascertain with what accuracy the direction of Bitcoin price in USD can be predicted. The price data is sourced from the Bitcoin Price Index. The task is achieved with varying degrees of success through the implementation of a Bayesian optimised recurrent neural network (RNN) and a Long Short Term Memory (LSTM) network. The LSTM achieves the highest classification accuracy of 52\% and a RMSE of 8\%. The popular ARIMA model for time series forecasting is implemented as a comparison to the deep learning models. As expected, the non-linear deep learning methods outperform the ARIMA forecast which performs poorly. Finally, both deep learning models are benchmarked on both a GPU and a CPU with the training time on the GPU outperforming the CPU implementation by 67.7\%.},
  file = {/Users/aslak/Zotero/storage/BTI2A3WD/8374483.html},
  keywords = {ARIMA,Bitcoin,Deep Learning,GPU,Graphics processing units,Long Short Term Memory,Machine learning,Predictive models,Recurrent Neural Network,Task analysis,Time series analysis,Training}
}

@incollection{olsenUsingIntelligentTutoring2014,
  title = {Using an {{Intelligent Tutoring System}} to {{Support Collaborative}} as Well as {{Individual Learning}}},
  booktitle = {Intelligent {{Tutoring Systems}}},
  author = {Olsen, Jennifer K. and Belenky, Daniel M. and Aleven, Vincent and Rummel, Nikol},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and {Trausan-Matu}, Stefan and Boyer, Kristy Elizabeth and Crosby, Martha and Panourgia, Kitty},
  year = {2014},
  volume = {8474},
  pages = {134--143},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-07221-0_16},
  abstract = {Collaborative learning has been shown to be beneficial for older students, but there has not been much research to show if these results transfer to elementary school students. In addition, collaborative and individual modes of instruction may be better for acquiring different types of knowledge. Collaborative Intelligent Tutoring Systems (ITS) provide a platform that may be able to provide both the cognitive and collaborative support that students need. This paper presents a study comparing collaborative and individual methods while receiving instruction on either procedural or conceptual knowledge. The collaborative groups had the same learning gains as the individual groups in both the procedural and conceptual learning conditions but were able to do so with fewer problems. This work indicates that by embedding collaboration scripts in ITSs, collaborative learning can be an effective instructional method even with young children.},
  file = {/Users/aslak/Zotero/storage/VXZ67JJN/Olsen et al. - 2014 - Using an Intelligent Tutoring System to Support Co.pdf},
  isbn = {978-3-319-07220-3 978-3-319-07221-0},
  language = {en}
}

@inproceedings{raptisUsingEyeGaze2017,
  title = {Using {{Eye Gaze Data}} and {{Visual Activities}} to {{Infer Human Cognitive Styles}}: {{Method}} and {{Feasibility Studies}}},
  shorttitle = {Using {{Eye Gaze Data}} and {{Visual Activities}} to {{Infer Human Cognitive Styles}}},
  booktitle = {Proceedings of the 25th {{Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  author = {Raptis, George E. and Katsini, Christina and Belk, Marios and Fidas, Christos and Samaras, George and Avouris, Nikolaos},
  year = {2017},
  month = jul,
  pages = {164--173},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3079628.3079690},
  abstract = {Recent research provides evidence that individual differences in human cognitive styles affect user performance and experience in diverse application domains. However, state-of-the-art elicitation methods of cognitive styles require researchers to apply explicit, in-lab, and time-consuming "paper-and-pencil" techniques, rendering real-time integration of cognitive styles? elicitation impractical in interactive system design. Aiming to elaborate an implicit elicitation method of cognitive styles, this paper reports two feasibility studies based on an eye-tracking multifactorial model. In both studies, participants performed visual activities of varying characteristics, and the eye-tracking analysis revealed quantitative differences on visual behavior among individuals with different cognitive styles. Based on these differences, a series of classification experiments were conducted, and the results revealed that gaze-based implicit elicitation of cognitive styles in real-time is feasible, which could be used by interactive systems to adapt to the users' cognitive needs and preferences, to better assist them, and improve their performance and experience.},
  file = {/Users/aslak/Zotero/storage/3U6H8BCX/Raptis et al. - 2017 - Using Eye Gaze Data and Visual Activities to Infer.pdf},
  isbn = {978-1-4503-4635-1},
  keywords = {eye-tracking,human cognitive styles,user study,visual decision-making tasks,visual search tasks},
  series = {{{UMAP}} '17}
}

@inproceedings{raptisUsingEyeTracking2016,
  title = {Using {{Eye Tracking}} to {{Identify Cognitive Differences}}: {{A Brief Literature Review}}},
  shorttitle = {Using {{Eye Tracking}} to {{Identify Cognitive Differences}}},
  booktitle = {Proceedings of the 20th {{Pan}}-{{Hellenic Conference}} on {{Informatics}}},
  author = {Raptis, George E. and Fidas, Christos A. and Avouris, Nikolaos M.},
  year = {2016},
  month = nov,
  pages = {1--6},
  publisher = {{ACM}},
  address = {{Patras Greece}},
  doi = {10.1145/3003733.3003762},
  abstract = {Being the windows to the soul, eyes reveal information about individuals' feelings, emotions and behaviour, affecting various cognitive tasks, such as focus of attention, spatial cognition and navigation, cognitive load, etc. With the increased use of computer systems, complex information is visualized and communicated through visual interfaces as a mean of information presentation to and processing by the users. However, people differ regarding the way they seek, retrieve, process, comprehend, organize and recall information, based on their individual perceptual characteristics, cognitive skills, abilities and styles. Therefore, the point and the motion of the eye gaze could reveal behavioural patterns related to individual cognitive differences; patterns that are extracted using eye tracking tools which quantify and provide compelling data regarding eye gaze movement. In this paper we review the current literature regarding the effect between the FD-I cognitive style of users in visual exploration and search activities and to correlate these with objective measures gathered through eye-tracking.},
  file = {/Users/aslak/Zotero/storage/WITRISTZ/Raptis et al. - 2016 - Using Eye Tracking to Identify Cognitive Differenc.pdf},
  isbn = {978-1-4503-4789-1},
  language = {en}
}

@inproceedings{rogersGeneralizabilityDocumentFeatures2017,
  title = {Generalizability of {{Document Features}} for {{Identifying Rationale}}},
  booktitle = {Design {{Computing}} and {{Cognition}} '16},
  author = {Rogers, Benjamin and Justice, Connor and Mathur, Tanmay and Burge, Janet E.},
  editor = {Gero, John. S},
  year = {2017},
  pages = {633--651},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-44989-0_34},
  abstract = {One of the challenges in using statistical machine learning for text mining is coming up with the right set of text features. We have developed a system that uses genetic algorithms (GAs) to evaluate candidate feature sets to classify sentences in a document. We have applied this tool to find design rationale (the reasons behind design decisions) in two different datasets to evaluate our approach for finding rationale and to see how features might differ for the same classification target in different types of data. We used Chrome bug reports and transcripts of design sessions. We found that we were able to get results with less overfitting by using a smaller set of features common to the set optimized for each document type.},
  file = {/Users/aslak/Zotero/storage/33YKA6T9/Rogers et al. - 2017 - Generalizability of Document Features for Identify.pdf},
  isbn = {978-3-319-44989-0},
  keywords = {Information Gain,Linguistic Feature,Machine Learning Classifier,Sentence Length,Text Mining},
  language = {en}
}

@article{saariGeneralizabilitySimplicityCriteria2011,
  title = {Generalizability and {{Simplicity}} as {{Criteria}} in {{Feature Selection}}: {{Application}} to {{Mood Classification}} in {{Music}}},
  shorttitle = {Generalizability and {{Simplicity}} as {{Criteria}} in {{Feature Selection}}},
  author = {Saari, P. and Eerola, T. and Lartillot, O.},
  year = {2011},
  month = aug,
  volume = {19},
  pages = {1802--1812},
  issn = {1558-7924},
  doi = {10.1109/TASL.2010.2101596},
  abstract = {Classification of musical audio signals according to expressed mood or emotion has evident applications to content-based music retrieval in large databases. Wrapper selection is a dimension reduction method that has been proposed for improving classification performance. However, the technique is prone to lead to overfitting of the training data, which decreases the generalizability of the obtained results. We claim that previous attempts to apply wrapper selection in the field of music information retrieval (MIR) have led to disputable conclusions about the used methods due to inadequate analysis frameworks, indicative of overfitting, and biased results. This paper presents a framework based on cross-indexing for obtaining realistic performance estimate of wrapper selection by taking into account the simplicity and generalizability of the classification models. The framework is applied on sets of film soundtrack excerpts that are consensually associated with particular basic emotions, comparing Naive Bayes, k-NN, and SVM classifiers using both forward selection (FS) and backward elimination (BE). K-NN with BE yields the most promising results - 56.5\% accuracy with only four features. The most useful feature subset for k-NN contains mode majorness and key clarity, combined with dynamical, rhythmical, and structural features.},
  file = {/Users/aslak/Zotero/storage/B6SMWPYH/5676183.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {Accuracy,audio signal processing,backward elimination,Bayes methods,content-based music retrieval,cross-indexing,Cross-indexing,dimension reduction method,Emotion recognition,Feature extraction,feature selection,film soundtrack,forward selection,k-NN,Materials,MIR,Mood,mood classification,music,Music,music and emotion,music information retrieval,musical audio signal classification,musical features,naive Bayes,overfitting,Prediction algorithms,signal classification,support vector machines,SVM classifier,wrapper selection},
  number = {6}
}

@article{salminenMachineLearningApproach2019,
  title = {Machine Learning Approach to Auto-Tagging Online Content for Content Marketing Efficiency: {{A}} Comparative Analysis between Methods and Content Type},
  shorttitle = {Machine Learning Approach to Auto-Tagging Online Content for Content Marketing Efficiency},
  author = {Salminen, Joni and Yoganathan, Vignesh and Corporan, Juan and Jansen, Bernard J. and Jung, Soon-Gyo},
  year = {2019},
  month = aug,
  volume = {101},
  pages = {203--217},
  issn = {0148-2963},
  doi = {10.1016/j.jbusres.2019.04.018},
  abstract = {As complex data becomes the norm, greater understanding of machine learning (ML) applications is needed for content marketers. Unstructured data, scattered across platforms in multiple forms, impedes performance and user experience. Automated classification offers a solution to this. We compare three state-of-the-art ML techniques for multilabel classification - Random Forest, K-Nearest Neighbor, and Neural Network - to automatically tag and classify online news articles. Neural Network performs the best, yielding an F1 Score of 70\% and provides satisfactory cross-platform applicability on the same organisation's YouTube content. The developed model can automatically label 99.6\% of the unlabelled website and 96.1\% of the unlabelled YouTube content. Thus, we contribute to marketing literature via comparative evaluation of ML models for multilabel content classification, and cross-channel validation for a different type of content. Results suggest that organisations may optimise ML to auto-tag content across various platforms, opening avenues for aggregated analyses of content performance.},
  file = {/Users/aslak/Zotero/storage/F756SSAH/Salminen et al. - 2019 - Machine learning approach to auto-tagging online c.pdf;/Users/aslak/Zotero/storage/HSAPPN3B/S0148296319302607.html},
  journal = {Journal of Business Research},
  keywords = {Auto-tagging,Content marketing,Digital marketing,Machine learning,Neural network,Web content},
  language = {en}
}

@article{sharmaAssessingCognitivePerformance2020,
  title = {Assessing {{Cognitive Performance Using Physiological}} and {{Facial Features}}: {{Generalizing}} across {{Contexts}}},
  shorttitle = {Assessing {{Cognitive Performance Using Physiological}} and {{Facial Features}}},
  author = {Sharma, Kshitij and Niforatos, Evangelos and Giannakos, Michail and Kostakos, Vassilis},
  year = {2020},
  month = sep,
  volume = {4},
  pages = {95:1--95:41},
  doi = {10.1145/3411811},
  abstract = {Sensing and machine learning advances have enabled the unobtrusive measurement of physiological responses and facial expressions so as to estimate one's cognitive performance. This often boils down to mapping the states of the cognitive processes underpinning human cognition: physiological responses (e.g., heart rate) and facial expressions (e.g., frowning) often reflect the states of our cognitive processes. However, it remains unclear whether physiological responses and facial expressions used in one particular task (e.g., gaming) can reliably assess cognitive performance in another task (e.g., coding), because complex and diverse tasks often require varying levels and combinations of cognitive processes. In this paper, we measure the cross-task reliability of physiological and facial responses. Specifically, we assess cognitive performance based on physiological responses and facial expressions for 123 participants in 4 independent studies (3 studies for out-of-sampling training and testing, and 1 study for evaluation only): (1) a Pac-Man game, (2) an adaptive-assessment task, (3) a code-debugging task, and (4) a gaze-based game. We follow an ensemble learning approach after cross-training and cross-testing with all possible combinations of the 3 first datasets. We save the 4th dataset only for testing purposes, and we showcase how to engineer generalizable features that predict cognitive performance. Our results show that the extracted features do generalize, and can reliably predict cognitive performance across a diverse set of cognitive tasks that require different combinations of problem-solving, decision-making, and learning processes for their completion.},
  file = {/Users/aslak/Zotero/storage/PFRPWWCK/Sharma et al. - 2020 - Assessing Cognitive Performance Using Physiologica.pdf},
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  keywords = {Adaptive assessment,Cognitive performance,Debugging,Educational technology,Ensemble learning,Feature Generalizability,GARCH,Gaze-contingency,Learning Analytics,MMLA,Multimodal Learning Analytics,Skill-acquisition},
  number = {3}
}

@article{sharmaEyetrackingArtificialIntelligence2020,
  title = {Eye-Tracking and Artificial Intelligence to Enhance Motivation and Learning},
  author = {Sharma, Kshitij and Giannakos, Michail and Dillenbourg, Pierre},
  year = {2020},
  month = apr,
  volume = {7},
  pages = {13},
  issn = {2196-7091},
  doi = {10.1186/s40561-020-00122-x},
  abstract = {The interaction with the various learners in a Massive Open Online Course (MOOC) is often complex. Contemporary MOOC learning analytics relate with click-streams, keystrokes and other user-input variables. Such variables however, do not always capture users' learning and behavior (e.g., passive video watching). In this paper, we present a study with 40 students who watched a MOOC lecture while their eye-movements were being recorded. We then proposed a method to define stimuli-based gaze variables that can be used for any kind of stimulus. The proposed stimuli-based gaze variables indicate students' content-coverage (in space and time) and reading processes (area of interest based variables) and attention (i.e., with-me-ness), at the perceptual (following teacher's deictic acts) and conceptual levels (following teacher discourse). In our experiment, we identified a significant mediation effect of the content coverage, reading patterns and the two levels of with-me-ness on the relation between students' motivation and their learning performance. Such variables enable common measurements for the different kind of stimuli present in distinct MOOCs. Our long-term goal is to create student profiles based on their performance and learning strategy using stimuli-based gaze variables and to provide students gaze-aware feedback to improve overall learning process. One key ingredient in the process of achieving a high level of adaptation in providing gaze-aware feedback to the students is to use Artificial Intelligence (AI) algorithms for prediction of student performance from their behaviour. In this contribution, we also present a method combining state-of-the-art AI technique with the eye-tracking data to predict student performance. The results show that the student performance can be predicted with an error of less than 5\%.},
  file = {/Users/aslak/Zotero/storage/3Q5DN52Q/Sharma et al. - 2020 - Eye-tracking and artificial intelligence to enhanc.pdf;/Users/aslak/Zotero/storage/W397JKF6/s40561-020-00122-x.html},
  journal = {Smart Learning Environments},
  keywords = {Deep learning,Eye-tracking,Learning,Massive open online courses,MOOCs,Motivation,Multimodal analytics,Video based learning},
  number = {1}
}

@inproceedings{sharmaLookingLookingDual2015,
  title = {Looking {{AT}} versus {{Looking THROUGH}}: {{A Dual Eye}}-{{Tracking Study}} in {{MOOC Context}}},
  shorttitle = {Looking {{AT}} versus {{Looking THROUGH}}},
  booktitle = {{{CSCL}}},
  author = {Sharma, K. and Caballero, Daniela and Verma, Himanshu and Jermann, Patrick and Dillenbourg, P.},
  year = {2015},
  abstract = {We report the results from an eye-tracking study to show the differences in gaze patterns across the MOOC learners, while they watch a lecture individually as well as when they collaborate on an add-on activity. 98 university students took part in a study where they watched the MOOC video individually and later they collaboratively constructed a concept map. In both phases the gaze data was recorded. We compute two gaze measures: (1) with-me-ness, to quantify how much students follow the teacher during the video lecture, (2) gaze similarity, to quantify how much the pair looks at the same set of objects while collaborating. The analysis shows that both of the measures correlate significantly with the learning outcome. We argue that these results, conforming to our previous findings, indicate that the proposed gaze measures give a fairly accurate proxy to learners' engagement and performance.}
}

@article{sharmaMeasuringCausalityCollaborative2021,
  title = {Measuring Causality between Collaborative and Individual Gaze Metrics for Collaborative Problem-Solving with Intelligent Tutoring Systems},
  author = {Sharma, Kshitij and Olsen, Jennifer K. and Aleven, Vincent and Rummel, Nikol},
  year = {2021},
  volume = {37},
  pages = {51--68},
  issn = {1365-2729},
  doi = {10.1111/jcal.12467},
  abstract = {When students are working collaboratively and communicating verbally in a technology-enhanced environment, the system cannot track what collaboration is happening outside of the technology, making it difficult to fully assess the collaboration of the students and adapt accordingly. In this article, we propose using gaze measures as a proxy for cognitive processes to achieve collaboration awareness. Specifically, we use Granger causality to analyse the causal relationships between collaborative and individual gaze measures from students working on a fractions intelligent tutoring system and the influence that the students' dialogue, prior knowledge, or success has on these relationships. We found that collaborative gaze patterns drive the individual focus in the pairs with high posttest scores and when they are engaged in problem-solving dialogues but the opposite with low performing students. Our work adds to the literature by extending the correlational relationships between individual and collaborative gaze measures to causal relationships and suggests indicators that can be used within an adaptive system.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcal.12467},
  file = {/Users/aslak/Zotero/storage/PLKQVXHT/Sharma et al. - 2021 - Measuring causality between collaborative and indi.pdf;/Users/aslak/Zotero/storage/ZJQQYA7L/jcal.html},
  journal = {Journal of Computer Assisted Learning},
  keywords = {collaboration,collaborative learning,CSCL,dual eye-tracking,Granger causality,ITS},
  language = {en},
  number = {1}
}

@article{shojaeizadehDetectingTaskDemand2019,
  title = {Detecting Task Demand via an Eye Tracking Machine Learning System},
  author = {Shojaeizadeh, Mina and Djamasbi, Soussan and Paffenroth, Randy C. and Trapp, Andrew C.},
  year = {2019},
  month = jan,
  volume = {116},
  pages = {91--101},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2018.10.012},
  abstract = {Computerized systems play a significant role in today's fast-paced digital economy. Because task demand is a major factor that influences how computerized systems are used to make decisions, identifying task demand automatically provides an opportunity for designing advanced decision support systems that can respond to user needs at a personalized level. A first step for designing such advanced decision tools is to investigate possibilities for developing automatic task load detectors. Grounded in decision making, eye tracking, and machine learning literature, we argue that task demand can be detected automatically, reliably, and unobtrusively using eye movements only. To investigate this possibility, we developed an eye tracking task load detection system and tested its effectiveness. Our results revealed that our task load detection system reliably predicted increased task demand from users' eye movement data. These results and their implications for research and practice are discussed.},
  file = {/Users/aslak/Zotero/storage/XSSUTKWT/Shojaeizadeh et al. - 2019 - Detecting task demand via an eye tracking machine .pdf;/Users/aslak/Zotero/storage/CV5AWDFS/S0167923618301696.html},
  journal = {Decision Support Systems},
  keywords = {Adaptive decision making,Cognitive effort,Eye tracking,Human computer interaction,Machine learning,Task demand},
  language = {en}
}

@inproceedings{steichenUseradaptiveInformationVisualization2013,
  title = {User-Adaptive Information Visualization: Using Eye Gaze Data to Infer Visualization Tasks and User Cognitive Abilities},
  shorttitle = {User-Adaptive Information Visualization},
  booktitle = {Proceedings of the 2013 International Conference on {{Intelligent}} User Interfaces},
  author = {Steichen, Ben and Carenini, Giuseppe and Conati, Cristina},
  year = {2013},
  month = mar,
  pages = {317--328},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2449396.2449439},
  abstract = {Information Visualization systems have traditionally followed a one-size-fits-all model, typically ignoring an individual user's needs, abilities and preferences. However, recent research has indicated that visualization performance could be improved by adapting aspects of the visualization to each individual user. To this end, this paper presents research aimed at supporting the design of novel user-adaptive visualization systems. In particular, we discuss results on using information on user eye gaze patterns while interacting with a given visualization to predict the user's visualization tasks, as well as user cognitive abilities including perceptual speed, visual working memory, and verbal working memory. We show that such predictions are significantly better than a baseline classifier even during the early stages of visualization usage. These findings are discussed in view of designing visualization systems that can adapt to each individual user in real-time.},
  file = {/Users/aslak/Zotero/storage/HPIBKG9W/Steichen et al. - 2013 - User-adaptive information visualization using eye.pdf},
  isbn = {978-1-4503-1965-2},
  keywords = {adaptation,adaptive information visualization,eye-tracking,machine learning},
  series = {{{IUI}} '13}
}

@inproceedings{tokerPupillometryHeadDistance2017,
  title = {Pupillometry and {{Head Distance}} to the {{Screen}} to {{Predict Skill Acquisition During Information Visualization Tasks}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Toker, Dereck and Lall{\'e}, S{\'e}bastien and Conati, Cristina},
  year = {2017},
  month = mar,
  pages = {221--231},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3025171.3025187},
  abstract = {In this paper we investigate using a variety of behavioral measures collectible with an eye tracker to predict a user's skill acquisition phase while performing various information visualization tasks with bar graphs. Our long term goal is to use this information in real-time to create user-adaptive visualizations that can provide personalized support to facilitate visualization processing based on the user's predicted skill level. We show that leveraging two additional content-independent data sources, namely information on a user's pupil dilation and head distance to the screen, yields a significant improvement for predictive accuracies of skill acquisition compared to predictions made using content-dependent information related to user eye gaze attention patterns, as was done in previous work. We show that including features from both pupil dilation and head distance to the screen improve the ability to predict users' skill acquisition state, beating both the baseline and a model using only content-dependent gaze information.},
  file = {/Users/aslak/Zotero/storage/ZRC2QT4T/Toker et al. - 2017 - Pupillometry and Head Distance to the Screen to Pr.pdf},
  isbn = {978-1-4503-4348-0},
  keywords = {classification,distance to the screen,eye tracking,information visualization,pupil dilation,skill acquisition,user modeling},
  series = {{{IUI}} '17}
}

@inproceedings{turneyExploitingContextWhen1993,
  title = {Exploiting {{Context When Learning}} to {{Classify}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Machine Learning}}},
  author = {Turney, Peter D.},
  year = {1993},
  month = apr,
  pages = {402--407},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  isbn = {978-3-540-56602-1},
  series = {{{ECML}} '93}
}

@article{turneyIdentificationContextSensitiveFeatures2002,
  title = {The {{Identification}} of {{Context}}-{{Sensitive Features}}: {{A Formal Definition}} of {{Context}} for {{Concept Learning}}},
  shorttitle = {The {{Identification}} of {{Context}}-{{Sensitive Features}}},
  author = {Turney, Peter},
  year = {2002},
  month = dec,
  volume = {cs.LG/0212038},
  abstract = {A large body of research in machine learning is concerned with supervised learning from examples. The examples are typically represented as vectors in a multi-dimensional feature space (also known as attribute-value descriptions). A teacher partitions a set of training examples into a finite number of classes. The task of the learning algorithm is to induce a concept from the training examples. In this paper, we formally distinguish three types of features: primary, contextual, and irrelevant features. We also formally define what it means for one feature to be context-sensitive to another feature. Context-sensitive features complicate the task of the learner and potentially impair the learner's performance. Our formal definitions make it possible for a learner to automatically identify context-sensitive features. After context-sensitive features have been identified, there are several strategies that the learner can employ for managing the features; however, a disc...},
  file = {/Users/aslak/Zotero/storage/6TLPHGER/Turney - 2002 - The Identification of Context-Sensitive Features .pdf},
  journal = {CoRR}
}

@inproceedings{turneyRobustClassificationContextsensitive1993,
  title = {Robust Classification with Context-Sensitive Features},
  booktitle = {Proceedings of the 6th International Conference on {{Industrial}} and Engineering Applications of Artificial Intelligence and Expert Systems},
  author = {Turney, P. D.},
  year = {1993},
  month = jun,
  pages = {268--276},
  publisher = {{Gordon \& Breach Science Publishers}},
  address = {{Edinburgh, Scotland}},
  isbn = {978-2-88124-604-3},
  series = {{{IEA}}/{{AIE}}'93}
}

@article{weintraubCognitionAssessmentUsing2013,
  title = {Cognition Assessment Using the {{NIH Toolbox}}},
  author = {Weintraub, S. and Dikmen, S. S. and Heaton, R. K. and Tulsky, D. S. and Zelazo, P. D. and Bauer, P. J. and Carlozzi, N. E. and Slotkin, J. and Blitz, D. and {Wallner-Allen}, K. and Fox, N. A. and Beaumont, J. L. and Mungas, D. and Nowinski, C. J. and Richler, J. and Deocampo, J. A. and Anderson, J. E. and Manly, J. J. and Borosh, B. and Havlik, R. and Conway, K. and Edwards, E. and Freund, L. and King, J. W. and Moy, C. and Witt, E. and Gershon, R. C.},
  year = {2013},
  month = mar,
  volume = {80},
  pages = {S54-S64},
  issn = {0028-3878, 1526-632X},
  doi = {10.1212/WNL.0b013e3182872ded},
  file = {/Users/aslak/Zotero/storage/M3FV5HA4/Weintraub et al. - 2013 - Cognition assessment using the NIH Toolbox.pdf},
  journal = {Neurology},
  language = {en},
  number = {Issue 11, Supplement 3}
}

@book{zhangTuC4DriverCognitive,
  title = {{{TuC4}}.1 {{Driver Cognitive Workload Estimation}}: {{A Data}}-Driven {{Perspective}}},
  shorttitle = {{{TuC4}}.1 {{Driver Cognitive Workload Estimation}}},
  author = {Zhang, Yilu and Owechko, Yuri and Zhang, Jing},
  abstract = {Abstract \textemdash{} Driver workload estimation (DWE) refers to the activities of monitoring the driver and the driving environment in real-time and acquiring the knowledge of driver's workload continuously. With this knowledge of driver's workload, the in-vehicle information systems (IVIS) can provide information when the driver has the spare capacity to receive and comprehend it, which is both effective and efficient. However, after years of study, it is still difficult to build a robust DWE system. In this paper, we analyze the difficulties facing the existing methodology of developing DWE systems and propose a machine-learning-based DWE development process. Some preliminary but promising results are reported using a popular machine-learning method, decision tree. I.},
  file = {/Users/aslak/Zotero/storage/NTPLC3PB/Zhang et al. - TuC4.1 Driver Cognitive Workload Estimation A Dat.pdf;/Users/aslak/Zotero/storage/MD2RQX25/download.html}
}


