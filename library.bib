
@article{bednarikEMIPEyeMovements2020,
  title = {{{EMIP}}: {{The}} Eye Movements in Programming Dataset},
  shorttitle = {{{EMIP}}},
  author = {Bednarik, Roman and Busjahn, Teresa and Gibaldi, Agostino and Ahadi, Alireza and Bielikova, Maria and Crosby, Martha and Essig, Kai and Fagerholm, Fabian and Jbara, Ahmad and Lister, Raymond and Orlov, Pavel and Paterson, James and Sharif, Bonita and Sirki{\"a}, Teemu and Stelovsky, Jan and Tvarozek, Jozef and Vrzakova, Hana and {van der Linde}, Ian},
  year = {2020},
  month = oct,
  volume = {198},
  pages = {102520},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2020.102520},
  abstract = {A large dataset that contains the eye movements of N=216 programmers of different experience levels captured during two code comprehension tasks is presented. Data are grouped in terms of programming expertise (from none to high) and other demographic descriptors. Data were collected through an international collaborative effort that involved eleven research teams across eight countries on four continents. The same eye tracking apparatus and software was used for the data collection. The Eye Movements in Programming (EMIP) dataset is freely available for download. The varied metadata in the EMIP dataset provides fertile ground for the analysis of gaze behavior and may be used to make novel insights about code comprehension.},
  file = {/Users/aslak/Zotero/storage/ECVX59TH/Bednarik et al. - 2020 - EMIP The eye movements in programming dataset.pdf;/Users/aslak/Zotero/storage/W2YVK52Q/S0167642320301283.html},
  journal = {Science of Computer Programming},
  keywords = {Dataset,Eye-tracking,Program comprehension},
  language = {en}
}

@inproceedings{duchowskiLowHighIndex2020,
  title = {The {{Low}}/{{High Index}} of {{Pupillary Activity}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Duchowski, Andrew T. and Krejtz, Krzysztof and Gehrer, Nina A. and Bafna, Tanya and B{\ae}kgaard, Per},
  year = {2020},
  month = apr,
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376394},
  abstract = {A novel eye-tracked measure of pupil diameter oscillation is derived as an indicator of cognitive load. The new metric, termed the Low/High Index of Pupillary Activity (LHIPA), is able to discriminate cognitive load (vis-a-vis task difficulty) in several experiments where the Index of Pupillary Activity fails to do so. Rationale for the LHIPA is tied to the functioning of the human autonomic nervous system yielding a hybrid measure based on the ratio of Low/High frequencies of pupil oscillation. The paper's contribution is twofold. First, full documentation is provided for the calculation of the LHIPA. As with the IPA, it is possible for researchers to apply this metric to their own experiments where a measure of cognitive load is of interest. Second, robustness of the LHIPA is shown in analysis of three experiments, a restrictive fixed-gaze number counting task, a less restrictive fixed-gaze n-back task, and an applied eye-typing task.},
  file = {/Users/aslak/Zotero/storage/9LNGISLY/Duchowski et al. - 2020 - The LowHigh Index of Pupillary Activity.pdf},
  isbn = {978-1-4503-6708-0},
  keywords = {eye tracking,pupillometry,task difficulty},
  series = {{{CHI}} '20}
}

@inproceedings{duchowskiLowHighIndex2020a,
  title = {The {{Low}}/{{High Index}} of {{Pupillary Activity}}},
  author = {Duchowski, Andrew and Krejtz, Krzysztof and Gehrer, Nina and Bafna, Tanya and Baekgaard, Per},
  year = {2020},
  month = apr,
  pages = {1--12},
  doi = {10.1145/3313831.3376394}
}

@inproceedings{duchowskiLowHighIndex2020b,
  title = {The {{Low}}/{{High Index}} of {{Pupillary Activity}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Duchowski, Andrew T. and Krejtz, Krzysztof and Gehrer, Nina A. and Bafna, Tanya and B{\ae}kgaard, Per},
  year = {2020},
  month = apr,
  pages = {1--12},
  publisher = {{ACM}},
  address = {{Honolulu HI USA}},
  doi = {10.1145/3313831.3376394},
  abstract = {A novel eye-tracked measure of pupil diameter oscillation is derived as an indicator of cognitive load. The new metric, termed the Low/High Index of Pupillary Activity (LHIPA), is able to discriminate cognitive load (vis-\`a-vis task difficulty) in several experiments where the Index of Pupillary Activity fails to do so. Rationale for the LHIPA is tied to the functioning of the human autonomic nervous system yielding a hybrid measure based on the ratio of Low/High frequencies of pupil oscillation. The paper's contribution is twofold. First, full documentation is provided for the calculation of the LHIPA. As with the IPA, it is possible for researchers to apply this metric to their own experiments where a measure of cognitive load is of interest. Second, robustness of the LHIPA is shown in analysis of three experiments, a restrictive fixed-gaze number counting task, a less restrictive fixed-gaze n-back task, and an applied eye-typing task.},
  file = {/Users/aslak/Zotero/storage/9PLT64JQ/Duchowski et al. - 2020 - The LowHigh Index of Pupillary Activity.pdf},
  isbn = {978-1-4503-6708-0},
  language = {en}
}

@inproceedings{fritzUsingPsychophysiologicalMeasures2014,
  title = {Using Psycho-Physiological Measures to Assess Task Difficulty in Software Development},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Software Engineering}}},
  author = {Fritz, Thomas and Begel, Andrew and M{\"u}ller, Sebastian C. and {Yigit-Elliott}, Serap and Z{\"u}ger, Manuela},
  year = {2014},
  month = may,
  pages = {402--413},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2568225.2568266},
  abstract = {Software developers make programming mistakes that cause serious bugs for their customers. Existing work to detect problematic software focuses mainly on post hoc identification of correlations between bug fixes and code. We propose a new approach to address this problem --- detect when software developers are experiencing difficulty while they work on their programming tasks, and stop them before they can introduce bugs into the code. In this paper, we investigate a novel approach to classify the difficulty of code comprehension tasks using data from psycho-physiological sensors. We present the results of a study we conducted with 15 professional programmers to see how well an eye-tracker, an electrodermal activity sensor, and an electroencephalography sensor could be used to predict whether developers would find a task to be difficult. We can predict nominal task difficulty (easy/difficult) for a new developer with 64.99\% precision and 64.58\% recall, and for a new task with 84.38\% precision and 69.79\% recall. We can improve the Naive Bayes classifier's performance if we trained it on just the eye-tracking data over the entire dataset, or by using a sliding window data collection schema with a 55 second time window. Our work brings the community closer to a viable and reliable measure of task difficulty that could power the next generation of programming support tools.},
  file = {/Users/aslak/Zotero/storage/4C5J3B8V/Fritz et al. - 2014 - Using psycho-physiological measures to assess task.pdf},
  isbn = {978-1-4503-2756-5},
  keywords = {psycho-physiological,study,task difficulty},
  series = {{{ICSE}} 2014}
}

@misc{LowHighIndex,
  title = {The {{Low}}/{{High Index}} of {{Pupillary Activity}} | {{Proceedings}} of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  file = {/Users/aslak/Zotero/storage/KVIR3VAS/3313831.html},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/3313831.3376394}
}

@incollection{olsenUsingIntelligentTutoring2014,
  title = {Using an {{Intelligent Tutoring System}} to {{Support Collaborative}} as Well as {{Individual Learning}}},
  booktitle = {Intelligent {{Tutoring Systems}}},
  author = {Olsen, Jennifer K. and Belenky, Daniel M. and Aleven, Vincent and Rummel, Nikol},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and {Trausan-Matu}, Stefan and Boyer, Kristy Elizabeth and Crosby, Martha and Panourgia, Kitty},
  year = {2014},
  volume = {8474},
  pages = {134--143},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-07221-0_16},
  abstract = {Collaborative learning has been shown to be beneficial for older students, but there has not been much research to show if these results transfer to elementary school students. In addition, collaborative and individual modes of instruction may be better for acquiring different types of knowledge. Collaborative Intelligent Tutoring Systems (ITS) provide a platform that may be able to provide both the cognitive and collaborative support that students need. This paper presents a study comparing collaborative and individual methods while receiving instruction on either procedural or conceptual knowledge. The collaborative groups had the same learning gains as the individual groups in both the procedural and conceptual learning conditions but were able to do so with fewer problems. This work indicates that by embedding collaboration scripts in ITSs, collaborative learning can be an effective instructional method even with young children.},
  file = {/Users/aslak/Zotero/storage/VXZ67JJN/Olsen et al. - 2014 - Using an Intelligent Tutoring System to Support Co.pdf},
  isbn = {978-3-319-07220-3 978-3-319-07221-0},
  language = {en}
}

@misc{PapersGoFractions,
  title = {Papers to Go with Fractions and Cscw Datasets - Aslakhol@gmail.Com - {{Gmail}}},
  file = {/Users/aslak/Zotero/storage/N7ABRY6S/0.html},
  howpublished = {https://mail.google.com/mail/u/0/\#inbox/KtbxLwHLvfXTmHZQJTHGsqLmbQHtClzwxV?projector=1\&messagePartId=0.1}
}

@article{saariGeneralizabilitySimplicityCriteria2011,
  title = {Generalizability and {{Simplicity}} as {{Criteria}} in {{Feature Selection}}: {{Application}} to {{Mood Classification}} in {{Music}}},
  shorttitle = {Generalizability and {{Simplicity}} as {{Criteria}} in {{Feature Selection}}},
  author = {Saari, P. and Eerola, T. and Lartillot, O.},
  year = {2011},
  month = aug,
  volume = {19},
  pages = {1802--1812},
  issn = {1558-7924},
  doi = {10.1109/TASL.2010.2101596},
  abstract = {Classification of musical audio signals according to expressed mood or emotion has evident applications to content-based music retrieval in large databases. Wrapper selection is a dimension reduction method that has been proposed for improving classification performance. However, the technique is prone to lead to overfitting of the training data, which decreases the generalizability of the obtained results. We claim that previous attempts to apply wrapper selection in the field of music information retrieval (MIR) have led to disputable conclusions about the used methods due to inadequate analysis frameworks, indicative of overfitting, and biased results. This paper presents a framework based on cross-indexing for obtaining realistic performance estimate of wrapper selection by taking into account the simplicity and generalizability of the classification models. The framework is applied on sets of film soundtrack excerpts that are consensually associated with particular basic emotions, comparing Naive Bayes, k-NN, and SVM classifiers using both forward selection (FS) and backward elimination (BE). K-NN with BE yields the most promising results - 56.5\% accuracy with only four features. The most useful feature subset for k-NN contains mode majorness and key clarity, combined with dynamical, rhythmical, and structural features.},
  file = {/Users/aslak/Zotero/storage/B6SMWPYH/5676183.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {Accuracy,audio signal processing,backward elimination,Bayes methods,content-based music retrieval,cross-indexing,Cross-indexing,dimension reduction method,Emotion recognition,Feature extraction,feature selection,film soundtrack,forward selection,k-NN,Materials,MIR,Mood,mood classification,music,Music,music and emotion,music information retrieval,musical audio signal classification,musical features,naive Bayes,overfitting,Prediction algorithms,signal classification,support vector machines,SVM classifier,wrapper selection},
  number = {6}
}

@article{sharmaAssessingCognitivePerformance2020,
  title = {Assessing {{Cognitive Performance Using Physiological}} and {{Facial Features}}: {{Generalizing}} across {{Contexts}}},
  shorttitle = {Assessing {{Cognitive Performance Using Physiological}} and {{Facial Features}}},
  author = {Sharma, Kshitij and Niforatos, Evangelos and Giannakos, Michail and Kostakos, Vassilis},
  year = {2020},
  month = sep,
  volume = {4},
  pages = {95:1--95:41},
  doi = {10.1145/3411811},
  abstract = {Sensing and machine learning advances have enabled the unobtrusive measurement of physiological responses and facial expressions so as to estimate one's cognitive performance. This often boils down to mapping the states of the cognitive processes underpinning human cognition: physiological responses (e.g., heart rate) and facial expressions (e.g., frowning) often reflect the states of our cognitive processes. However, it remains unclear whether physiological responses and facial expressions used in one particular task (e.g., gaming) can reliably assess cognitive performance in another task (e.g., coding), because complex and diverse tasks often require varying levels and combinations of cognitive processes. In this paper, we measure the cross-task reliability of physiological and facial responses. Specifically, we assess cognitive performance based on physiological responses and facial expressions for 123 participants in 4 independent studies (3 studies for out-of-sampling training and testing, and 1 study for evaluation only): (1) a Pac-Man game, (2) an adaptive-assessment task, (3) a code-debugging task, and (4) a gaze-based game. We follow an ensemble learning approach after cross-training and cross-testing with all possible combinations of the 3 first datasets. We save the 4th dataset only for testing purposes, and we showcase how to engineer generalizable features that predict cognitive performance. Our results show that the extracted features do generalize, and can reliably predict cognitive performance across a diverse set of cognitive tasks that require different combinations of problem-solving, decision-making, and learning processes for their completion.},
  file = {/Users/aslak/Zotero/storage/PFRPWWCK/Sharma et al. - 2020 - Assessing Cognitive Performance Using Physiologica.pdf},
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  keywords = {Adaptive assessment,Cognitive performance,Debugging,Educational technology,Ensemble learning,Feature Generalizability,GARCH,Gaze-contingency,Learning Analytics,MMLA,Multimodal Learning Analytics,Skill-acquisition},
  number = {3}
}

@inproceedings{sharmaLookingLookingDual2015,
  title = {Looking {{AT}} versus {{Looking THROUGH}}: {{A Dual Eye}}-{{Tracking Study}} in {{MOOC Context}}},
  shorttitle = {Looking {{AT}} versus {{Looking THROUGH}}},
  booktitle = {{{CSCL}}},
  author = {Sharma, K. and Caballero, Daniela and Verma, Himanshu and Jermann, Patrick and Dillenbourg, P.},
  year = {2015},
  abstract = {We report the results from an eye-tracking study to show the differences in gaze patterns across the MOOC learners, while they watch a lecture individually as well as when they collaborate on an add-on activity. 98 university students took part in a study where they watched the MOOC video individually and later they collaboratively constructed a concept map. In both phases the gaze data was recorded. We compute two gaze measures: (1) with-me-ness, to quantify how much students follow the teacher during the video lecture, (2) gaze similarity, to quantify how much the pair looks at the same set of objects while collaborating. The analysis shows that both of the measures correlate significantly with the learning outcome. We argue that these results, conforming to our previous findings, indicate that the proposed gaze measures give a fairly accurate proxy to learners' engagement and performance.}
}

@article{sharmaMeasuringCausalityCollaborative2021,
  title = {Measuring Causality between Collaborative and Individual Gaze Metrics for Collaborative Problem-Solving with Intelligent Tutoring Systems},
  author = {Sharma, Kshitij and Olsen, Jennifer K. and Aleven, Vincent and Rummel, Nikol},
  year = {2021},
  volume = {37},
  pages = {51--68},
  issn = {1365-2729},
  doi = {10.1111/jcal.12467},
  abstract = {When students are working collaboratively and communicating verbally in a technology-enhanced environment, the system cannot track what collaboration is happening outside of the technology, making it difficult to fully assess the collaboration of the students and adapt accordingly. In this article, we propose using gaze measures as a proxy for cognitive processes to achieve collaboration awareness. Specifically, we use Granger causality to analyse the causal relationships between collaborative and individual gaze measures from students working on a fractions intelligent tutoring system and the influence that the students' dialogue, prior knowledge, or success has on these relationships. We found that collaborative gaze patterns drive the individual focus in the pairs with high posttest scores and when they are engaged in problem-solving dialogues but the opposite with low performing students. Our work adds to the literature by extending the correlational relationships between individual and collaborative gaze measures to causal relationships and suggests indicators that can be used within an adaptive system.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcal.12467},
  file = {/Users/aslak/Zotero/storage/PLKQVXHT/Sharma et al. - 2021 - Measuring causality between collaborative and indi.pdf;/Users/aslak/Zotero/storage/ZJQQYA7L/jcal.html},
  journal = {Journal of Computer Assisted Learning},
  keywords = {collaboration,collaborative learning,CSCL,dual eye-tracking,Granger causality,ITS},
  language = {en},
  number = {1}
}

@article{shojaeizadehDetectingTaskDemand2019,
  title = {Detecting Task Demand via an Eye Tracking Machine Learning System},
  author = {Shojaeizadeh, Mina and Djamasbi, Soussan and Paffenroth, Randy C. and Trapp, Andrew C.},
  year = {2019},
  month = jan,
  volume = {116},
  pages = {91--101},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2018.10.012},
  abstract = {Computerized systems play a significant role in today's fast-paced digital economy. Because task demand is a major factor that influences how computerized systems are used to make decisions, identifying task demand automatically provides an opportunity for designing advanced decision support systems that can respond to user needs at a personalized level. A first step for designing such advanced decision tools is to investigate possibilities for developing automatic task load detectors. Grounded in decision making, eye tracking, and machine learning literature, we argue that task demand can be detected automatically, reliably, and unobtrusively using eye movements only. To investigate this possibility, we developed an eye tracking task load detection system and tested its effectiveness. Our results revealed that our task load detection system reliably predicted increased task demand from users' eye movement data. These results and their implications for research and practice are discussed.},
  file = {/Users/aslak/Zotero/storage/XSSUTKWT/Shojaeizadeh et al. - 2019 - Detecting task demand via an eye tracking machine .pdf;/Users/aslak/Zotero/storage/CV5AWDFS/S0167923618301696.html},
  journal = {Decision Support Systems},
  keywords = {Adaptive decision making,Cognitive effort,Eye tracking,Human computer interaction,Machine learning,Task demand},
  language = {en}
}


