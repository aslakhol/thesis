:bibtex-file: library.bibtex
:bibtex-order: alphabetical
:bibtex-style: ieee

= Engineering Generalizable Features for Eye-Tracking Data Through a Cloud Based Machine Learning Platform
:toc:

== Introduction

=== Cognitive Workload

=== Feature Generalizability

Why do we want to generalize for features?

Machine learning often needs large quantities of data to get good results.
In some instances, you can still get good results with less data.
Generally you would then create a model that is specifically tailored to the context you are working in.
This would involve an investigation of which features  are best suited for that specific context.
Our hypothesis is that for a set of related contexts, there exists a set of features that would be useful for creating models in each of those contexts.
We will be following the Feature Generalizability Index methodology set out in Sharma et al to investigate the generalizability of several different features derived from EEG and Eye-tracking data.

If we want to be able to quickly build models for new contexts it will undoutedly be useful to have a deeper understand of which features will likely give good or decent results for your context. citenp:[sharmaAssessingCognitivePerformance2020]

Why is it different than transfer learning?

Feature generalizability isn't the only technique that can be used to approach the problem of more effeciently getting good predictions with less effort, or in data-poor environments.
Transfer learning is a popular approach which, through different techniques, train a model partially on a domain or context where there is a large amount of data avaialable, and then adapt the model to the context with less available data.
This technique differs from feature generalization in that feature generalization doesn't aim at providing those faced with these issues with a pretrained model, but rather some a priori knowledge about what features could lead to a good result.





In general machine learning problems you are optimizing a value

In machine learning problems where there is limited access to data, transfer learning is a popular approach. "Transfer learning and domain adaptation refer to the situation where what has been learned in one setting â€¦ is exploited to improve generalization in another setting". Transfer Learning

"Feature generalizability we define as the extent to which extracted features can predict the same variable in different context." [0] As in Sharma et al, the variable we are predicting in multiple contexts is Cognitive Performance, with features engineered from eye-tracking data.

== Related Work


=== Eye Tracking

=== EEG

=== Generalizability

== Datasets

=== EMIP

=== Jetris

== Implementation

Our goal with this system is to create a platform on which we can perform our feature generalizability experiments efficiently and consistently.

The system must also allow for full reproducibility of any experiments ran.

Problems that we want to solve:

* Cloud. We want to be able to run the system in the cloud. So that we can run multiple experiments in parallel and not be limited by our own devices.
* Handle multiple datasets
* Feature set as hyperparameters
* Reproducibility
* Multiple different feature types (heatmap/ts)
* Creating features

=== Cloud
Our cloud provider for this project is google cloud provider.



=== Reproducability
Our reproducibility strategy primarily consists of two different components.
The version-control tool, git; and the machine learning management tool comet.ml.

==== Git
Git keeps track of all versions of our source-code.
We have set up our system to demand that all local changes to the code be committed to git before a run in the cloud will be allowed.
We ensure that all our parameters are represented in the code. This ensures that we always know the state of the code responsible for each experiment.
When we run an experiment in the cloud we log the start parameters of the system and the hash associated with the commit.

==== comet.ml
comet.ml is a machine learning management tool. It can handle user-management, visualization, tracking of experiments, and much more.
In our case we use it to track the results of our experiements, and how they relate to eachother.



One of the primary complications is our need for the combination of different datasets.


== Analysis

=== Cross-Sudy Data Collection

=== Data Pre-Processing

=== Feature Extraction

=== Dimensinality Reduction

=== Prediction: Ensamble Learning

=== Training, validation and testing setup

=== Feature Generalizability Index

=== Bench-marking the Generalizable Features

== Results and Discussion

=== Selecting Generalizable Features

=== Engineering Generalizable Eye-Tracking Features

=== Engineering Generalizable EEG Features

=== On Feature Generalizablity

=== Bench-Mark Results for Generalizable Features

=== Context-Specific Features

=== Implications

=== Limitations

== Conclusion and Further Work


bibliography::[]
