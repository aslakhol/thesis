:bibtex-file: library.bibtex
:bibtex-order: alphabetical
:bibtex-style: ieee

= Engineering Generalizable Features for Eye-Tracking Data Through a Cloud Based Machine Learning Platform
:toc:

== Introduction

=== Cognitive Workload

=== Feature Generalizability

Why do we want to generalize for features?

Machine learning often needs large quantities of data to get good results.
In some instances, you can still get good results with less data.
Generally you would then create a model that is specifically tailored to the context you are working in.
This would involve an investigation of which features  are best suited for that specific context.
Our hypothesis is that for a set of related contexts, there exists a set of features that would be useful for creating models in each of those contexts.
We will be following the Feature Generalizability Index methodology set out in Sharma et al to investigate the generalizability of several different features derived from EEG and Eye-tracking data.

If we want to be able to quickly build models for new contexts it will undoutedly be useful to have a deeper understand of which features will likely give good or decent results for your context. citenp:[sharmaAssessingCognitivePerformance2020]

Why is it different than transfer learning?

Feature generalizability isn't the only technique that can be used to approach the problem of more effeciently getting good predictions with less effort, or in data-poor environments.
Transfer learning is a popular approach which, through different techniques, train a model partially on a domain or context where there is a large amount of data avaialable, and then adapt the model to the context with less available data.
This technique differs from feature generalization in that feature generalization doesn't aim at providing those faced with these issues with a pretrained model, but rather some a priori knowledge about what features could lead to a good result.





In general machine learning problems you are optimizing a value

In machine learning problems where there is limited access to data, transfer learning is a popular approach. "Transfer learning and domain adaptation refer to the situation where what has been learned in one setting â€¦ is exploited to improve generalization in another setting". Transfer Learning

"Feature generalizability we define as the extent to which extracted features can predict the same variable in different context." [0] As in Sharma et al, the variable we are predicting in multiple contexts is Cognitive Performance, with features engineered from eye-tracking data.

== Related Work



=== Eye Tracking
LHIPA citenp:[duchowskiLowHighIndex2020]

=== EEG

=== Generalizability


== Datasets

=== EMIP

=== Jetris

== Implementation

Our goal with this system is to create a platform on which we can perform our feature generalizability experiments efficiently and consistently.

The system must also allow for full reproducibility of any experiments ran.

Problems that we want to solve:

* Cloud. We want to be able to run the system in the cloud. So that we can run multiple experiments in parallel and not be limited by our own devices.
* Handle multiple datasets
* Feature set as hyperparameters
* Reproducibility
* Multiple different feature types (heatmap/ts)
* Creating features

=== Cloud
Our cloud provider for this project is google cloud provider.

AI-platform for running jobs
Google Cloud Storage for storing datasets and generated features


=== HP-tuning

Our pipelines are built with Scikit-learn pipelines which makes


=== Reproducability
Our reproducibility strategy primarily consists of two different components.
The version-control tool, git; and the machine learning management tool comet.ml.

==== Git
Git keeps track of all versions of our source-code.
We have set up our system to demand that all local changes to the code be committed to git before a run in the cloud will be allowed.
We ensure that all our parameters are represented in the code. This ensures that we always know the state of the code responsible for each experiment.
When we run an experiment in the cloud we log the start parameters of the system and the hash associated with the commit.

==== comet.ml
comet.ml is a machine learning management tool. It can handle user-management, visualization, tracking of experiments, and much more.
In our case we use it to track the results of our experiements, and how they relate to eachother.

Comet for hyperparameters

==== TS fresh

One of the primary complications is our need for the combination of different datasets.


== Analysis

=== Cross-Sudy Data Collection

=== Data Pre-Processing

We separate the preprocessing of the emip dataset in two parts, pre-preprocessing which is mostly quality of life changes to the dataset to make it easier to work with. And actual preprocessing for cleaning and normalzing the data.

==== EMIP dataset
We changed the dataset to make it easier to work with.

. Created a new column for the status for each timeframe cotaining "CALIBRATION", "READING", "TEST"
. Created a new column for which trial they were performing
. Removed rows for where the values were all 0, as that could be interpreted as nan.

Preprocessing

. Remove 0 values as they are nan
.

==== Generating Heatmaps
We used this and that for generating heatmpas

===== Mooc-images
We got the dataset

===== EMIP
The heatmaps for emip we generated ourselves with a python library called heatmappy. We used the preprocessed emip-dataset as explained in preprocessing.

. Split each subjects into 54 partitions to match the mooc-images dataset
. We only chose the datapoints where the subjects were reading code
. We took the average of the left and right position of the eye
. Created a 1920 * 1080 image
. Plotted the x,y postions with heatmappy
. Resized the image to 640*360

The emip-dataset is separated into two trials. We chose not to separate these trials since the heatmaps became to sparse when we did.

=== Feature Extraction

==== VGG19 Heatmaps

From the heatmaps used a pretrained vgg19 model with the imagenet weights to generate a feature vector of size 1000 features per image

1. Scale the images down using the preprocess_input function found in `keras.applications.image_netutils`
2. Use the pretrained VGG-19 model to extract features per image
3. Flatten the matrix to a single list of values

==== Powerspectrum

==== Arma

==== Garch

==== Markov models

==== LHIPA


=== Dimensionality Reduction

==== Lasso

=== Prediction: Ensamble Learning

=== Training, validation and testing setup

=== Feature Generalizability Index

=== Bench-marking the Generalizable Features

== Results and Discussion

=== Selecting Generalizable Features

=== Engineering Generalizable Eye-Tracking Features

=== Engineering Generalizable EEG Features

=== On Feature Generalizablity

=== Bench-Mark Results for Generalizable Features

=== Context-Specific Features

=== Implications

=== Limitations

== Conclusion and Further Work


bibliography::[]
