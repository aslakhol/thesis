:bibtex-file: library.bibtex
:bibtex-order: alphabetical
:bibtex-style: ieee

= Engineering Generalizable Features for Eye-Tracking Data Through a Cloud-Based Machine Learning Platform
:toc:

== Introduction

Eye-tracking has emerged as a promising non-invasive way to evaluate the effect a task has on a person.


We are developing a set of generalizable eye-tracking features that can predict cognitive performance and a machine learning platform to facilitate these features' testing and development.
Core to our methodology is the Feature Generalizability Index developed by Sharma et al. citenp:[sharmaAssessingCognitivePerformance2020]

=== Cognitive Workload

When engineering generalizable features, we are attempting to identify a relationship between the data and the factor we are trying to predict.
As such, it makes no sense to have generalizable features that can predict arbitrary labels.
We have decided to focus our efforts on predicting cognitive workload.

Cognitive workload describes the level of mental resources that one person requires for performing a task.
The perceived workload can be affected by many different factors such as the complexity of the task, the task's size, and external factors like how well one slept that night.
Cognitive workload is a valuable way of talking about how "heavy" a task is to complete.

Cognitive workload will be an essential concept in any system that helps users learn or make decisions.
For this reason, we have decided to focus our efforts on predicting cognitive workload.
However, cognitive workload is not a concrete metric; it is not something we can measure directly.

As an alternative, we have decided to work with cognitive performance.
By cognitive performance, we mean some measure that correlates to the quality of cognitive work.
Our project will include data gathered from studies handling many different cognitive tasks.
Therefore, we will argue that each task has some measure, such as a score, that correlates to the cognitive performance for that task.

=== Feature Generalizability
Feature generalizability is the degree to which a given feature, extracted from one context, is applicable in predictions on data gathered from other contexts.

When we are successful in predicting cognitive performance within one context, two things could be happening.
The first possibility is that we have identified some patterns or features in the dataset that correlate to cognitive performance within the experiment's context.
For example, suppose an exam-score is our measure of cognitive performance. In that case, we could assume that hours spent studying for that exam would be a good predictor of one's performance, with a relatively high degree of context specificity.
The other possibility is that we have found some pattern or feature that is directly related to cognitive performance without being linked closely to the context.
Studying for a specific test would probably give one good results on that test. However, being well-rested would be closely linked to one's performance while not closely linked to that particular test.

We hypothesize that when developing these generalizable features, some pattern in the eye-tracking data correlates directly to cognitive performance and not merely correlates given the specific context.
Our goal in this thesis will be to identify and engineer a set of features that exhibit this underlying relationship between themselves and cognitive performance.

So why would this be useful?
As a rule of thumb, machine learning needs sufficiently large datasets to provide good results.
However, there are certain domains where predictive power would be useful, but the necessary data is unavailable or hard to obtain.
(Maybe add some examples of these domains with references)
Feature generalizability could be a technique to utilize data gathered in separate but related contexts to achieve good results in even data-poor environments.

Transfer learning, another popular approach to data-poor contexts, is related to feature generalizability; however, they are distinct.
In transfer learning, through different techniques, one would train a model partially on a domain or context where there is a large amount of data available and then adapt that model to the context with less available data.

Another related but distinct technique from feature generalizability is the expert knowledge an experienced data scientist accumulates throughout several projects.
An experienced data scientist or a subject matter expert could have a priori knowledge about which features typically perform well for a given context or domain.

Feature generalizability could be said to exist in the space between these two approaches to the issue.
It is not developing a model adapted to the problem at hand when necessary. Neither is it not understanding which features would typically be good to use for a specific problem.
Feature generalizability is understanding which features could be extracted from one dataset and build models that could predict in another related dataset.


==== Feature Generalizability Index (FGI)

To measure feature generalizability, we will follow the method laid out by Sharma et al. citenp:[sharmaAssessingCognitivePerformance2020].
Their method provides us with a Feature Generalizability Index (FGI) calculated using ANOSIM (Analysis of similarity).
To measure how generalizable our features, we need a statistical test to see the similarities between the tests we run in our in-study and our out-of-study-experiments.
We have used NRMSE to measure the error in our predictions.
As there is no theoretical distribution that describes the NRMSE values, we need a non-parametric test to compare our two distributions.
The FGI method uses ANOSIM (Analysis of similarities) to do this.
ANOSIM is a non-parametric test that bears the null-hypothesis that two or more groups have a different mean and variance.
Our groups will be the NRMSE-values from the in-study-tests and the NRMSE values from the out-of-study-tests

== Related Work



=== Eye Tracking

Eye-tracking uses devices and software to track and record the position of a subject's eyes while interacting with digital devices. Eye-tracking can be used for input control or recording behavior during interactions with a system.

As the technology has improved and systems become cheaper and cheaper, eye-tracking has emerged as an effective, efficient, and cheap non-invasive method of tracking attention and cognitive workload and many other factors.

There are several different ways of performing eye-tracking. We are working with optical eye-trackers, which point the camera to the subject and record their pupils' position. The imagery is interpreted by software, and the eyes' positions are extracted, as well as any blinks and the pupillary response, how much the pupils dilate and trick. This information is recorded in the form of a time-series of the x and y position of where each subject's eyes are looking.

From this data, we can extract several features. The position of one's gaze on the page could itself be a valuable point of information, usually referred to as areas of interest.

Pupil dilation in and of itself has been shown to have direct relationships with how one processes data presented one is presented with. As such pupillary response over time is a promising feature. Blinking can, in the same way, give us some indication of how one is processing information.

A fixation in attracting is when your gaze rests on a particular point for a certain amount of time fixation would usually indicate a higher level of attention to that specific region of the screen.

Saccades are the rapid eye movement between two fixations. Information is not processed during a saccade. However, we can still learn something about how one processes information and the information being processed. For example, one would see a higher degree of saccades for texts that consist of longer and more complicated words.

The duration of the saccades and fixations, the lengths of saccades, and the relationship between saccades and fixations in the dataset can give us insight into how the subject processes information.

The features we are engineering in this thesis are primarily higher-order features built on top of the lower order features that we have just mentioned.



LHIPA citenp:[duchowskiLowHighIndex2020]



=== Generalizability


== Datasets

=== EMIP

One of the dates that we are working on is the EMIP dataset it's from a private study that deals with eye-movement in programming where they've had people of varying different skill levels in programming look at cold while tracking their islands

=== Jetris

=== CSCW

A dataset of students who were working in groups of 2 or 3.
They were first shown a video, which they watched at their own pace.
The videoplayer had the ability to speed up or slow down the video, and the students could jump around in the timeline if they so chose.
After watching the video they would create a concept map with the other students in their group.
They were given a set of terms from the video and would create a concept map that would describe the relationship between the terms.

While the task was cooperative, we are chosing to treat the data as individual, as all the measurements are individual.

The eyetracking data is split into two parts.
One part describes the data gathered during the video watching phase, and the other describes the data gathered during the concept mapping phase.

=== Fractions

== Implementation

Our goal with this system is to create a platform on which we can perform our feature generalizability experiments efficiently and consistently.

The system must also allow for full reproducibility of any experiments ran.

Problems that we want to solve:

* Cloud. We want to be able to run the system in the cloud. So that we can run multiple experiments in parallel and not be limited by our own devices.
* Handle multiple datasets
* Feature set as hyperparameters
* Reproducibility
* Multiple different feature types (heatmap/ts)
* Creating features

.These are the steps to our platform:
* Data pre-preprocessing
** Correct units (get everything do milliseconds)
** Move the data into buckets in gcp
** Fix or remove broken data
* Feature generation
** This is a seperate job that generates a large set of features from our specifications
** When completed it uploads the generated features to gcp
* Training and evaluation
** This step downloads all the features from gcp and trains our model with those features
** It trains and evaluates many models
** In the end the best model is chosen and everything is logged.


=== Cloud
Our cloud provider for this project is google cloud provider.

AI-platform for running jobs
Google Cloud Storage for storing datasets and generated features


=== HP-tuning

Our pipelines are built with Scikit-learn pipelines which makes


=== Reproducability
Our reproducibility strategy primarily consists of two different components.
The version-control tool, git; and the machine learning management tool comet.ml.

==== Git
Git keeps track of all versions of our source-code.
Our system is set up to demand that all local changes to the code be committed to git before a run in the cloud will be allowed.
We ensure that all our parameters are represented in the code. This in turn ensures that we always know the state of the code responsible for each experiment.
When we run an experiment in the cloud we log the start parameters of the system and the hash associated with the commit.

==== comet.ml
comet.ml is a machine learning management tool. It can handle user-management, visualization, tracking of experiments, and much more.
In our case we use it to track the results of our experiements, and how they relate to eachother.

Comet for hyperparameters

==== TS fresh

One of the primary complications is our need for the combination of different datasets.


== Analysis

=== Cross-Study Data Collection

=== Data Pre-Processing

We separate the preprocessing of the emip dataset in two parts, pre-preprocessing which is mostly quality of life changes to the dataset to make it easier to work with. And actual preprocessing for cleaning and normalzing the data.

==== EMIP dataset
We changed the dataset to make it easier to handle.

. Created a new column for the status for each timeframe cotaining "CALIBRATION", "READING", "TEST"
. Created a new column for which trial they were performing
. Removed rows for where the values were all 0, as that could be interpreted as nan.

Preprocessing

. Remove 0 values as they are nan
.

==== Generating Heatmaps
We used this and that for generating heatmaps

===== Mooc-images
We got the dataset

===== EMIP
The heatmaps for emip we generated ourselves with a python library called heatmappy. We used the preprocessed emip-dataset as explained in preprocessing.

. Split each subjects into 54 partitions to match the mooc-images dataset
. We only chose the datapoints where the subjects were reading code
. We took the average of the left and right position of the eye
. Created a 1920 * 1080 image
. Plotted the x,y postions with heatmappy
. Resized the image to 640*360

The emip-dataset is separated into two trials. We chose not to separate these trials since the heatmaps became to sparse when we did.

=== Feature Extraction

==== VGG19 Heatmaps

From the heatmaps used a pretrained vgg19 model with the imagenet weights to generate a feature vector of size 1000 features per image

1. Scale the images down using the preprocess_input function found in `keras.applications.image_netutils`
2. Use the pretrained VGG-19 model to extract features per image
3. Flatten the matrix to a single list of values

==== Powerspectrum

==== Arma

==== Garch

==== Markov models

==== LHIPA


=== Dimensionality Reduction

==== Lasso

=== Prediction: Ensamble Learning

=== Training, validation and testing setup

=== Feature Generalizability Index

=== Bench-marking the Generalizable Features

== Results and Discussion

=== Selecting Generalizable Features

=== Engineering Generalizable Eye-Tracking Features

=== Engineering Generalizable EEG Features

=== On Feature Generalizablity

=== Bench-Mark Results for Generalizable Features

=== Context-Specific Features

=== Implications

=== Limitations

== Conclusion and Further Work


bibliography::[]
