:bibtex-file: library.bib
:bibtex-order: alphabetical
:bibtex-style: ieee
:stem: asciimath

= Engineering Generalizable Features for Eye-Tracking Data Through a Cloud-Based Machine Learning Platform
:toc:

== Introduction

Eye-tracking has emerged as a promising non-invasive way to evaluate the effect a task has on a person.


We are developing a set of generalizable eye-tracking features that can predict cognitive performance and a machine learning platform to facilitate these features' testing and development.
Core to our methodology is the Feature Generalizability Index developed by Sharma et al. citenp:[sharmaAssessingCognitivePerformance2020]

=== Cognitive Workload

When engineering generalizable features, we are attempting to identify a relationship between the data and the factor we are trying to predict.
As such, it makes no sense to have generalizable features that can predict arbitrary labels.
We have decided to focus our efforts on predicting cognitive workload.

Cognitive workload describes the level of mental resources that one person requires for performing a task.
The perceived workload can be affected by many different factors such as the complexity of the task, the task's size, and external factors like how well one slept that night.
Cognitive workload is a valuable way of talking about how "heavy" a task is to complete.

Cognitive workload will be an essential concept in any system that helps users learn or make decisions.
For this reason, we have decided to focus our efforts on predicting cognitive workload.
However, cognitive workload is not a concrete metric; it is not something we can measure directly.

As an alternative, we have decided to work with cognitive performance.
By cognitive performance, we mean some measure that correlates to the quality of cognitive work.
Our project will include data gathered from studies handling many different cognitive tasks.
Therefore, we will argue that each task has some measure, such as a score, that correlates to the cognitive performance for that task.

=== Feature Generalizability
Feature generalizability is the degree to which a given feature, extracted from one context, is applicable in predictions on data gathered from other contexts.

When we are successful in predicting cognitive performance within one context, two things could be happening.
The first possibility is that we have identified some patterns or features in the dataset that correlate to cognitive performance within the experiment's context.
For example, suppose an exam score is our measure of cognitive performance. In that case, we could assume that hours spent studying for that exam would be a good predictor of one's performance, with a relatively high degree of context specificity.
The other possibility is that we have found some pattern or feature directly related to cognitive performance without being linked closely to the context.
Studying for a specific test would probably give one good results on that test. However, being well-rested would be closely linked to one's performance while not closely linked to that particular test.

We hypothesize that when developing these generalizable features, some pattern in the eye-tracking data correlates directly to cognitive performance and not merely correlates given the specific context.
Our goal in this thesis will be to identify and engineer a set of features that exhibit this underlying relationship between themselves and cognitive performance.

So why would this be useful?
As a rule of thumb, machine learning needs sufficiently large datasets to provide good results.
However, there are certain domains where predictive power would be helpful, but the necessary data is unavailable or hard to obtain.
(Maybe add some examples of these domains with references)
Feature generalizability could be a technique to utilize data gathered in separate but related contexts to achieve good results in even data-poor environments.

Transfer learning, another popular approach to data-poor contexts, is related to feature generalizability; however, they are distinct.
In transfer learning, through different techniques, one would train a model partially on a domain or context where there is a large amount of data available and then adapt that model to the context with less available data.

Another related but distinct technique from feature generalizability is the expert knowledge an experienced data scientist accumulates throughout several projects.
An experienced data scientist or a subject matter expert could have a priori knowledge about which features typically perform well for a given context or domain.

Feature generalizability could be said to exist in the space between these two approaches to the issue.
It is not developing a model adapted to the problem at hand when necessary. Neither is it not understanding which features would typically be good to use for a specific problem.
Feature generalizability is understanding which features could be extracted from one dataset and build models that could predict in another related dataset.


==== Feature Generalizability Index (FGI)

To measure feature generalizability, we will follow the method laid out by Sharma et al. citenp:[sharmaAssessingCognitivePerformance2020].
Their method provides us with a Feature Generalizability Index (FGI) calculated using ANOSIM (Analysis of similarity).
To measure how generalizable our features, we need a statistical test to see the similarities between the tests we run in our in-study and our out-of-study experiments.
We have used NRMSE to measure the error in our predictions.
As there is no theoretical distribution that describes the NRMSE values, we need a non-parametric test to compare our two distributions.
The FGI method uses ANOSIM (Analysis of similarities) to do this.
ANOSIM is a non-parametric test that bears the null hypothesis that two or more groups have a different mean and variance.
Our groups will be the NRMSE-values from the in-study-tests and the NRMSE values from the out-of-study-tests


== Related Work



=== Eye Tracking

Eye-tracking uses devices and software to track and record the position of a subject's eyes while interacting with digital devices. Eye-tracking can be used for input control or recording behavior during interactions with a system.

As the technology has improved and systems become cheaper and cheaper, eye-tracking has emerged as an effective, efficient, and cheap non-invasive method of tracking attention and cognitive workload and many other factors.

There are several different ways of performing eye-tracking. We are working with optical eye-trackers, which point the camera to the subject and record their pupils' position. The imagery is interpreted by software, and the eyes' positions are extracted, as well as any blinks and the pupillary response, how much the pupils dilate and trick. This information is recorded in the form of a time series of the x and y position of where each subject's eyes are looking.

From this data, we can extract several features. The position of one's gaze on the page could itself be a valuable point of information, usually referred to as areas of interest.

Pupil dilation in and of itself has been shown to have direct relationships with how one processes data presented one is presented with. As such pupillary response over time is a promising feature. Blinking can, in the same way, give us some indication of how one is processing information.

A fixation in attracting is when your gaze rests on a particular point for a certain amount of time fixation would usually indicate a higher level of attention to that specific region of the screen.

Saccades are the rapid eye movement between two fixations. Information is not processed during a saccade. However, we can still learn something about how one processes information and the information being processed. For example, one would see a higher degree of saccades for texts that consist of longer and more complicated words.

The duration of the saccades and fixations, the lengths of saccades, and the relationship between saccades and fixations in the dataset can give us insight into how the subject processes information.

The features we are engineering in this thesis are primarily higher-order features built on top of the lower order features that we have just mentioned.



LHIPA citenp:[duchowskiLowHighIndex2020]


=== Generalizability


== Datasets

We have been working with three different datasets gathered and published by other researchers.


=== EMIP

The Eye-Movements In Programming (EMIP) dataset is a large eye-tracking dataset collected as a community effort involving 11 research teams across four continents.
The goal was to provide a substantially large dataset open and free to stimulate research relating to development and eye-tracking.
216 programmers of differing experience levels were recorded while performing two code comprehension tasks.
In addition to the eye-tracking information, a wealth of metadata is also provided. citenp:[bednarikEMIPEyeMovements2020]

The recording was performed using a screen-mounted SMI RED25 mobile video-based eye tracker.
Stimuli were presented on a laptop computer screen with a resolution of 1920 x 1080 pixels. citenp:[bednarikEMIPEyeMovements2020]

The participants were primarily university students enrolled in computing courses but included academic and administrative staff and professional programmers.
There were 41 female participants and 175 male participants.
The mean age was 26.56 years with a standard deviation of 9.28. citenp:[bednarikEMIPEyeMovements2020]


=== CSCW

CSCW is a product of a study run by Sharma et al. citenp:[sharmaLookingLookingDual2015].
In this study, 98 university students partook in an exercise designed to show the different gaze patterns of learnings using a Massive Open Online Course (MOOC).

The study used a pre-test as a contextual primer.
The primer came in two different forms, one text-based test, and one schema-based test.
Participants that got the textual primer were denoted T, and participants that got the schematic primer were denoted S.

After the primer, participants watched a video from Khan Academy on the topic of "resting membrane potential."
Arranged in 16 TT pairs, 16 SS pairs, and 17 ST pairs, each pair collaborated to create a concept map using IHMC CMap tools.

The participants were recorded with SMI RED 250 eye-trackers as they completed the individual video task and the collaborative concept mapping task.
After completing the pre-test, the video task, and the concept map, the participants also completed a post-test.

While the concept mapping task was cooperative, all measurements are individual.
We will be working with the data on the individual level.
The data is also split into one part for the video watching phase and one part for the concept mapping phase.
We will not be considering any links between the two and will treat them as separate.


=== Fractions

The dataset that we refer to as fractions was gathered by Olson et al. citenp:[olsenUsingIntelligentTutoring2014].
It is an eye-tracking dataset from an experiment intending to investigate the differences between individual and collaborative performance when working on conceptually or procedurally oriented problems.
The study included 84 4th and 5th grades from two US elementary schools.
The students completed either individual tasks or collaborative tasks using an interactive tutoring system developed by the researchers.
Participants in the study also completed a pretest on the morning of the experiment, and a post-test the day after.
The results of the pre- and post-test are included with the data.

The students selected for the collaborative tasks were paired by their teachers to ensure that the students collaborate effectively.
They completed tasks in the interactive tutoring system, communicating verbally through a skype connection.
They did not transmit any video signal.

Our dataset consists of only the data used by Sharma et al. citenp:[sharmaMeasuringCausalityCollaborative2021] This only includes the data from the pairs that worked on the collaborative tasks, not the students that worked individually.

== Implementation

Our goal with this system is to create a platform on which we can perform our feature generalizability experiments efficiently and consistently.

In order to achieve this goal, multiple components have to be present.

. We need methods to standardize datasets, so the units are the same and the data is in the same form.
. We need to clean the data to achieve high data quality which can produce good features
. We need a platform that can generate computationally expensive features for multiple large datasets
. We need a platform that can run multiple concurrent pipelines for combinations of datasets, features, and methods for dimensionality reduction
. We need an evaluation step that collects the results from all the pipelines, and can prove pipelines generalizable.
. We need complete reproducibility.

=== Preprocessing

This subsection explains how we achieved goal 1 & 2 of creating a platform for generating generalizable features.

==== Standardization of Datasets
We have found three datasets from different experiments with different contexts.
They also vary in units used and the name of the columns.
Some of the datasets measure time in milliseconds, while others measure it in microseconds.
The datasets also use different names for the same attributes.
These were renamed to a consistent naming scheme.
Some of the subjects were missing labels, we solved this by removing the sample.
We also fixed inconsistencies such as wrong capitalizations of filenames.
The scripts for standardization can be found at Github. In misc/fix*

Something about us generating fixations for EMIP from rawdata

Below we describe in detail how we standardized each of the datasets.

===== EMIP
We changed the dataset to make it easier to handle.

. Created a new column for the status for each timeframe containing "CALIBRATION", "READING", "TEST"
. Created a new column for which trial they were performing
. Removed rows for where the values were all 0, as that could be interpreted as nan.

Preprocessing

. Remove 0 values as they are nan
.

==== Data cleaning
The datasets contains missing values


==== Normalization and Outlier removal
As our subjects comes from multiple contexts, the need for normalization and outlier removals is extra apparent.
The baseline for a subjects pupil dialation is very sensitive to lighting and how well rested you are, so it is important to normalize it.
We chose to min-max normalize the pupil diameter in the range of 0 to 1 per subject.

// The normalized x and y postitions is only used in the entropy feature so it should maybe be mentioned there
The screen sizes in the different experiments where the datasets were from are different. So we normalized the x and y positions in a 1000 by 1000 grid.

As we are working on fixations our sense of time is discretized to the start of each fixation.
But there can be artificially large periods of time between fixations, due to blinking, the subject looking away from the screen or technical malfunction on the equipment.
To mitigate this we remove the outliers by setting a threshold of 1000 ms for saccade duration, and all timegaps over 1000 ms were set to 1000ms,





=== Feature generation

To save computational time, we chose to separate the feature generation and the model training in to two separate jobs. This subsection explains how we achieved goal 3.

==== Flow
The flow of the feature generation job is as follows:

. Download the datasets from google cloud storage
. Load data as a list of pandas dataframes
. Standardize data. This step is different for each dataset
. Normalize data
. Generate aggregated attributes, such as saccade duration, saccade_length and angle of saccades.
. Take the rolling mean of the signals
. Generate features
. Upload features to google cloud storage

==== Features
The features we generate can be separated into 3 different groups based on how they were made.

* Timeseries Features
* Eyetracking Features
* Heatmap features

==== Timeseries Features
Agnostic features, they are a description of the signal, not the meaning behind the signal.
The signals we used are pupil diameter, fixation duration, saccade duration and saccade length.

Pupil diameter is the average diameter of the pupil over a fixation.
Fixation duration is the duration of a fixation, and is the difference between the endtime and starttime of a fixation.
Saccade duration is the time between two fixations.
Saccade length is the euclidiean distance between the coordinates of two fixations.

From these signals we calculate 5 features.


===== Power Spectral Histogram.
The power spectrum of a time series, decomposes the time series to the frequncies present in the signal, and the amplitude of each of these frequencies.
Once compouted, they can be represented as a histogram which is called the power spectral histogram.
 We computed the centroid, variance, skew and kurtosis of the power spectral histogram.

===== Autoregressive Moving Average model (Arma)
An ARMA process describes a time series with two polynomials.
The first of these polynomials describes the autoregressive part of the timeseries.
The second part describes the moving average.
Arma is formally described by the following formula.

Arma takes the historical values to account
Arma is a non-stationary process
Problem solving is also a non-stationary process, since your mental model always depends on the previous mental models of the problem.
Hence Problem solving follows can be described as an arma process.



asciimath:[X_t = sum_(j=1)^p phi_j X_(t-j) + sum_(i=1)^q theta_i epsilon_(t-i) + epsilon_t]



The features we extract from arma are extracted with the following algorithm

```
best_fit = null
for p up to 4
    for q up to 4
        fit = arma(timeseries, p, q)
        if(best_fit.aic > fit.aic)
            best_fit = fit
return best_fit["ar"], best_fit["ma"], best_fit["exog"]


```
===== Garch
Garch is similar to Arma, but it is applied to the variance of the data instead of the mean.

We extract features from Garch similar to how we extract features from Arma.
  homogenity principle, garch is more generalizable than arma.
Garch has proved to be a

```
best_fit = null
for p up to 4
    for q up to 4
        fit = garch(timeseries, p, q)
        if(best_fit.aic > fit.aic)
            best_fit = fit
return best_fit["mu"], best_fit["omega"], best_fit["alpha"], best_fit["gamma"], best_fit["beta"]
```


===== Markov Models
I don't know the reason behind testing markov models, so we need to work that out.

Discrete arma
presenting the present value as a function of the past values.
Is discrete, when you have more continuous information the mutual information of the predictable can decrease.


```
normalized_timeseries = (timeseries - min(timeseries)) / (max(timeseries) - min(timeseries))
discretized_timeseries = discretize timeseries in 100 bins
best_fit = null
for i up to 8
   fit = GaussianHMM(covariance_type="full").fit(discretized_timeseries, n_components=i)
   if(best_fit.aic > fit.aic)
           best_fit = fit
flat_transistion_matrix = best_fit.transition_matrix.flatten()
padded_transition_matrix = pad flat_transistion matrix with n zeroes so the length is 64
return padded_transition_matrix
```

===== The Low/High Index of Pupillary Activity (LHIPA)
LHIPA citenp:[duchowskiLowHighIndex2020] is an enhancement to the Index of Pupilary Activity, which is a metric to discriminate cognitive load from pupil diameter oscillation.

For simplicity we extracted the LHIPA metric from all the signals even though it is only proven to have value on pupil diameter signals.


==== Eyetracking features
These are features that are connected to the domain of eye-tracking and not signal processing features.


===== Ratio of Information Processing Ratio


Global information processing (GIP) is often synonymous with skimming text.
Your gaze is jumping around to larger sections of the screen, and not staying at a place for a longer time.
Which results in shorter fixations and longer saccades.

Local information processing (LIP) is the exactly opposite, your gaze is focusing on smaller areas for a longer duration and not moving around that much.

For this metric fixations are measured in time, while saccades are measured in distance.
This is because we are interested in how big the area you are moving around is, and for how long you are focusing on specific areas.

To compute the ratio, we divide GIP by LIP.

The following algorithm extracts the feature:

```
upper_threshold_saccade_length = get 75 percentile of saccade_lengths
lower_threshold_saccade_length = get 25 percentile of saccade_lengths
upper_threshold_fixation_duration = get 75 percentile of fixation_durations
lower_threshold_fixation_duration = get 25 percentile of fixation_durations

LIP = 0
GIP = 0
for saccade_length, fixation_duration in saccade_lengths, fixation_durations
    fixation_is_short = fixation_duration <= lower_threshold_fixation_duration
    fixation_is_long = upper_threshold_fixation_duration <= fixation_duration
    saccade_is_short = saccade_length <= lower_threshold_saccade_length
    saccade_is_long = upper_threshold_saccade_length <= saccade_length

    if fixation_is_long and saccade_is_short:
        LIP += 1
    elif fixation_is_short and saccade_is_long:
        GIP += 1

return GIP / (LIP + 1)
```





===== Skewness of saccade speed
Saccade velocity skewness has been shown to correlate with familiarity.
If the skewness is highly positive, that means that the overall speeds were high.
It means that you knew where to look.

Familiarity and expertise is different. You can know where to look, but have to think twice before doing it.

To calculate this feature we calculated the speed by dividing the saccade length on the saccade duration.
Then we got the skew of the distribution outputted.

===== Verticality of Saccades
Something about reasoning behind looking at horizontality of saccades.

To get the horizontality of saccades we get the angle between every fixation, with respect to the x axis. We do that with arctan2, which outputs the angle in radians between pi and -pi. Since we're only interested in the horizontality of the saccade, we take the absolute value of the angle.
```
radians = atan2(y2 - y1, x2 - x1)
return abs(radians)
```
To describe the horizontality of each point in a range between 0 and 1, we take take sinus of every angle.
```
for angle in angles
   angle = sin(angle)
```
Then we average all the sinus values.

```
verticality = angles.average()
```

===== Entropy of Gaze

We use the entropy of the gaze to compute the focus size of the subject. To calculate it we create a grid of 50 by 50 px bins. And placing each fixation in one of these bins based on which bin its x and y position corresponds to. When we have this grid we flatten it and take the entropy of the resulting distribution.

This tells us if the gaze was evenly spread over the whole screen, or if the subject was more focused on specific areas of the screen. For this feature we used the normalized values for x and y, to keep the number of bins consistent between datasets.

The following algorithm extracts the feature:
```
x_normalized = normalize x between 0 and 1000
y_normalized = normalize y between 0 and 1000

x_axis = [50, 100, 150 ... ,1000]
y_axis = [50, 100, 150 ... ,1000]
2d_histogram = 2d_histogram(xaxis, yaxis, x_normalized, y_normalized)
return entropy(2d_histogram.flatten())

```



==== Heatmaps
As shown in cite:[K paper about heatmaps], heatmaps of the gaze can predict performance in learning activities.

The heatmaps for emip we generated ourselves with a python library called heatmappy.

. Split each subjects into 30 partitions
. Created a 1920 * 1080 image
. Plotted the x,y postions with heatmappy
. Resized the image to 175*90

This will return a list of heatmaps per subject.

From the heatmaps used a pretrained vgg19 model with the imagenet weights to generate a feature vector of size 1000 features per image

1. Scale the images down using the preprocess_input function found in `keras.applications.image_netutils`
2. Use the pretrained VGG-19 model to extract features per image
3. Flatten the matrix to a single list of values

===== Pseudocode
```
frames = Split each subject into 30 partitions
features = []
for frame in frames
    image = image of with dimensions 1920, 1080
    heatmap = heatmappy.heatmap_on_img(frame.x_and_y_postions, image)
    scaled_down_heatmap = keras.applications.image_netutils(heatmap)
    heatmap_features = vgg19model.predict(scaled_down_heatmap)
    features.append(heatmap_feature.flatten())
return features.flatten()
```

==== Final Feature set
After feature generation these are the features that has been generated per subject.

include::feature_set_table.adoc[]

=== Classification

This section explains how we solved goal 4 and 5 of creating our platform.
We need a platform that can run multiple concurrent pipelines for combinations of datasets, features, and methods for dimensionality reduction

==== Flow

* Download features from Google Cloud Storage
*


==== Grouping of features

Since it would be too computationally exhaustive to test all combinations of the features, we separated them into 13 different feature groups, which we will test.

The features are divided in this way:

===== Feature Groups

include::feature_groups_table.adoc[]

=== Dimensionality Reduction / Feature Selection
All of our pipelines does either feature selection or Dimensionality reduction.
The method we use for dimensionality reduction is PCA and the one for Feature selection is Lasso.
For all pipelines we also use a zero-variance filter to remove the features that has no variance in its data

==== Dimensionality Reduction
Dimensionality reduction is the process of mapping the current feature space to another feature space with a lower dimensionality.
We use PCA for dimensionality reduction.


==== Feature Selection
Feature selection is the process of reducing the number of features used by removing features that give less information.
We use Lasso as our feature selection alorithm because it performs well when the number of samples is less than the number of features.
Which is often the case in our pipelines.
For instance the heatmap feature group contains 153600 features per subject.

=== Prediction: Ensemble Learning
Our classifier is a weighted voting ensemble with a KNN-regressor, Random forest regressor and a Support Vector regressor.
To find the weights we do a cross-validation and set the respective weights as 1 - Root Mean square error.

=== Training, validation and testing setup
To evaluate our pipelines we do two types of testing, *out-of-sampling-testing* and *out-of-study-testing*.

To perform out of sampling testing, we split our in study dataset in three parts.
Training, validation, and testing data.
The training data is used to train the models, the validation set is used weight the different classifiers in the voting ensemble.
The test data is to evaluate the model on unseen data.

Out of study testing is how we can evaluate the generalizibility of the pipeline.
We are using two out of three datasets to train and evaluate the model with out of sampling testing.
The out of study testing is done with the dataset we do not use in the in study testing.
If EMIP and fractions are used for in study, then cscw is used for out of study testing.
Since all the datasets are from different contexts, we can evaluate how our pipelines generalize over contexts.

==== Evaluation Criteira

All of our pipelines are evaluated with Root mean squared error.
Since the labels are normalized in a range from 0 to 1 before training, our RMSE is equivalent to NRMSE, so we can reliably compare the results from one dataset to another.
RMSE is calculated by the following formula.


:asciimath:[RMSE = sqrt ((sum_(i=1)^text(Number of samples) (text(predicted)_i - text(original)_i)^2) / text(Number of Samples))]

=== Feature Generalizability Index
I think we can wait with this section until we have decided on a metric. I want to incorporate random-baseline


=== Bench-marking the Generalizable Features




[%header,format=csv]
|===
// include::results_filtered.csv[]
|===

==== Reproducability
This section explains how we reached goal 6 of our platform.
* We need complete reproducibility.

Our reproducibility strategy primarily consists of two different components.
The version-control tool, git; and the machine learning management tool comet.ml.

==== Git
Git keeps track of all versions of our source-code.
Our system is set up to demand that all local changes to the code be committed to git before a run in the cloud will be allowed.
We ensure that all our parameters are represented in the code.
This in turn ensures that we always know the state of the code responsible for each experiment.
When we run an experiment in the cloud we log the start parameters of the system and the hash associated with the commit.

==== comet.ml
comet.ml is a machine learning management tool. It can handle user-management, visualization, tracking of experiments, and much more.
In our case we use it to track the results of our experiements, and how they relate to eachother.

==== Seeding
All of our experiments ran with seeded randomness. Our implementation for seeding

```
seed = 69420
np.random.seed(seed)
random.seed(seed)
os.environ["PYTHONHASHSEED"] = str(seed)
tf.random.set_seed(seed)
```

== Results and Discussion

== Conclusion and Further Work


bibliography::[]
