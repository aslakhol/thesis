[[discussion]]
== Discussion

=== Findings

- 2-to-1 pipelines are more generalizable than 1-to-1 pipelines.
- 2-to-1 pipelines where Fractions and CSCW are the in-study datasets produce more generalizable pipelines.
- GARCH, Power Spectral Histogram, heatmaps, Saccade Duration, Saccade Length, and HMM can produce generalizable pipelines.
- Pupil Diameter, Gaze Characteristics, LHIPA, and ARMA can produce context-sensitive pipelines, and we do not observe any tendencies to generalize.

=== Filtering on the Baseline

Our goal is to identify generalizable pipelines, and for that, we have the FGI measure.
FGI describes how similar the distribution of errors from out-of-sample testing and out-of-study testing and is a measure of generalizability.
However, it is not enough that the two distributions are similar.
Generalizability should be a measure of usable predictive power in outside contexts.
Random guesses would create identical distributions of error in out-of-sample testing and out-of-study testing, and as such, would have a good FGI value.
To counteract this, we chose to disregard all pipelines that do not outperform their respective baselines for the purpose of measuring generalizability.
We do this by filtering the set of completed baselines.

=== 2-to-1 versus 1-to-1

As shown in xref:distribution_fgi[] 2-to-1 pipelines tend to generalize better then 1-to-1 pipelines.
Combining two datasets from two separate contexts for training introduces more variability to the dataset.
The result is a model which has lower variance and generalizes better than those that use only one dataset for training.
These results align with the findings of Sharma et al. citenp:[sharmaAssessingCognitivePerformance2020].
In further discussions, we will focus on results from the 2-to-1 pipelines.

=== Generalizability of our Datasets

Our results indicate that pipelines that combine Fractions and CSCW for training and use EMIP for testing are more generalizable than pipelines where EMIP is included in the training set.
The EMIP dataset consists of gaze data recorded of people completing individual tasks.
All datasets include information from one or more context-specific tasks, but the Fractions and CSCW also have a collaborative aspect.
We theorize that pipelines trained on EMIP do worse at describing this collaborative aspect and that the variability introduced by the interactions that come with collaborative work assists those pipelines in generalizing.
Pinpointing the effectiveness of training on collaborative data when generalizing to contexts without a collaborative aspect is a promising area for more experimentation.

=== Generalizable Features

In this section, we will argue a few reasons we believe as to why the features generalize.

==== Heatmaps

We observe that heatmaps are among the generalizable features.
Our heatmap feature is the gaze-position for each subject over time, split into 30 partitions, converted to a heatmap, and fed to a pre-trained VGG19 model.
The resulting feature vector is the feature we call heatmaps.
While the stimulus is not included in the heatmap image, the stimulus is captured through the subject's interaction with it, as the gaze pattern will follow the stimulus.
The feature vector we call heatmaps represents how the gaze interacts with the given stimuli. Taking EMIP as an example, a heatmap will tell us the shape of the code being presented to the subject for all subjects.
However,  it will also tell us how much time a subject spends reading the syntactic structures versus the variable names or conditional logic.
The latter is a focus point for experienced coders.
Heatmaps generalize well because instead of just capturing how a subject interacts with their stimulus, it also captures their interaction patterns and how they relate to their presented stimuli.
In addition, heatmaps encode information about time and space, rather than just one of the dimensions.

==== GARCH, Hidden Markov Models and Power Spectral Histogram

GARCH is a statistical modeling technique used to model timeseries data.
Our data suggest that GARCH is the most generalizable feature group of the ones we have tested.
GARCH is models the variance of data and has the heteroscedasticity property.
Heteroscedasticity means that the random disturbance is different across the elements.
In our case, this means the variance of indicators of cognitive performance varies from point to point in the timeseries.
In different contexts, the timing of the indicators will vary based on the task and other factors.
This might explain why GARCH is generalizable across contexts.

Our results indicate that spectral histograms generalize pretty well.
Spectral histograms are the distribution of frequencies and their corresponding amplitudes.
Spectral histograms capture repetitiveness and hence might capture patterns of behavior exhibited by the subjects.
Repetitiveness indicates a flow that suggests that the subject is comfortable in their task; on the other hand, chaotic non-repetitive behavior might occur when one is not comfortable.
We suggest that this relationship should be represented across contexts and could be why spectral histograms seem to generalize quite well.

We do not know why HMM generalizes this is a mystery and a subject for future discussions with Sharma et al. citenp:[sharmaAssessingCognitivePerformance2020, sharmaEyetrackingArtificialIntelligence2020, sharmaLookingLookingDual2015, sharmaMeasuringCausalityCollaborative2021]

==== Saccade Duration and Saccade Length

Both saccade duration and saccade length seem to generalize.
Saccades are the movements of the eyes from one fixation to another.
As one example, both lower saccade durations and longer saccade lengths might indicate familiarity with the stimulus.
As discussed in xref:skewness_saccade_speed[] higher saccade speeds correlate with familiarity with the stimulus citenp:[pappasVisualAestheticsECommerce2018a].
Saccade speed is a function of saccade duration and saccade length; both factors map familiarity to some degree.
Since familiarity with the interface would be relevant in all tasks that our datasets contain, this might be why saccade duration and saccade length generalize.
De Luca et al. citenp:[delucaReadingWordsPseudowords2002] show that saccadic length increases when reading longer pseudowords.
According to this perspective, saccadic length correlates with perceived task difficulty.
Perceived task difficulty or increased cognitive workload will correlate to cognitive performance across contexts.

=== Context Specific Features

In this section, we will argue a few reasons we believe as to why the features are context-specific.

==== ARMA

Our results indicate that ARMA produces quite context-sensitive pipelines.
ARMA models the conditional mean of a timeseries, as opposed to the variance which GARCH models.
Intuitively the mean value would be more dependent on the measurement's context than the variance.
For example, an ARMA model would be impacted by a stimulus with longer distances between points of interest when modeling saccade length, while GARCH would not.

====  LHIPA and Pupil Diameter

LHIPA was developed to be an indicator of cognitive load citenp:[duchowskiLowHighIndex2020].
The cognitive load of a task will have many factors, but the specific complexities of the given task must undoubtedly be a part of those.
LHIPA likely models these complexities and other factors directly related to the task.
Our observations that LHIPA can produce context-sensitive pipelines are in line with this thinking.

Pupil diameter has been shown to be affected by several task-specific factors.
Both task difficulty and time limits have an impact on pupil dilation citenp:[shojaeizadehDetectingTaskDemand2019].
These are task-related factors, and as such, it might explain why pupil diameter seems to produce context-specific pipelines.

Shojaeizadeh et al. also point out that pupil size might convey information about variation in cognitive effort.
This factor seems more likely to generalize and is what we model with GARCH.
GARCH of pupil diameter is included in this feature group.
However, the group primarily consists of features that model the mean of pupil diameter.

==== Eye-Tracking Features

The eye-tracking feature tries to encapsulate different strategies for interacting with the stimulus.
The information processing ratio represents the tendency to skim text versus more focused reading.
A strategy of either skimming or focused reading might be more appropriate for a specific task, which might be why this indicates context-specificity.
However, this is not an entirely specific trait.
Some skill might be involved in picking the correct strategy when presented with a stimulus, and greater familiarity might lead to a faster transition to focused reading.

Entropy models the spread of the gaze over the stimulus, which might model the generalizable aspect of focus; however, it is also affected by the task design. The verticality of saccades is also certainly context-specific as it relies heavily on the nature of and how the stimulus is organized.


=== Limitations and Further Work

In xref:study_contexts[], we outline how we believe our datasets are representative of a significant portion of human cognition.
However, it would be presumptuous to say that three datasets from three different contexts could represent all of the cognitive processes.
Our goal has been to generalize between our three contexts, and we hope that our methods provide meaningful insights into how our one could create generalizable features for other contexts.
We do not mean to say that our features will generalize to any context.
Nevertheless, this is a first step that provides evidence on how gaze-related features provide a certain level of generalizability across three distinct and commonly employed contexts.

Our results show some indications that datasets from individual tasks generalize poorly to contexts that include collaborative work.
Had individual work been better represented in our data, we might be able to say more about how individual tasks generalize in general.
Ideally, we should have had at least one more dataset for individual tasks.

Our work assumes that cognitive performance can be characterized by labels in our datasets and represented in gaze data.
For our approach, we need an object, quantifiable, metric to assess cognitive performance, but as with many other things in cognition, the reality is likely more complex.

For complete external repeatability, we would ideally publish the data we used to perform our experiments.
However, the scope of our thesis project was such that it would be impossible to gather our own data to perform the analyses we have performed.
As a result, we had to turn to generous researchers who allowed us to work with their data, which means that the data is not ours to share.

Due to the considerable effort put into creating our experimental platform, it would be possible to expand the different pipeline components we test greatly.
In our work, we tested 22 features in 12 feature groups, three datasets in 9 combinations, two methods for reducing the feature space, and a single ensemble classifier.
While our tested features are quite exhaustive, we limited how many feature-space reduction methods we worked with and tested only a single ensemble classifier.
It would be possible to investigate the effects of other variants of these pipeline components on generalizability in further work.

While we can identify feature groups that can produce generalizable pipelines, we do not know how the individual features in each group affect the generalizability.
It is also likely that combinations of features from different groups would create very generalizable pipelines.
