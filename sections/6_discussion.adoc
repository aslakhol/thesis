[[discussion]]
== Discussion

=== Findings

- 2-to-1 pipelines are more generalizable than 1-to-1 pipelines.
- 2-to-1 pipelines where Fractions and CSCW are the in-study datasets produce more generalizable pipelines.
- GARCH, Power Spectral Histogram, heatmaps, Saccade Duration, Saccade Length, and HMM can produce generalizable pipelines.
- Pupil Diameter, Gaze Characteristics, LHIPA, and ARMA can produce context-sensitive pipelines, and we do not observe any tendencies to generalize.

=== Filtering on the Baseline

Our goal was to identify generalizable pipelines, and for that, we have the FGI measure described in xref:feature_generalizability_index[].
FGI describes how similar the distribution of errors from out-of-sample testing and out-of-study testing and is a measure of generalizability citenp:[sharmaAssessingCognitivePerformance2020].
However, it is not enough that the two distributions are similar.
Generalizability should be a measure of usable predictive power in contexts other than the training context.
Random guesses would create identical distributions of error in out-of-sample testing and out-of-study testing, and as such, would have a good FGI value.
To counteract this, we chose to disregard all pipelines that do not outperform their respective baselines for the purpose of measuring generalizability.

=== 2-to-1 versus 1-to-1

As shown in xref:distribution_fgi[] 2-to-1 pipelines tend to generalize better than 1-to-1 pipelines.
Combining two datasets from two separate contexts for training introduces more variability to the dataset.
By introducing more variability, we are optimizing for both bias and variance, which is crucial for generalizability citenp:[kidzinskiGeneralizabilityMOOCModels2016].
These results align with the findings of Sharma et al. citenp:[sharmaAssessingCognitivePerformance2020].
When two datasets are used for in-study, they contain more information, and naturally, the chance of finding more mutual information with the out-of-study dataset is greater.
In further discussions, we will focus on results from the 2-to-1 pipelines.

[[generalizability_of_our_datasets]]
=== Generalizability of our Datasets

xref:generalizable_pipelines[] indicates that pipelines that combine Fractions and CSCW for training and use EMIP for testing are more generalizable than pipelines where EMIP was included in the training set.
The EMIP dataset consists of gaze data recorded of people completing individual tasks.
All datasets include information from one or more context-specific tasks, but the Fractions and CSCW also have a collaborative aspect.
We theorize that pipelines trained on EMIP do worse at describing this collaborative aspect.
The variability introduced by the interactions that come with collaborative work assists those pipelines in generalizing.
There is a degree of division of labor in collaboration, which gives rise to individual tasks even in the collaborative context; as such, there is mutual information even between the datasets when training on collaborative tasks even when predicting individual tasks.
Pinpointing the effectiveness of training on collaborative data when generalizing to contexts without a collaborative aspect is a promising area for more experimentation.

=== Generalizable Features

xref:generalizable_pipelines[] presents our more generalizable pipelines.
In this section, we will explain why we believe the feature groups used in those pipelines generalize.

==== Heatmaps

We observe that Heatmaps are among the generalizable feature groups.
The feature group only contains one feature vector, the heatmaps.
Our heatmap feature is the gaze-position for each subject over time, split into 30 partitions, converted to a heatmap, and fed to a pre-trained VGG19 model.
The resulting feature vector is the feature we call heatmaps.
In a heatmap, the stimulus is captured through the subject's interaction with it, as the gaze pattern follows the stimulus.
While the stimulus is not included in the heatmap image, information about the stimulus is still encoded.
The feature vector we call heatmaps represents how the gaze interacts with the given stimulus.
Taking EMIP as an example, a heatmap will tell us the shape of the code being presented to the subject.
However, it will also tell us how much time a subject spends reading the syntactic structures versus the variable names or conditional logic.
The latter two are focus points for experienced coders.
Heatmaps generalize well because instead of just capturing how a subject interacts with their stimulus, it also captures their interaction patterns and how they relate to their presented stimulus.
The stimulus and the interaction pattern individually might be context-specific; however, the relationship between the two should generalize.
In addition, heatmaps encode information about time and space rather than just one of the dimensions.

==== GARCH and Spectral Histogram

GARCH is a statistical modeling technique used to model timeseries data.
Our data suggest that GARCH is the most generalizable feature group of the ones we have tested.
GARCH models the variance of data and has the heteroscedasticity property.
Heteroscedasticity means that the random disturbance is different across the elements.
In our case, this means the variance of indicators of cognitive performance varies from point to point in the timeseries.
In different contexts, the timing of the indicators will vary based on the task and other factors.
This might explain why GARCH is generalizable across contexts.

Our results indicate that spectral histograms generalize quite well.
Spectral histograms are the distribution of frequencies and their corresponding amplitudes.
Spectral histograms capture repetitiveness and hence might capture patterns of behavior exhibited by the subjects.
Repetitiveness indicates a flow that suggests that the subject is comfortable in their task; on the other hand, chaotic non-repetitive behavior might occur when one is not comfortable.
We suggest that this relationship should be represented across contexts and could be why spectral histograms seem to generalize.

==== Saccade Duration and Saccade Length

Both saccade duration and saccade length seem to generalize.
Saccades are the movements of the eyes from one fixation to another.
As discussed in xref:skewness_saccade_speed[] higher saccade speeds correlate with familiarity with the stimulus citenp:[pappasVisualAestheticsECommerce2018a].
Saccade speed is a function of saccade duration and saccade length; both factors map familiarity to some degree.
The fact that saccade duration and saccade length generalizes might be explained by the fact that familiarity with the interface is relevant for all tasks contained by our datasets.
xref:context_sensitive_pipelines[] shows that saccade length also displays context-sensitivity in some pipelines.
De Luca et al. citenp:[delucaReadingWordsPseudowords2002] show that saccadic length increases when reading longer pseudowords.
Due to human limitations on how quickly an eye can move, saccade length and saccade duration will correlate after a certain threshold of saccade velocity is met.
Reading code is analogous to reading pseudowords, which might explain why saccade length displays context sensitivity when EMIP is one of the designated in-study datasets.

==== All

In pipelines with the All feature group, we combine all features in one group.
Our results show that these pipelines capture both generalizable patterns and context-sensitive as seen xref:generalizable_pipelines[] and xref:context_sensitive_pipelines[].
The feature preferred by the regressor will be the feature with the most mutual information with the in-study datasets; when this feature also has mutual information with the out-of-study dataset, the pipeline can generalize.
Since the All feature group contain all the features, the generalizability of the pipeline is highly dependent on the selected in-study dataset xref:generalizability_of_our_datasets[].
As a result, we see pipelines with the All feature group both among the most generalizable and the most context-specific pipelines.

=== Context-Specific Features

xref:context_sensitive_pipelines[] presents our more context-sensitive pipelines.
In this section, we will explain why we believe the feature groups exhibit context sensitivity.

[[discuss_arma]]
==== ARMA

Our results indicate that ARMA produces quite context-sensitive pipelines.
ARMA models the conditional mean of a timeseries, as opposed to the variance which GARCH models.
Intuitively the mean value would be more dependent on the measurement's context than the variance.
For example, an ARMA model would be impacted by a stimulus with longer distances between points of interest when modeling saccade length, while GARCH would not.

====  LHIPA and Pupil Diameter

LHIPA was developed to be an indicator of cognitive load citenp:[duchowskiLowHighIndex2020].
Cognitive load consists of intrinsic, extraneous and germane loads citenp:[swellerCognitiveLoadProblem1988, swellerCognitiveArchitectureInstructional1998, swellerElementInteractivityIntrinsic2010].
The characteristics of the material determine intrinsic and extraneous cognitive load.
Intrinsic load is determined complexity of the information and extraneous load by the presentation of the material.
Both of these are context-specific factors.
On the other hand, the germane load is concerned with the subject's personal characteristics and would likely be more generalizable.
Our observations that LHIPA can produce context-sensitive pipelines indicate that LHIPA captures germane cognitive loads to a lesser degree.

Pupil diameter has been shown to be affected by several task-specific factors.
Both task difficulty and time limits have an impact on pupil dilation citenp:[shojaeizadehDetectingTaskDemand2019].
These are task-related factors, and as such, it might explain why pupil diameter seems to produce context-specific pipelines.

Shojaeizadeh et al. also point out that pupil size might convey information about variation in cognitive effort.
This factor seems more likely to generalize and is what we model with GARCH.
GARCH of pupil diameter is included in this feature group.
However, the group primarily consists of features that model the mean of the pupil diameter.

==== Gaze Characteristics

Gaze Characteristics is the group of features that are interpreted directly from the eye-tracking data and are not subject to additional signal processing.
The feature group tries to encapsulate different strategies for interacting with the stimulus.
The information processing ratio represents the tendency to skim text versus more focused reading.
A skimming or focused reading strategy might be more appropriate for a specific task, which might be why this indicates context-specificity.
However, this is not an entirely specific trait.
There might be some skill involved in picking the correct strategy when presented with a stimulus, and greater familiarity might lead to a faster transition to focused reading.

Entropy models the spread of the gaze over the stimulus, which might model the generalizable aspect of focus; however, it is also affected by the task design.
The verticality of saccades is also certainly context-specific as it relies heavily on the nature of and how the stimulus is organized.


=== Limitations and Further Work

In xref:study_contexts[], we outline how we believe our datasets are representative of a significant portion of human cognition.
However, it would be presumptuous to say that three datasets from three different contexts could represent all of the cognitive processes.
Our goal has been to generalize between our three contexts, and we believe that our methods provide meaningful insights into how one could create generalizable features for other contexts.
We do not mean to say that our features will generalize to any context.
Nevertheless, this is a first step that provides evidence on how gaze-related features provide a certain level of generalizability across three distinct and commonly employed contexts.

xref:generalizable_pipelines[] and xref:context_sensitive_pipelines[] show some indications that datasets from individual tasks generalize poorly to contexts that include collaborative work.
Had individual work been better represented in our data, we might be able to say more about how individual tasks generalize in general.
Ideally, we should have had at least one more dataset for individual tasks.

Our work assumes that cognitive performance can be characterized by labels in our datasets and represented in gaze data.
For our approach, we need an object, quantifiable metric to assess cognitive performance, but as with many other things in cognition, the reality is likely more complex.

For complete external repeatability, we would ideally publish the data we used to perform our experiments.
However, the scope of our thesis project was such that it would be impossible to gather our own data to perform the analyses we have performed.
As a result, we had to turn to generous researchers who allowed us to work with their data, which in turn means that the data is not ours to share.

Due to the considerable effort put into creating our experimental platform, it would be possible to expand the different pipeline components we test greatly.
In our work, we tested 22 features in 12 feature groups, three datasets in 9 combinations, two methods for reducing the feature space, and a single ensemble classifier.
While our tested features are quite exhaustive, we limited how many feature-space reduction methods we worked with and tested only a single ensemble classifier.
It would be possible to investigate the effects of other variants of these pipeline components on generalizability in further work.

While we can identify feature groups that can produce generalizable pipelines, we do not know how the individual features in each group affect the generalizability.
It is also likely that combinations of features from different groups would create very generalizable pipelines.

The Hidden Markov Models (HMM) are included in both xref:generalizable_pipelines[] and xref:context_sensitive_pipelines[].
That HMMs generalize seems counter-intuitive, especially given that ARMA does not generalize (see xref:discuss_arma[]).
At its core, the transition matrix of HMM represents a discrete version of ARMA.
ARMA models how previous values in a timeseries affect the current value, while HMMs describe how previous states affect the current state.
What dataset was used might be a significant contributing factor to why HMMs either generalize or exhibit context-specificity; However, more research is needed to draw any conclusions.
