[[study]]
== Research Design

The study design, described below, is inspired by the study performed by Sharma et al. citenp:[sharmaAssessingCognitivePerformance2020].

We have selected three datasets used in published articles that all include eye-tracking data from subjects completing one or more cognitive tasks.
The studies that produced the datasets were distinct, and their differences allow us to argue that the datasets have different contexts.
With these datasets, we generate several different pipelines consisting of different feature groups, different dimensionality reduction or feature selection techniques, and different combinations of the datasets.
These pipelines are then evaluated using both *out-of-sample testing* to determine the predictive power on unseen data and *out-of-study testing* to assess the generalizability of the pipeline.

For each pipeline, we designate either one or two datasets as in-study and one dataset as out-of-study.
In the cases where two datasets are selected as in-study, they are combined into one larger dataset.
If we use only one dataset as in-study, we do not include the third dataset in that pipeline.

We split the in-study dataset into three parts: training, validation, and testing data by leave-one-participant-out.
The training data is used to train the model; the validation data is used to set weights for the classifiers in the voting ensemble, and the testing data is used to evaluate the predictive power of each pipeline on unseen data.
The testing of the pipeline on unseen data from the same dataset(s) is our *out-of-sample testing*.

Our next step is the *out-of-study-testing*.
In this step, we use the model outputted from a pipeline to make predictions on data gathered in separate contexts.
This allows us to analyze the generalizability of the pipeline.


=== Datasets

We have been working with three different datasets gathered and published by other researchers.
They are all gathered during cognitive tasks and, as such, are suitable for investigating cognitive workload.
We have used the shorthand names EMIP, CSCW, and Fractions to refer to the datasets.
They are further described in the following section.


==== EMIP

The Eye-Movements In Programming (EMIP) dataset is a large eye-tracking dataset collected as a community effort involving 11 research teams across four continents.
The goal was to provide a substantially large dataset open and free to stimulate research relating to programming and eye-tracking citenp:[bednarikEMIPEyeMovements2020].

===== Participants

Each site recruited participants by opportunity sampling.
Data from 216 participants at differing levels of expertise is included in the dataset.
There were 41 female and 175 male participants; their mean age was 26.56 with a standard deviation of 9.28.
The participants were primarily university students enrolled in computing courses but included academic and administrative staff and professional programmers citenp:[bednarikEMIPEyeMovements2020].


===== Tasks

The participants were recorded while performing two code comprehension tasks.
They filled out a questioneer where they self-reported their level of programming expertise from the options none, low, medium, and high, recorded years of programming experience.
In addition, they answered several other questions that make up the metadata that accompanies the eye-tracking data.
The participants also selected the language to be used in the task, from Java, Scala, or Python.

After the questioner, the eye-tracker was calibrated, and the code comprehension task was started.
The task consisted of reading two different pieces of code called Vehicle and Rectangle, each comprising 11-22 code lines.
After reading and trying to understand a piece of code, the participants would indicate that they were ready to proceed by pressing the space bar.
Next, the system presented the participants with a multiple-choice question that evaluated code comprehension.
After completing the question for Vehicle, they were presented with the code for Rectangle and subsequently its accompanying question.

The rectangle code consists of a class representing a rectangle, instantiated through a constructer that takes four coordinates.
The class also has methods to width, height, and area.
In the _main_ function of the class, two rectangles are instantiated, and their areas are printed.
The code was based on a code comprehension task developed by Hansen citenp:[hansenQuantifyingProgramComplexity], and the EMIP researchers translated code from python two Scala, and Java.
The vehicle code is a class that represents a vehicle with several variables and a function to accelerate the car.
A constructor takes a producer, a type, and a top speed.
In the _main_ function, a vehicle is created, and its speed is accelerated citenp:[bednarikEMIPEyeMovements2020].

===== Technology and experimental setup

The recording was performed using a screen-mounted SMI RED25 mobile video-based eye tracker.
The tracker has a sample rate of 250 Hz with an accuracy < 0.4◦ and a precision of ≈ 0.03◦ of visual angle.
Stimuli were presented on a laptop computer screen with a resolution of 1920 x 1080 pixels and were viewed without a headrest in a fashion that closely simulated a familiar programming environment.
The data collection procedure was implemented in the SMI Experimental Suite software.
The laptop, eye-tracker, and software were shipped between the locations to minimize differences in setup citenp:[bednarikEMIPEyeMovements2020].

===== Description of the data

Below is a list of the contest of the dataset provided by Bednarik et al. citenp:[bednarikEMIPEyeMovements2020].

.Contents of the dataset
* rawdata: a folder with 216 TSV files containing raw eye movement data.
* stimuli: screenshots of the experiment slides in JPG-format and CSV files with AOI (area of interest) coordinates for the stimulus program
* metadata: a CSV file with information with background information on the participants, results from the questioner, and the order in which the stimulus programs were shown.
* data: TXT file specifying when the dataset was uploaded.

==== CSCW

The CSCW dataset was gathered during the Computer-Supported Cooperative Work and Social Computing course at École Polytechnique fédérale de Lausanne.
The study intended to show the different gaze patterns across learners in Massive Open Online Courses (MOOCS) and study the priming effect on learning citenp:[sharmaLookingLookingDual2015].

===== Participants

The dataset contains gaze data from 98 students in the CSCW course at EPFL.
There were 49 participants in each of the two priming groups citenp:[sharmaLookingLookingDual2015].

===== Tasks

The experiment centered around a collaborative task with a contextual primer.
Participants were presented with pre-tests, which also served as primers.
They were given either a schematic or a text-based primer.
The textual primer had questions described in text-form, while the schematic primer had the same questions but presented as a schema.
Participants that received the textual primer were called T, and participants that received the schematic primer were called S.

After the primer, participants watched a video from Khan Academy citenp:[KhanAcademyFree] on the topic of "resting membrane potential."
Arranged in 16 TT pairs, 16 SS pairs, and 17 ST pairs, each team collaborated to create a concept map using IHMC CMap tools citenp:[CmapCmapTools].
Lastly, the participants also completed a post-test citenp:[sharmaLookingLookingDual2015].

===== Technology and experimental setup

Gaze data was recorded using SMI RED250 eye trackers citenp:[SMIRED250IMotions].
The participants were while they completed watched the video and during the collaborative concept mapping task citenp:[sharmaLookingLookingDual2015].

===== Description of the data

The dataset contains two files with gaze data for each participant.
One file describes the video watching phase, and one part describes the concept mapping phase.
We will not be considering any links between the two and will treat them as separate.
While the concept mapping task was cooperative, all measurements are individual.
We will be working with the data on the individual level citenp:[sharmaLookingLookingDual2015].

==== Fractions
The dataset that we refer to as Fractions was gathered by Olson et al. citenp:[olsenUsingIntelligentTutoring2014].
It is an eye-tracking dataset from an experiment intending to investigate the differences between individual and collaborative performance when working on conceptually or procedurally oriented problems in a intelligent tutoring system (ITS) designed to teach fractions.

===== Participants

The study was conducted with 84 4th and 5th graders from two US elementary schools in the same school district.
The students left their regular instruction during the school day to participate in the study.
Teachers from the student's classes paired the students based on their mathematical abilities and who would work well together.
Before participating in the experiment, the students worked with the Fractions Tutor during two of their regular classes to acclimatize them to the software.
The pairs of students were randomly assigned to four groups completing different tasks.
They where: collaborative conceptual, collaborative procedural, individual conceptual and individual procedural.
Twice as many pairs were assigned the collaborative tasks as the individual citenp:[olsenUsingIntelligentTutoring2014].

===== Tasks

Olsen et al. hypothesized that students working collaboratively would show learning gains on both procedural and conceptual tasks, and that of those working on conceptual tasks, students working collaboratively would have stronger learning gains than those working individually.
They also hypothesized that students working individually would have greater learning gains than those working cooperatively for procedural tasks.

To investigate these hypotheses the pairs of students worked with their assigned tasks in an ITS.
The tasks used different techniques to assist the students in learning equivalent fractions.
Participants also completed a pre-test on the morning of the experiment and a post-test the next day citenp:[olsenUsingIntelligentTutoring2014].

===== Technology and experimental setup

Students participating in the study completed their tasks in an interactive tutoring system developed by the researchers.
They communicated verbally through a skype connection.
No video signal was transmitted.
Gaze data recorded using SMI RED250 eye trackers citenp:[SMIRED250IMotions, olsenUsingIntelligentTutoring2014].

===== Description of the data

The data includes individual files with gaze data for each student as well as a file describing all the results from the pre and post-tests.
Our dataset consists of only the data used by Sharma et al. citenp:[sharmaMeasuringCausalityCollaborative2021].
This only includes the data from the pairs that worked on the collaborative tasks, not the students that worked individually.

[[study_contexts]]
=== Contexts

Our work seeks to investigate generalizability between specific contexts; thus, we must be aware of our contextual biases.
We have gathered datasets that we consider to cover a significant spectrum of cognitive processes.

EMIP is an individual task that is about reading and understanding programming code.
We hypothesize that the task in EMIP relies primarily on three of the cognitive subdomains deemed most critical by Weintraub et al. citenp:[weintraubCognitionAssessmentUsing2013].
Reading and understanding code is a trained skill reliant heavily on ones understanding of language.
Understanding the entirety of a class requires keeping all functions of the class in one's working memory. The post-test is organized so that one needs to remember these functions for a short time after reading the code.
As with almost all cognitive tasks, attention is a critical part of performing well when reading and understanding code.

CSCW is a task where participants collaborate in creating a concept map from a video they have watched individually.
Naturally, language will be an important part of any collaborative work as one needs to express one's understanding of the content to one collaborator.
Executive function, specifically planning, is essential for creating concept maps.
Concept maps include tying different pieces of information together in a complete whole.
Before starting the concept map, they viewed a video explaining the concept they were to map.
In order to remember information presented in the videos, the cognitive subdomain episodic memory is at work.
Again, attention is essential when viewing a video for learning and successful collaboration with another party.

The Fractions dataset also stems from a collaborative task.
Students work together to learn about equivalent fractions in an ITS.
For the collaborative aspect, attention and language are again important.

All cognitive tasks likely include some aspect of all cognitive subdomains.
What we intend with this section is to illustrate how our three datasets cover tasks that rely more heavily on five of the six cognitive subdomains presented by Weintraub et al. citenp:[weintraubCognitionAssessmentUsing2013] as most important.
Of the six cognitive subdomains, our datasets do not include tasks that rely heavily on processing speed.
Processing speed is an important factor in good collaboration, but we did not consider this subdomain to be as central in any of the tasks and thus will not claim to cover it with these datasets.
