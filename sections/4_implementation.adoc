== Implementation

Our goal with this system is to create a platform on which we can perform our feature generalizability experiments efficiently and consistently.

In order to achieve this goal, multiple components have to be present.

. We need methods to standardize datasets, so the units are the same and the data is in the same form.
. We need to clean the data to achieve high data quality which can produce good features
. We need a platform that can generate computationally expensive features for multiple large datasets
. We need a platform that can run multiple concurrent pipelines for combinations of datasets, features, and methods for dimensionality reduction
. We need an evaluation step that collects the results from all the pipelines, and can prove pipelines generalizable.
. We need complete reproducibility.

=== Preprocessing


This subsection explains how we achieved goal 1 & 2 of creating a platform for generating generalizable features.

==== Standardization of Datasets
We have found three datasets from different experiments with different contexts.
They also vary in units used and the name of the columns.
Some of the datasets measure time in milliseconds, while others measure it in microseconds.
The datasets also use different names for the same attributes.
These were renamed to a consistent naming scheme.
Some of the subjects were missing labels, we solved this by removing the sample.
We also fixed inconsistencies such as wrong capitalizations of filenames.
The scripts for standardization can be found at Github. In misc/fix*

Something about us generating fixations for EMIP from rawdata

Below we describe in detail how we standardized each of the datasets.

===== EMIP
We changed the dataset to make it easier to handle.

. Created a new column for the status for each timeframe containing "CALIBRATION", "READING", "TEST"
. Created a new column for which trial they were performing
. Removed rows for where the values were all 0, as that could be interpreted as nan.

Preprocessing

. Remove 0 values as they are nan
.

==== Data cleaning
The datasets contains missing values


==== Normalization and Outlier removal
As our subjects comes from multiple contexts, the need for normalization and outlier removals is extra apparent.
The baseline for a subjects pupil dialation is very sensitive to lighting and how well rested you are, so it is important to normalize it.
We chose to min-max normalize the pupil diameter in the range of 0 to 1 per subject.

// The normalized x and y postitions is only used in the entropy feature so it should maybe be mentioned there
The screen sizes in the different experiments where the datasets were from are different. So we normalized the x and y positions in a 1000 by 1000 grid.

As we are working on fixations our sense of time is discretized to the start of each fixation.
But there can be artificially large periods of time between fixations, due to blinking, the subject looking away from the screen or technical malfunction on the equipment.
To mitigate this we remove the outliers by setting a threshold of 1000 ms for saccade duration, and all timegaps over 1000 ms were set to 1000ms,





=== Feature generation

To save computational time, we chose to separate the feature generation and the model training in to two separate jobs. This subsection explains how we achieved goal 3.

==== Flow
The flow of the feature generation job is as follows:

. Download the datasets from google cloud storage
. Load data as a list of pandas dataframes
. Standardize data. This step is different for each dataset
. Normalize data
. Generate aggregated attributes, such as saccade duration, saccade_length and angle of saccades.
. Take the rolling mean of the signals
. Generate features
. Upload features to google cloud storage

==== Features
The features we generate can be separated into 3 different groups based on how they were made.

* Timeseries Features
* Eyetracking Features
* Heatmap features

==== Timeseries Features
Agnostic features, they are a description of the signal, not the meaning behind the signal.
The signals we used are pupil diameter, fixation duration, saccade duration and saccade length.

Pupil diameter is the average diameter of the pupil over a fixation.
Fixation duration is the duration of a fixation, and is the difference between the endtime and starttime of a fixation.
Saccade duration is the time between two fixations.
Saccade length is the euclidiean distance between the coordinates of two fixations.

From these signals we calculate 5 features.


===== Power Spectral Histogram.
The power spectrum of a time series, decomposes the time series to the frequncies present in the signal, and the amplitude of each of these frequencies.
Once compouted, they can be represented as a histogram which is called the power spectral histogram.
 We computed the centroid, variance, skew and kurtosis of the power spectral histogram.

===== Autoregressive Moving Average model (Arma)
We know that a coginitive process takes place over a span of time, and our gaze data is organized as a time-series.
If our goal is to model cognitive workload we need to capture the temporal aspects of cognition.
ARMA combines an auto-regressive part and an moving average part to model movements in a timeseries.
It uses information fo previous events to predict future events with the same source.

An ARMA process describes a time series with two polynomials.
The first of these polynomials describes the autoregressive part of the timeseries.
The second part describes the moving average.
Arma is formally described by the following formula.

Arma takes the historical values to account
Arma is a non-stationary process
Problem solving is also a non-stationary process, since your mental model always depends on the previous mental models of the Problem.
Hence Problem solving follows can be described as an arma process.



asciimath:[X_t = sum_(j=1)^p phi_j X_(t-j) + sum_(i=1)^q theta_i epsilon_(t-i) + epsilon_t]



The features we extract from arma are extracted with the following algorithm

```
best_fit = null
for p up to 4
    for q up to 4
        fit = arma(timeseries, p, q)
        if(best_fit.aic > fit.aic)
            best_fit = fit
return best_fit["ar"], best_fit["ma"], best_fit["exog"]


```
===== Garch
Garch is similar to Arma, but it is applied to the variance of the data instead of the mean.

Arma assumes a uniform distribution of the events it's modeling with shifting trends.
However, we know that this is not entirely descriptive of cognitive processes.
When working with a cognitive task, you have periods where you focus intently to understand one aspect of the problem and other periods when your mind rests.
The heteroskedasticity aspect of GARCH accounts for this grouping of events.
Just as, in the analysis of financial markets, GARCH is able to account for the spikes and sudden drops in prices of certain investment vehicles citenp:[bollerslevGeneralizedAutoregressiveConditional1986], we posit that GARCH will allow us to model the perceived grouping of cognitive workload spikes.

We extract features from Garch similar to how we extract features from Arma.
  homogenity principle, garch is more generalizable than arma.
Garch has proved to be a

```
best_fit = null
for p up to 4
    for q up to 4
        fit = garch(timeseries, p, q)
        if(best_fit.aic > fit.aic)
            best_fit = fit
return best_fit["mu"], best_fit["omega"], best_fit["alpha"], best_fit["gamma"], best_fit["beta"]
```


===== Markov Models
I don't know the reason behind testing markov models, so we need to work that out.

Discrete arma
presenting the present value as a function of the past values.
Is discrete, when you have more continuous information the mutual information of the predictable can decrease.


```
normalized_timeseries = (timeseries - min(timeseries)) / (max(timeseries) - min(timeseries))
discretized_timeseries = discretize timeseries in 100 bins
best_fit = null
for i up to 8
   fit = GaussianHMM(covariance_type="full").fit(discretized_timeseries, n_components=i)
   if(best_fit.aic > fit.aic)
           best_fit = fit
flat_transistion_matrix = best_fit.transition_matrix.flatten()
padded_transition_matrix = pad flat_transistion matrix with n zeroes so the length is 64
return padded_transition_matrix
```

===== The Low/High Index of Pupillary Activity (LHIPA)
LHIPA citenp:[duchowskiLowHighIndex2020] is an enhancement to the Index of Pupilary Activity, which is a metric to discriminate cognitive load from pupil diameter oscillation.

For simplicity we extracted the LHIPA metric from all the signals even though it is only proven to have value on pupil diameter signals.


==== Eyetracking features
These are features that are connected to the domain of eye-tracking and not signal processing features.


===== Ratio of Information Processing Ratio


Global information processing (GIP) is often synonymous with skimming text.
Your gaze is jumping around to larger sections of the screen, and not staying at a place for a longer time.
Which results in shorter fixations and longer saccades.

Local information processing (LIP) is the exactly opposite, your gaze is focusing on smaller areas for a longer duration and not moving around that much.

For this metric fixations are measured in time, while saccades are measured in distance.
This is because we are interested in how big the area you are moving around is, and for how long you are focusing on specific areas.

To compute the ratio, we divide GIP by LIP.

The following algorithm extracts the feature:

```
upper_threshold_saccade_length = get 75 percentile of saccade_lengths
lower_threshold_saccade_length = get 25 percentile of saccade_lengths
upper_threshold_fixation_duration = get 75 percentile of fixation_durations
lower_threshold_fixation_duration = get 25 percentile of fixation_durations

LIP = 0
GIP = 0
for saccade_length, fixation_duration in saccade_lengths, fixation_durations
    fixation_is_short = fixation_duration <= lower_threshold_fixation_duration
    fixation_is_long = upper_threshold_fixation_duration <= fixation_duration
    saccade_is_short = saccade_length <= lower_threshold_saccade_length
    saccade_is_long = upper_threshold_saccade_length <= saccade_length

    if fixation_is_long and saccade_is_short:
        LIP += 1
    elif fixation_is_short and saccade_is_long:
        GIP += 1

return GIP / (LIP + 1)
```





===== Skewness of saccade speed
Saccade velocity skewness has been shown to correlate with familiarity.
If the skewness is highly positive, that means that the overall speeds were high.
It means that you knew where to look.

Familiarity and expertise is different. You can know where to look, but have to think twice before doing it.

To calculate this feature we calculated the speed by dividing the saccade length on the saccade duration.
Then we got the skew of the distribution outputted.

===== Verticality of Saccades
Something about reasoning behind looking at horizontality of saccades.

To get the horizontality of saccades we get the angle between every fixation, with respect to the x axis. We do that with arctan2, which outputs the angle in radians between pi and -pi. Since we're only interested in the horizontality of the saccade, we take the absolute value of the angle.
```
radians = atan2(y2 - y1, x2 - x1)
return abs(radians)
```
To describe the horizontality of each point in a range between 0 and 1, we take take sinus of every angle.
```
for angle in angles
   angle = sin(angle)
```
Then we average all the sinus values.

```
verticality = angles.average()
```

===== Entropy of Gaze

We use the entropy of the gaze to compute the focus size of the subject. To calculate it we create a grid of 50 by 50 px bins. And placing each fixation in one of these bins based on which bin its x and y position corresponds to. When we have this grid we flatten it and take the entropy of the resulting distribution.

This tells us if the gaze was evenly spread over the whole screen, or if the subject was more focused on specific areas of the screen. For this feature we used the normalized values for x and y, to keep the number of bins consistent between datasets.

The following algorithm extracts the feature:
```
x_normalized = normalize x between 0 and 1000
y_normalized = normalize y between 0 and 1000

x_axis = [50, 100, 150 ... ,1000]
y_axis = [50, 100, 150 ... ,1000]
2d_histogram = 2d_histogram(xaxis, yaxis, x_normalized, y_normalized)
return entropy(2d_histogram.flatten())

```



==== Heatmaps
As shown in cite:[K paper about heatmaps], heatmaps of the gaze can predict performance in learning activities.

The heatmaps for emip we generated ourselves with a python library called heatmappy.

. Split each subjects into 30 partitions
. Created a 1920 * 1080 image
. Plotted the x,y postions with heatmappy
. Resized the image to 175*90

This will return a list of heatmaps per subject.

From the heatmaps used a pretrained vgg19 model with the imagenet weights to generate a feature vector of size 1000 features per image

1. Scale the images down using the preprocess_input function found in `keras.applications.image_netutils`
2. Use the pretrained VGG-19 model to extract features per image
3. Flatten the matrix to a single list of values

===== Pseudocode
```
frames = Split each subject into 30 partitions
features = []
for frame in frames
    image = image of with dimensions 1920, 1080
    heatmap = heatmappy.heatmap_on_img(frame.x_and_y_postions, image)
    scaled_down_heatmap = keras.applications.image_netutils(heatmap)
    heatmap_features = vgg19model.predict(scaled_down_heatmap)
    features.append(heatmap_feature.flatten())
return features.flatten()
```

==== Final Feature set
After feature generation these are the features that has been generated per subject.

include::../tables/feature_set_table.adoc[]

=== Classification

This section explains how we solved goal 4 and 5 of creating our platform.
We need a platform that can run multiple concurrent pipelines for combinations of datasets, features, and methods for dimensionality reduction

==== Flow

* Download features from Google Cloud Storage
*


==== Grouping of features

Since it would be too computationally exhaustive to test all combinations of the features, we separated them into 13 different feature groups, which we will test.

The features are divided in this way:

===== Feature Groups

include::../tables/feature_groups_table.adoc[]

=== Dimensionality Reduction / Feature Selection
All of our pipelines does either feature selection or Dimensionality reduction.
The method we use for dimensionality reduction is PCA and the one for Feature selection is Lasso.
For all pipelines we also use a zero-variance filter to remove the features that has no variance in its data

==== Dimensionality Reduction
Dimensionality reduction is the process of mapping the current feature space to another feature space with a lower dimensionality.
We use PCA for dimensionality reduction.


==== Feature Selection
Feature selection is the process of reducing the number of features used by removing features that give less information.
We use Lasso as our feature selection algorithm because it performs well when the number of samples is less than the number of features.
Which is often the case in our pipelines.
For instance the heatmap feature group contains 153600 features per subject.

=== Prediction: Ensemble Learning
Our classifier is a weighted voting ensemble with a KNN-regressor, Random forest regressor and a Support Vector regressor.
To find the weights we do a cross-validation and set the respective weights as 1 - Root Mean square error.

=== Training, validation and testing setup
We do two types of testing to evaluate our pipelines: *out-of-sampling-testing* and *out-of-study-testing*.

To perform out of sampling testing, we split our in study dataset in three parts.
Training, validation, and testing data.
The training data is used to train the models, the validation set is used weight the different classifiers in the voting ensemble.
The test data is to evaluate the model on unseen data.

Out of study testing is how we can evaluate the generalizibility of the pipeline.
We are using two out of three datasets to train and evaluate the model with out of sampling testing.
The out of study testing is done with the dataset we do not use in the in study testing.
If EMIP and fractions are used for in study, then cscw is used for out of study testing.
Since all the datasets are from different contexts, we can evaluate how our pipelines generalize over contexts.

==== Evaluation Criteira

All of our pipelines are evaluated with NRMSE(normalized root mean squared error).
Since the datasets' labels are in different ranges, we need a metric that can be compared across datasets.
NRMSE is normalized from 0 to 1 where 1 is the maximum error possible for the dataset.
NRMSE is commonly used in learning technologies to evaluate learning predictions citenp:[PredictionMOOCsReview]  and is the preferred metric for student models. citenp:[pelanekMetricsEvaluationStudent2015]
Since we normalize the labels in a range from 0 to 1 before training, RMSE is equivalent to NRMSE in this thesis.
Hence we refer to it as NRMSE in this thesis.


The following formulas calculate NRMSE.

:asciimath:[RMSE = sqrt ((sum_(i=1)^text(Number of samples) (text(predicted)_i - text(original)_i)^2) / text(Number of Samples))]

:asciimath:[NRMSE = text(NRMSE)/(text(original)_max - text(original)_min)]


=== Feature Generalizability Index
We measure the generalizability of features by comparing the distributions of NRMSES from the in-study cross-validation and the out of study testing, as done in citenp:[sharmaAssessingCognitivePerformance2020].
Since the ground truth in both the in study datasets and the out of study datasets indicate cognitive performance we can safely assume that similar behaviour will indicate similar results.
To compare the distributions of RMSE we use ANOSIM (ANalysis Of SIMiliarity) which is a non parametric function to find the similarity of two distributions citenp:[clarkeNonparametricMultivariateAnalyses1993]


asciimath::[(text(mean ranks between groups) - text(mean ranks within groups))/N(N-1)/4]

The denominator normalizes the values between -1 and +1 where 0 is a random grouping.



=== Bench-marking the Generalizable Features




[%header,format=csv]
|===
// include::results_filtered.csv[]
|===

==== Reproducability
This section explains how we reached goal 6 of our platform.
* We need complete reproducibility.

Our reproducibility strategy primarily consists of two different components.
The version-control tool, git; and the machine learning management tool comet.ml.

==== Git
Git keeps track of all versions of our source-code.
Our system is set up to demand that all local changes to the code be committed to git before a run in the cloud will be allowed.
We ensure that all our parameters are represented in the code.
This in turn ensures that we always know the state of the code responsible for each experiment.
When we run an experiment in the cloud we log the start parameters of the system and the hash associated with the commit.

==== comet.ml
comet.ml is a machine learning management tool. It can handle user-management, visualization, tracking of experiments, and much more.
In our case we use it to track the results of our experiements, and how they relate to eachother.

==== Seeding
All of our experiments ran with seeded randomness. Our implementation for seeding

```
seed = 69420
np.random.seed(seed)
random.seed(seed)
os.environ["PYTHONHASHSEED"] = str(seed)
tf.random.set_seed(seed)
```