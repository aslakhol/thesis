== Implementation

Our goal with this system is to create a platform on which we can perform our feature generalizability experiments efficiently and consistently.

In order to achieve this goal, multiple components have to be present.

. We need methods to standardize datasets so the units are the same and the data is in the same form.
. We need to clean the data to achieve high data quality, which can produce good features
. We need a platform that can generate computationally expensive features for multiple large datasets
. We need a platform that can run multiple concurrent pipelines for combinations of datasets, features, and methods for dimensionality reduction
. We need an evaluation step that collects the results from all the pipelines and can prove pipelines generalizable.
. We need complete reproducibility.

=== Preprocessing


This subsection explains how we achieved goal 1 & 2 of creating a platform for generating generalizable features.

==== Standardization of Datasets
We have found three datasets from different experiments with different contexts.
They also vary in units used and the name of the columns.
Some of the datasets measure time in milliseconds, while others measure it in microseconds.
The datasets also use different names for the same attributes.
These were renamed to a consistent naming scheme.
Some of the subjects were missing labels, we solved this by removing the sample.
We also fixed inconsistencies such as wrong capitalizations of filenames.
The scripts for standardization can be found at Github. In misc/fix*

Something about us generating fixations for EMIP from rawdata


==== Data cleaning
The datasets contains missing values


==== Normalization and Outlier removal
As our subjects comes from multiple contexts, the need for normalization and outlier removals is extra apparent.
The baseline for a subjects pupil dialation is very sensitive to lighting and how well rested you are, so it is important to normalize it.
We chose to min-max normalize the pupil diameter in the range of 0 to 1 per subject.

// The normalized x and y postitions is only used in the entropy feature so it should maybe be mentioned there
The screen sizes in the different experiments where the datasets were from are different. So we normalized the x and y positions in a 1000 by 1000 grid.

As we are working on fixations our sense of time is discretized to the start of each fixation.
But there can be artificially large periods of time between fixations, due to blinking, the subject looking away from the screen or technical malfunction on the equipment.
To mitigate this we remove the outliers by setting a threshold of 1000 ms for saccade duration, and all timegaps over 1000 ms were set to 1000ms,





=== Feature generation

To save computational time, we chose to separate the feature generation and the model training in to two separate jobs. This subsection explains how we achieved goal 3.

==== Flow
The flow of the feature generation job is as follows:

. Download the datasets from google cloud storage
. Load data as a list of pandas dataframes
. Standardize data. This step is different for each dataset
. Normalize data
. Generate aggregated attributes, such as saccade duration, saccade_length and angle of saccades.
. Take the rolling mean of the signals
. Generate features
. Upload features to google cloud storage

==== Features
The features we generate can be separated into 3 different groups based on how they were made.

* Timeseries Features
* Eyetracking Features
* Heatmap features

==== Timeseries Features
Timeseries feature are features that are agnostic to the source of data.
They are a description of the signal, not the meaning behind the signal.
The signals we describe with timeseries features are pupil diameter, fixation duration, saccade duration, and saccade length.


From eac of these signals we calculate 5 features. Power Spectral Histrogram, ARMA, GARCH, Markov models and LHIPA. They are described below.


===== Power Spectral Histogram.
The power spectrum of a timeseries decomposes the time series to the frequencies present in the signal and the amplitude of each frequency.
Once computed, they can be represented as a histogram called the power spectral histogram.
We computed the centroid, variance, skew, and kurtosis of the power spectral histogram.

The power spectral histogram describes the repetition of patterns in the data.
We hypothesize that similar pattern repetitions will be present in subjects that display a high cognitive performance across contexts.


===== Autoregressive Moving Average model (ARMA)
We know that a cognitive process takes place over a span of time, and our gaze data is organized as a timeseries.
If our goal is to model cognitive workload we need to capture the temporal aspects of cognition.
ARMA combines an auto-regressive part and an moving average part to model movements in a timeseries.
It uses information fo previous events to predict future events with the same source.

An ARMA process describes a time series with two polynomials.
The first of these polynomials describes the autoregressive part of the timeseries.
The second part describes the moving average.
Arma is formally described by the following formula.

Arma takes the historical values to account
Arma is a non-stationary process
Problem solving is also a non-stationary process, since your mental model always depends on the previous mental models of the Problem.
Hence Problem solving follows can be described as an arma process.



asciimath:[X_t = sum_(j=1)^p phi_j X_(t-j) + sum_(i=1)^q theta_i epsilon_(t-i) + epsilon_t]



The features we extract from arma are extracted with the following algorithm

```
best_fit = null
for p up to 4
    for q up to 4
        fit = arma(timeseries, p, q)
        if(best_fit.aic > fit.aic)
            best_fit = fit
return best_fit["ar"], best_fit["ma"], best_fit["exog"]


```
===== Garch
Garch is similar to Arma, but it is applied to the variance of the data instead of the mean.

Arma assumes a uniform distribution of the events it's modeling with shifting trends.
However, we know that this is not entirely descriptive of cognitive processes.
When working with a cognitive task, you have periods where you focus intently to understand one aspect of the problem and other periods when your mind rests.
The heteroskedasticity aspect of GARCH accounts for this grouping of events.
Just as, in the analysis of financial markets, GARCH is able to account for the spikes and sudden drops in prices of certain investment vehicles citenp:[bollerslevGeneralizedAutoregressiveConditional1986], we posit that GARCH will allow us to model the perceived grouping of cognitive workload spikes.

We extract features from Garch similar to how we extract features from Arma.
  homogenity principle, garch is more generalizable than arma.
Garch has proved to be a

```''
best_fit = null
for p up to 4
    for q up to 4
        fit = garch(timeseries, p, q)
        if(best_fit.aic > fit.aic)
            best_fit = fit
return [best_fit["alpha"],
       best_fit["beta"],
       best_fit["gamma"],
       best_fit["mu"],
       best_fit["omega"]]

```


===== Hidden Markov Models

Hidden Markov Models contains the Markov assumption, which assumes that the present value is conditionally independent on its non-ancestors given its n last parents.
Similarly to ARMA, this means that the current value is a function of the past values.
While ARMA models continuous values, HMM is discrete.
Hence we discretize the values into 100 buckets before we model the transitions between these buckets with HMM.
We then use the resulting transition matrix as the feature.

The reasoning behind using HMM is the same as to why we chose ARMA.
We hypothesize that HMM might model the changing state nature of cognitive work.

```
normalized_timeseries = normalize the timeseries between 0 and 1
discretized_timeseries = discretize timeseries in 100 bins
best_fit = null
for i up to 8
   fit = GaussianHMM(covariance_type="full").fit(discretized_timeseries, n_components=i)
   if(best_fit.aic > fit.aic)
           best_fit = fit
flat_transistion_matrix = best_fit.transition_matrix.flatten()
padded_transition_matrix = pad flat_transistion matrix with n zeroes so the length is 64
return padded_transition_matrix
```

===== The Low/High Index of Pupillary Activity (LHIPA)
LHIPA citenp:[duchowskiLowHighIndex2020] enhances the Index of Pupillary Activity citenp:[duchowskiIndexPupillaryActivity2018], which is a metric to discriminate cognitive load from pupil diameter oscillation.
LHIPA partially models the functioning of the human autonomic nervous system by looking at the low and high frequencies of pupil oscillation.

Cognitive load has been shown to correlate with cognitive performance citenp:[hebbDrivesConceptualNervous1955].
The Yerkes-Dodson law describes the relationship between the two, indicating an optimal plateau where a certain degree of cognitive workload is tied to maximized cognitive performance.
If the cognitive workload is increased beyond this point, cognitive performance is diminished citenp:[hebbDrivesConceptualNervous1955].

Our implementation of LHIPA is based on the code found in citenp:[duchowskiIndexPupillaryActivity2018, duchowskiLowHighIndex2020]



==== Eyetracking features
These are features that are connected to the domain of eye-tracking and not signal processing features.


===== Ratio of Information Processing Ratio


Global information processing (GIP) is often synonymous with skimming text.
Your gaze is jumping around to larger sections of the screen, and not staying at a place for a longer time.
Which results in shorter fixations and longer saccades.

Local information processing (LIP) is the exactly opposite, your gaze is focusing on smaller areas for a longer duration and not moving around that much.

For this metric fixations are measured in time, while saccades are measured in distance.
This is because we are interested in how big the area you are moving around is, and for how long you are focusing on specific areas.

To compute the ratio, we divide GIP by LIP.

The following algorithm extracts the feature:

```
upper_threshold_saccade_length = get 75 percentile of saccade_lengths
lower_threshold_saccade_length = get 25 percentile of saccade_lengths
upper_threshold_fixation_duration = get 75 percentile of fixation_durations
lower_threshold_fixation_duration = get 25 percentile of fixation_durations

LIP = 0
GIP = 0
for saccade_length, fixation_duration in saccade_lengths, fixation_durations
    fixation_is_short = fixation_duration <= lower_threshold_fixation_duration
    fixation_is_long = upper_threshold_fixation_duration <= fixation_duration
    saccade_is_short = saccade_length <= lower_threshold_saccade_length
    saccade_is_long = upper_threshold_saccade_length <= saccade_length

    if fixation_is_long and saccade_is_short:
        LIP += 1
    elif fixation_is_short and saccade_is_long:
        GIP += 1

return GIP / (LIP + 1)
```





===== Skewness of saccade speed
Saccade velocity skewness has been shown to correlate with familiarity.
If the skewness is highly positive, that means that the overall speeds were high.
It means that you knew where to look.

Familiarity and expertise is different. You can know where to look, but have to think twice before doing it.

To calculate this feature we calculated the speed by dividing the saccade length on the saccade duration.
Then we got the skew of the distribution outputted.

```
get_skewness_of_saccades(saccade_duration, saccade_length):
    saccade_speed = saccade_length/saccade_duration
    return saccade_speed.skew()

```

===== Verticality of Saccades
By verticality of saccades, we mean the ratio of which saccades move vertically over horizontally.
Our intuition for generating this feature is based on the difference between how we read code versus how we read text.
An experienced coder is read vertically, focusing on definitions and conditionals.
Traditional text, on the other hand, is read line by line in a horizontal fashion.
Based on this anecdotal observation, we were interested in how well the verticality of saccades would generalize.


To calculate the feature, we get the angle between every consecutive fixation with respect to the x-axis.
We do that with arctan2, which outputs the angle in radians between pi and -pi.
Since we are only interested in the verticality of the saccade, we take the absolute value of the angle.

```
radians = atan2(y2 - y1, x2 - x1)
return abs(radians)
```
To describe the horizontality of each point in a range between 0 and 1, we take the sine of every angle.
```
for angle in angles
   angle = sin(angle)
```
Then we average all the sinus values.

```
verticality = angles.average()
```

===== Entropy of Gaze
The entropy of gaze explains the size of the field of focus for a subject.
Entropy is higher the subject's attention is more evenly spread over the stimulus and lower if the subject focuses on a minor part of the stimulus.


To calculate entropy, of gaze we create a grid of 50 by 50 px bins.
We then normalize the x and y positions of each fixation in a range from 0 to 1000.
Further, we place each fixation in one of these bins based on which bin its x and y position corresponds to.
When we have this grid, we flatten it and take the entropy of the resulting distribution.

The following algorithm extracts the feature:
```
x_normalized = normalize x between 0 and 1000
y_normalized = normalize y between 0 and 1000

x_axis = [50, 100, 150 ... ,1000]
y_axis = [50, 100, 150 ... ,1000]
2d_histogram = 2d_histogram(xaxis, yaxis, x_normalized, y_normalized)
return entropy(2d_histogram.flatten())

```



==== Heatmaps
A heatmap is a graphical representation of data where values are depicted by color.
Areas of higher activity will be highlighted more.
Our heatmaps represent the gaze position of a subject over time.
To capture both spatial and temporal data, we create multiple heatmaps for each subject.

These are the steps we take to create our heatmaps

. Split the data from each subject into 30 partitions
. Create a 1920 * 1080 image
. Plot the gaze position with heatmappy citenp:[LumenResearchHeatmappy2021]
. Resize the image to 175*90

.Heatmap without stimulus from EMIP
image:../figures/heatmap_emip.png[]

From the heatmaps used a pretrained vgg19 model citenp:[simonyanVeryDeepConvolutional2015] with the imagenet weights citenp:[krizhevskyImageNetClassificationDeep2017] to generate a feature vector per image.

1. Scale the images down using the preprocess_input function found in `keras.applications.image_netutils`
2. Use the pre-trained VGG-19 model to extract features per image
3. Flatten the matrices outputted by the VGG19 model to a single list of values and concatenate them.


As shown by Sharma et al. citenp:[sharmaEyetrackingArtificialIntelligence2020], deep features of heatmaps from gaze data can predict cognitive performance in learning activities.




===== Pseudocode
```
frames = Split each subject into 30 partitions
features = []
for frame in frames
    image = image of with dimensions 1920, 1080
    heatmap = heatmappy.heatmap_on_img(frame.x_and_y_postions, image)
    scaled_down_heatmap = keras.applications.image_netutils(heatmap)
    heatmap_features = vgg19model.predict(scaled_down_heatmap)
    features.append(heatmap_feature.flatten())
return features.flatten()
```

==== Final Feature set
After feature generation these are the features that has been generated per subject.

include::../tables/feature_set_table.adoc[]

=== Pipelines

This section explains how we solved goal 4 of creating our platform.
> We need a platform that can run multiple concurrent pipelines for combinations of datasets, features, and methods for reducing the feature space.

By a pipeline we mean a specific combination of datasets, feature groups and methods for reducing the feature space (dimensionality reduction or feature selection).

For simplicity, we will refer to these as pipeline components.


==== Datasets
We have three different datasets: EMIP, Fractions, and CSCW.
As is discussed in xref:study[] for each of our pipelines, we designate either one or two datasets as in-study.
All pipelines include a single dataset as the out-of-study dataset.
No dataset is used twice in one pipeline.

We refer to the pipelines where one dataset is designated in-study as 1-to-1 pipelines, as these pipelines train on one dataset and test on another.
Pipelines, where two datasets are designated in-study, are referred to as 2-to-1 pipelines since we train on two datasets combined and test on one dataset.
We have three datasets, which make up nine dataset combinations, six of which create 1-to-1 pipelines and three go into 2-to-1 pipelines.

==== Feature groups
Initially, we considered running pipelines to test all combinations of features.
These would prove to be unfeasible.
With 27 features, 9 dataset combinations, and two methods of reducing the feature space, there would be 2 415 919 104 pipelines.
With a theoretical runtime of 1 second per pipeline, the total computing time necessary to tackle this challenge would be approximately 76 years, and our grandchildren would have to submit this thesis.

As an alternative, we decided to group our features manually.
We created one group for the eye-tracking features, four for the type of signal, and five for the different timeseries features.
In addition, we have a separate group for heatmaps and, lastly, one group with includes all the features.
The groups are presented in the following table:

include::../tables/feature_groups_table.adoc[]

==== Reducing the feature space
Our pipelines perform either feature selection or dimensionality reduction to reduce the number of features and improve predictive power.
The focus of our thesis was on testing different features, and as such, we decided not to test a wide range of alternatives.
The effect of different methods for reducing the feature space on generalizability is a potential area for further study.
The method we use for dimensionality reduction is PCA, and the one for feature selection is LASSO.
For all pipelines, we also use a zero-variance filter to remove the features that have no variance in their data

Lasso was selected as our feature selection algorithm because it has been shown to perform very well when the number of samples is less than the number of features citenp:[tibshiraniRegressionShrinkageSelection1996, giannakosMultimodalDataMeans2019], which is the case for most of our feature groups.


==== Prediction: Ensemble Learning
Our pipelines were tested with the same classifier, a weighted voting ensemble with a KNN-regressor, a Random forest regressor, and a Support Vector regressor.
To find the weights for the voting, we perform cross-validation on each of the regressors and set their respective weights as 1 - Root Mean square error.
We use an ensemble because it has been shown to improve stability and robustness, capture linear as well as non-linear relationships, and the variance in our datasets suggests that a single model would perform inadequately citenp:[sharmaAssessingCognitivePerformance2020].

=== Evaluation
This section will outline how we achieve the fifth goal of our platform:
    > We need an evaluation step that collects the results from all the pipelines and can prove pipelines generalizable.

Models produce by our pipelines are evaluated in two ways, with out-of-sample testing and out-of-study testing.
Out-of-sample testing uses a subset the in-study dataset to evaluate the predictive power of the model, while out-of-study testing uses the dataset designated as out-of-study to evaluate generalizability.
This subsection explains how we evaluate the results of each test.

==== Evaluation Criteira

All of our pipelines are evaluated with NRMSE(normalized root mean squared error).
Since the datasets' labels are in different ranges, we need a metric that can be compared across datasets.
NRMSE is normalized from 0 to 1 where 1 is the maximum error possible for the dataset.
NRMSE is commonly used in learning technologies to evaluate learning predictions citenp:[PredictionMOOCsReview]  and is the preferred metric for student models. citenp:[pelanekMetricsEvaluationStudent2015]
Since we normalize the labels in a range from 0 to 1 before training, RMSE is equivalent to NRMSE in this thesis.
Hence we refer to it as NRMSE in this thesis.


The following formulas calculate NRMSE.

asciimath:[RMSE = sqrt ((sum_(i=1)^text(Number of samples) (text(predicted)_i - text(original)_i)^2) / text(Number of Samples))]

asciimath:[NRMSE = text(NRMSE)/(text(original)_max - text(original)_min)]


=== Feature Generalizability Index
We measure the generalizability of features by comparing the distributions of NRMSES from the in-study cross-validation and the out of study testing, as done in citenp:[sharmaAssessingCognitivePerformance2020].
Since the ground truth in both the in study datasets and the out of study datasets indicate cognitive performance we can safely assume that similar behaviour will indicate similar results.
To compare the distributions of RMSE we use ANOSIM (ANalysis Of SIMiliarity) which is a non parametric function to find the similarity of two distributions citenp:[clarkeNonparametricMultivariateAnalyses1993]


asciimath:[(text(mean ranks between groups) - text(mean ranks within groups))/N(N-1)/4]

The denominator normalizes the values between -1 and +1 where 0 is a random grouping.


=== Reproducability
This section explains how we reached goal 6 of our platform.
* We need complete reproducibility.

Our reproducibility strategy primarily consists of two different components.
The version-control tool, git; and the machine learning management tool comet.ml.

==== Git
Git keeps track of all versions of our source-code.
Our system is set up to demand that all local changes to the code be committed to git before a run in the cloud will be allowed.
We ensure that all our parameters are represented in the code.
This in turn ensures that we always know the state of the code responsible for each experiment.
When we run an experiment in the cloud we log the start parameters of the system and the hash associated with the commit.

==== comet.ml
comet.ml is a machine learning management tool. It can handle user-management, visualization, tracking of experiments, and much more.
In our case we use it to track the results of our experiements, and how they relate to eachother.

==== Seeding
All of our experiments ran with seeded randomness. Our implementation for seeding

```
seed = 69420
np.random.seed(seed)
random.seed(seed)
os.environ["PYTHONHASHSEED"] = str(seed)
tf.random.set_seed(seed)
```