== Results
We ran a total of 216 pipelines with different combinations of datasets, dimensionality reduction, and feature groups.
144 of these pipelines were 1 to 1 pipelines, where we trained on one dataset and tested on another.
72 of them were 2 to 1 where we had two in-study datasets and 1 out of study.
In this section, we will present the results from these pipelines.
xref:baselines[] explains how we calculate the baselines for the datasets, xref:evaluating_generalizability[] how we handle pipelines that do not exceed the baseline, and xref:aggregation[] explains how we aggregated the results across all the pipelines.
xref:feature_selection_dim_reduction[] and presents the results and answers whether feature selection or dimensionality reduction performs best in prediction power and generalizability.
xref:features[] does the same for the features.
The two sections start by evaluating how the respective variable performs when training and testing in the same context before considering generalizability.

[[baselines]]
=== Baselines
We use two different baselines to evaluate our results, in study baseline and out of study baseline. They are described in the following subsections.

==== In study baseline
The in-study baseline would be the NRMSE value if all the predictions were the labels' mean, regardless of the feature vectors.
We calculate a baseline for each of the dataset combinations and use these to evaluate the pipelines with the dataset combination as in-study.
We do this because the baselines differ between the datasets since the distribution of labels is different.
include::../tables/in_study_baselines_table.adoc[]

Pseudocode for the in-study baseline:
```
def get_baseline(labels):
    error = labels - labels.mean()
    error_squared = (error**2).mean()
    baseline = math.sqrt(error_squared)
    return baseline
```

==== Out of study baseline
We use a different method to calculate the out-of-study baselines.
We do this because the model has no a priori knowledge of the distribution of labels, and it would be a tremendous achievement to beat the baselines based on the mean.
We calculate the out-of-study baselines by taking the mean of the errors created by choosing values from a uniform and a normal distribution.


include::../tables/out_of_study_baselines.adoc[]

Pseudocode for the out-of-study baseline:
```
def get_random_normal_prediction(labels):
    prediction = [np.random.normal(loc=labels.mean(), scale=labels.std()) for i in labels]
    rmse_baseline = mean_squared_error(labels, prediction, squared=False)
    return rmse_baseline

def get_random_uniform_prediction(labels):
    prediction = [np.random.uniform(0,1) for i in labels]
    rmse_baseline = mean_squared_error(labels, prediction, squared=False)
    return rmse_baseline

def get_oos_baseline(labels):
    normal_rmses = [get_random_normal_prediction(labels) for i in range(300)]
    avg_normal_rmse= sum(normal_rmses)/len(normal_rmses)
    uniform_rmses = [get_random_uniform_prediction(labels) for i in range(300)]
    avg_uniform_rmse= sum(uniform_rmses)/len(uniform_rmses)
    return (avg_normal_rmse + avg_uniform_rmse)/2
```

[[evaluating_generalizability]]
=== Evaluating Generalizability
We evaluate the generalizability of the pipelines with the out of study dataset.
The results from predicting the out-of-study dataset are compared to the cross-validation results with FGI, which we explained in chapter 4.
To find the most generalizable features, we need to filter out the pipelines that perform poorly on both the in-study and out-of-study datasets since two poor predictions yield a good FGI.
We do this by filtering out the pipelines that do not perform better than the baseline on both in-study and out-of-study and rank the remaining pipelines on FGI.

[[aggregation]]
=== Aggregation of the results
To evaluate how each dimensionality reduction method and each feature combination performs across all the pipelines, we need to aggregate the pipelines' results.
We do this by ranking, giving each pipeline a rank for NRMSE and FGI, then grouping on either dimensionality reduction, feature combination, or both, then taking the average.
This mean of ranks gives us the results of which variables produce the most generalizable pipelines and perform best when testing it on a partition of the dataset it was trained on.


[[same_training_and_testing_context]]
===  Same Training and testing context


====  Dimensionality reduction and feature selection
xref:pipelines_nrmses_dim_reduction[] shows which dimensionality reduction method is used for the pipelines with the 5 smallest NRMSES per dataset.
Which method performs better seems to rely heavily on the in-study dataset, making it hard to conclude which of the two performs better.

[[pipelines_nrmses_dim_reduction]]
.The pipelines with the 5 lowest NRMSES per dataset and if they used dimensionality reduction or feature selection
image::../figures/dimensionality_reduction_context_sensitivity.jpg[]

However, when we aggregate the results as seen in xref:one_to_one_dim_nrmse_aggregated[], and xref:two_to_one_dim_nrmse_aggregated[], we can see that Lasso performs slightly better than PCA across all 1 to 1 pipelines, and clearly better across the 2 to 1 pipelines.
So our results indicate that feature selection performs better than dimensionality in predicting cognitive performance on gaze-data in the same context.

[[one_to_one_dim_nrmse_aggregated]]
.NRMSE Rank and mean NRMSE for Dimensionality reduction and feature selection across all 1 to 1 pipelines
[format="csv", options="header"]
|===
include::../tables/results/1_to_1_dimensionality_reduction_samecontext_aggregated.csv[]
|===

[[two_to_one_dim_nrmse_aggregated]]
.NRMSE Rank and mean NRMSE for Dimensionality reduction and feature selection across all 2 to 1 pipelines
[format="csv", options="header"]
|===
include::../tables/results/2_to_1_dimensionality_reduction_samecontext_aggregated.csv[]
|===

==== Features


xref:pipelines_nrmses_features[] shows some overlap to which features work in both the 1 to 1 pipelines and 2 to 1 pipelines.
_FFT_ gave good predictions on _fractions_ and is also among the top 5 for _emip_fractions_ and _fractions_cscw_.
This pattern repeats for _cscw_ where _saccade_duration_ are among the top 5 in _cscw_, _fractions_cscw_ and _emip_cscw_.
For _EMIP_ this is shown with Arma.


[[pipelines_nrmses_features]]
.The pipelines with the 5 lowest NRMSES per dataset and which feature group it contained
image::../figures/feature_groups_context_sensitivity.jpg[]

When we aggregate the results, as seen in xref:[[one_to_one_features_nrmse_aggregated]] and xref:[[two_to_one_features_nrmse_aggregated]]. We see that Heatmaps is the best feature for training and testing on the same dataset for 1 to 1 pipelines, performing slightly better than _fixation_duration_ and _fft_.
However, _fixation_duration_ and _fft_ are also among the top five of the 2-to-1 pipelines, indicating that they are overall better choices. _Garch_ is the feature that performs best on the 2 to 1 pipelines.

[[one_to_one_features_nrmse_aggregated]]
.5 smallest NRMSE Rank and mean NRMSE for the feature groups across all 1 to 1 pipelines
[format="csv", options="header"]
|===
include::../tables/results/1_to_1_features_samecontext_aggregated.csv[]
|===

[[two_to_one_features_nrmse_aggregated]]
.5 smallest NRMSE Rank and mean NRMSE for the feature groups across all 2 to 1 pipelines
[format="csv", options="header"]
|===
include::../tables/results/2_to_1_features_samecontext_aggregated.csv[]
|===




=== Generalizability

====  Feature Selection vs Dimensionality reduction

xref:generalizable_pipelines[] also has no clear indication of whether Lasso or PCA is the most generalizable.

[[generalizable_pipelines]]
.Generalizable pipelines
[format="csv", options="header"]
|===
include::../tables/results/two_to_one_top_generalizable_pipelines.csv[]
|===



=== Context Sensitivity

[[context_sensitive_pipelines]]
.Context sensitive pipelines
[format="csv", options="header"]
|===
include::../tables/results/two_to_one_context_sensitivity.csv[]
|===