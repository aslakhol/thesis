[[results]]
== Results
We ran a total of 216 pipelines with different combinations of datasets, dimensionality reduction, and feature groups.
144 of these pipelines were 1-to-1 pipelines, where we trained on only one dataset and tested on another.
72 of the pipelines were 2-to-1, where two datasets are designated in-study and one dataset designated out-of-study.
In this section, we will present the results from these pipelines.
In xref:baselines[], we explain how baselines for the different datasets are calculated.
xref:same_training_and_testing_context[] presents how pipelines performed in out-of-sample testing and which components performed the best overall by aggregating the results.
The pipelines that show generalizability are presented in xref:generalizability[].
The section also presents which components make up those pipelines.
Pipelines that exhibit more context sensitivity are presented in xref:context_sensitivity[].

[[baselines]]
=== Baselines

We calculate a specific for each dataset combination.
We use the NRMSE value equivalent to predicting the mean label value from the in-study dataset as the baseline for each combination.
Our pipelines are evaluated against the baseline for the given dataset combination.

include::../tables/in_study_baselines_table.adoc[]

Pseudocode for the in-study baseline:
```
def get_baseline(labels):
    error = labels - labels.mean()
    error_squared = (error**2).mean()
    baseline = math.sqrt(error_squared)
    return baseline
```

[[same_training_and_testing_context]]
===  Out-of-sample testing

Our work focuses on engineering generalizable features.
This section outlines the results from the out-of-sample testing, which does not indicate generalizability.
However, the section is presented to the reader to benchmark the range of predictive power that our features exhibit in more traditional tasks.

==== Aggregation of the results

To evaluate how each dimensionality reduction method and each feature combination performs across all the pipelines, we need to aggregate the pipelines' results.
We do this by ranking, giving each pipeline a rank for NRMSE, then grouping on either dimensionality reduction, feature combination, then taking the average.
This mean of ranks gives us the results of what pipeline components perform best when testing in the same context it was trained.

====  Dimensionality reduction and feature selection
xref:pipelines_nrmses_dim_reduction[] shows which method reduces the feature space for the pipelines with the five smallest NRMSEs per dataset.
Which method performs better seems to rely heavily on the in-study dataset, making it hard to conclude which of the two performs better.

[[pipelines_nrmses_dim_reduction]]
.The four best pipelines by NRMSE per dataset and if whether used dimensionality reduction or feature selection
image::../figures/dimensionality_reduction_context_sensitivity.jpg[]

However, when we aggregate the results as seen in xref:one_to_one_dim_nrmse_aggregated[], and xref:two_to_one_dim_nrmse_aggregated[], we can see that LASSO performs slightly better than PCA across all 1-to-1 pipelines, and clearly better across the 2-to-1 pipelines.
So our results indicate that feature selection performs better than dimensionality reduction in predicting cognitive performance on gaze data for out-of-sample testing.

[[one_to_one_dim_nrmse_aggregated]]
.NRMSE Rank and mean NRMSE for PCA and LASSO for all 1-to-1 pipelines
[format="csv", options="header"]
|===
include::../tables/results/1_to_1_dimensionality_reduction_samecontext_aggregated.csv[]
|===

[[two_to_one_dim_nrmse_aggregated]]
.NRMSE Rank and mean NRMSE for PCA and LASSO for all 2-to-1 pipelines
[format="csv", options="header"]
|===
include::../tables/results/2_to_1_dimensionality_reduction_samecontext_aggregated.csv[]
|===

==== Features

xref:pipelines_nrmses_features[] shows the best four pipelines by NRMSE and their feature groups.
It seems that several different feature groups perform quite well for the different dataset combinations.
We note that fixation duration performs well in most combinations and that spectral histograms perform well in several pipelines and that ALL performs well, particularly for fractions_cscw.

[[pipelines_nrmses_features]]
.The four best pipelines by NRMSES per dataset and what feature group it contained.
image::../figures/feature_groups_context_sensitivity.jpg[]

When we aggregate the results, as seen in xref:[one_to_one_features_nrmse_aggregated] and xref:[two_to_one_features_nrmse_aggregated]. We see that Heatmaps is the best feature for training and testing on the same dataset for 1-to-1 pipelines, performing slightly better than _fixation_duration_ and _fft_.
However, _fixation_duration_ and _fft_ are also among the top four of the 2-to-1 pipelines, indicating that they are overall better choices. _Garch_ is the feature that performs best on the 2-to-1 pipelines.

[[one_to_one_features_nrmse_aggregated]]
.Four smallest NRMSE Rank and mean NRMSE for the feature groups across all 1-to-1 pipelines
[format="csv", options="header"]
|===
include::../tables/results/1_to_1_features_samecontext_aggregated.csv[]
|===

[[two_to_one_features_nrmse_aggregated]]
.Four smallest NRMSE Rank and mean NRMSE for the feature groups across all 2-to-1 pipelines
[format="csv", options="header"]
|===
include::../tables/results/2_to_1_features_samecontext_aggregated.csv[]
|===



[[generalizability]]
=== Generalizability
We evaluate the generalizability of the pipelines with the out of study dataset.
We compare the errors from predicting the out-of-study dataset to the cross-validation results with FGI, which we explained in chapter 4.
To find the most generalizable features, we need to filter out the pipelines that perform poorly on out-of-sample testing.
We do this by filtering out the pipelines that do not perform better than the baseline.
From 216 pipelines, 72 beat the baseline.

[[distribution_fgi]]
.Kernel Density Estimation plot of the FGI by pipeline type
image::../figures/distribution_of_FGI.png[]

As represented in xref:distribution_fgi[], 2-to-1 pipelines are in general more generalizable than 1-to-1 pipelines.
This is in line with the results of citenp:[sharmaAssessingCognitivePerformance2020].
For simplicity and since our focus is generalizability, we will only refer to the 2-to-1 pipelines in the following sections.


[[generalizable_pipelines]]
.Generalizable pipelines
[format="csv", options="header"]
|===
include::../tables/results/generalizable_pipelines.csv[]
|===

xref:generalizable_pipelines[] shows the 10 most generalizable pipelines.
The first and most prevalent factor for generalizable pipelines is which datasets were the in study dataset.
When the in-study dataset is the combination of fractions and cscw, and emip is the out of study dataset, the pipelines are more generalizable.
Nine of the ten generalizable pipelines contains this combination of datasets.

For dimensionality reduction or feature selection Lasso is the most represented method among the more generalizable pipelines.
It is also included in the two pipelines with the lowest FGI.
However in combination with the feature group _all_ (ID 23, ID 22) and _heatmaps_ (ID 3, ID 2) PCA has a lower FGI than Lasso.
These are the two largest feature groups containing hundreds of thousands of values.

In the feature groups there is a high variety among the more generalizable pipelines.
However three feature groups shows up more than once, _all_, _heatmaps_ and _Garch_.
This indicates that they might be more generalizable feature groups.
It should also be noted that _GARCH_ is also the only pipeline with another in study dataset than fractions_cscw.




[[context_sensitivity]]
=== Context Sensitivity
The bottom third of the filtered baselines, contains the pipelines that are more context specific.

[[context_sensitive_pipelines]]
.Context sensitive pipelines
[format="csv", options="header"]
|===
include::../tables/results/context_sensitive_pipelines.csv[]
|===


Again we can see that the dataset combination highly affects which pipelines are deemed context-sensitive, with a combination of emip_cscw and emip_fractions as in-study datasets.
Lasso has a majority in more context specific pipelines, however there are more pipelines with Lasso that beats the baseline than there are pipelines with PCA.
For the features we can see some difference from the generalizable pipelines.
_ARMA_, _LHIPA_, _eye_tracking_, and _pupil_diameter_, are present in the context-sensitive pipelines but not in the more generalizable ones.
Indicating that they are more context-sensitive features.
_Heatmaps_, _fft_, _markov_, _all_ and _saccade_length_ shows up in both, again indicating that dataset combination is an important variable for generalizability.
