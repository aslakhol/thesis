== Results and Discussion
We ran all different combinations of datasets, dimensionality reduction and feature groups.

=== Baselines
We use two different baselines to evaluate our results, in study baseline and out of study baseline.

==== In study baseline
The in study baseline is a basic error of something i need to find a reference for this.
We calculate a baseline for each of the dataset combinations and use these to evaluate the pipelines with the datasets combinations.
This is because the baselines differ between the datasets, since the distribution of labels is different.
include::../tables/in_study_baselines_table.adoc[]

Pseudocode for the in study baseline.
```
def get_baseline(labels):
    error = labels - labels.mean()
    error_squared = (error**2).mean()
    baseline = math.sqrt(error_squared)
    return baseline
```

==== Out of study baseline
We use a different method to calculate the out of study baselines.
We do this because the model has no a priori knowledge of the distribution of labels, and it would be a tremendous achievement to beat the baselines based on the mean.
We calculate the out of study baselines by taking the mean of the errors created by prediciting with a uniform and a normal distribution.


include::../tables/out_of_study_baselines.adoc[]

Pseudocode for the out of study baseline.
```
def get_random_normal_prediction(labels):
    prediction = [np.random.normal(loc=labels.mean(), scale=labels.std()) for i in labels]
    rmse_baseline = mean_squared_error(labels, prediction, squared=False)
    return rmse_baseline

def get_random_uniform_prediction(labels):
    prediction = [np.random.uniform(0,1) for i in labels]
    rmse_baseline = mean_squared_error(labels, prediction, squared=False)
    return rmse_baseline

def get_oos_baseline(labels):
    normal_rmses = [get_random_normal_prediction(labels) for i in range(300)]
    avg_normal_rmse= sum(normal_rmses)/len(normal_rmses)
    uniform_rmses = [get_random_uniform_prediction(labels) for i in range(300)]
    avg_uniform_rmse= sum(uniform_rmses)/len(uniform_rmses)
    return (avg_normal_rmse + avg_uniform_rmse)/2
```

=== Evaluating Generalizability
We evaluate the generalizability of the pipelines with the out of study dataset.
The results from predicting on the out of study dataset is compared to the cross-validation results with FGI which is explained in chapter 4.
To find the most generalizable features, we need to filter out the pipelines performs badly on both the in study and out of study dataset, since two models performs badly may yield a good FGI.
This is because trash is similar to trash.
We do this by filtering out the pipelines that does not perform better than the baseline on both in study and out of study, and rank the remaining pipelines on FGI.



=== Feature Selection and Dimensionality reduction

==== Same training and testing context
There seems to be no clear results to if Lasso or PCA is best for training and testing in the same context.

image::../figures/dimensionality_reduction_context_sensitivity.jpg[]

==== Generalizability
There seems to be no clear results to if Lasso or PCA is most generalizable.

image::../figures/dimensionality_reduction_generalizability.jpg[]


=== Feature groups

==== Same training and testing context

image::../figures/feature_groups_context_sensitivity.jpg[]

==== Generalizability
image::../figures/feature_groups_generalizability.jpg[]
